{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import collections\n",
    "import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some value\n",
    "max_seq_length = 512\n",
    "masked_lm_prob = 0.15\n",
    "max_predictions_per_seq = 20\n",
    "rng = random.Random()\n",
    "\n",
    "pretrain_model_path = './chinese_wwm_pytorch/'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['type','title','text']\n",
    "dftrain = pd.read_csv('./data_after_sep/train.tsv',sep='\\t',names=column_names)\n",
    "dftest = pd.read_csv('./data_after_sep/test.tsv',sep='\\t',names=column_names)\n",
    "dfdev = pd.read_csv('./data_after_sep/dev.tsv',sep='\\t',names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from zhon.hanzi import stops\n",
    "def cut_sent(para):\n",
    "    para = re.sub(\"([。！？\\?])([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\.{6})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\…{2})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"([。！？\\?][”’])([^，。！？\\?])\", r\"\\1\\n\\2\", para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？').replace('.','。')\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content\n",
    "\n",
    "class TrainingInstance(object):\n",
    "    # \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,is_random_next,attention_mask,original_tokens):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.attention_mask = attention_mask\n",
    "        self.original_tokens = original_tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "# \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()   \n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(\n",
    "    tokens,\n",
    "    masked_lm_prob,\n",
    "    max_predictions_per_seq,\n",
    "    vocab_words,\n",
    "    rng,\n",
    "    ):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]' or token == '[PAD]':\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "\n",
    "    output_tokens = list(tokens)\n",
    "\n",
    "    num_to_predict = min(max_predictions_per_seq, max(1,\n",
    "                         int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if index in covered_indexes:\n",
    "            continue\n",
    "        covered_indexes.add(index)\n",
    "\n",
    "        masked_token = None\n",
    "\n",
    "    # 80% of the time, replace with [MASK]\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = '[MASK]'\n",
    "        else:\n",
    "\n",
    "      # 10% of the time, keep original\n",
    "\n",
    "            if rng.random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "            else:\n",
    "\n",
    "      # 10% of the time, replace with random word\n",
    "\n",
    "                masked_token = vocab_words[rng.randint(0,\n",
    "                        len(vocab_words) - 1)]\n",
    "\n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        masked_lms.append(MaskedLmInstance(index=index,\n",
    "                          label=tokens[index]))\n",
    "\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-222fe66829cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdftrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfdev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dfall = pd.concat([dftrain,dftest,dfdev])\n",
    "all_texts = dfall['text'].tolist()\n",
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35546\n"
     ]
    }
   ],
   "source": [
    "all_documents = [[]]\n",
    "for text in all_texts:\n",
    "    text = clean_string(text)\n",
    "    sentences = cut_sent(text)\n",
    "    all_documents.append(sentences)\n",
    "\n",
    "# Remove empty documents\n",
    "all_documents = [x for x in all_documents if x]\n",
    "rng.shuffle(all_documents)\n",
    "print(len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_instance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76608\n"
     ]
    }
   ],
   "source": [
    "for document_index in range(len(all_documents)):\n",
    "    document = all_documents[document_index]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "    target_seq_length = max_num_tokens\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                tokens_b = []\n",
    "\n",
    "        # Random next\n",
    "\n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = rng.randint(0,\n",
    "                                len(all_documents) - 1)\n",
    "                        if random_document_index != document_index:\n",
    "                            break\n",
    "\n",
    "                    random_document = \\\n",
    "                        all_documents[random_document_index]\n",
    "                    random_start = rng.randint(0, len(random_document)\n",
    "                            - 1)\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                else:\n",
    "\n",
    "        # Actual next\n",
    "\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens,\n",
    "                                  rng)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                tokens.append('[CLS]')\n",
    "                segment_ids.append(0)\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                attention_mask = [1] * len(tokens)\n",
    "                \n",
    "                while len(tokens) < max_seq_length:\n",
    "                    tokens.append('[PAD]')\n",
    "                    segment_ids.append(0)\n",
    "                    attention_mask.append(0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                original_tokens = tokens\n",
    "                (tokens, masked_lm_positions, masked_lm_labels) = \\\n",
    "                    create_masked_lm_predictions(tokens,\n",
    "                        masked_lm_prob, max_predictions_per_seq,\n",
    "                        vocab_words, rng)\n",
    "                \n",
    "                instance = TrainingInstance(tokens=tokens,\n",
    "                        segment_ids=segment_ids,\n",
    "                        is_random_next=is_random_next,\n",
    "                        masked_lm_positions=masked_lm_positions,\n",
    "                        masked_lm_labels=masked_lm_labels,\n",
    "                        attention_mask = attention_mask,\n",
    "                        original_tokens = original_tokens)\n",
    "                \n",
    "                instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "#     print(instances)\n",
    "    pretrain_instance.extend(instances)\n",
    "\n",
    "print(len(pretrain_instance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '人', '團', '聚', '，', '讓', '她', '笑', '說', '：', '「', '回', '去', '當', '然', '是', '好', '好', '當', '公', '主', '呀', '。', '媽', '媽', '、', '阿', '姨', '們', '輪', '流', '煮', '飯', '給', '我', '吃', '，', '還', '得', '與', '大', '家', '夜', '聊', '到', '天', '明', '，', '真', '的', '比', '工', '[MASK]', '還', '累', '。', '」', '孝', '順', '的', '她', '更', '樂', '當', '財', '神', '爺', '，', '發', '出', '超', '過', '3', '0', '包', '紅', '包', '給', '親', '戚', '家', '人', '。', '除', '了', '回', '鄉', '[MASK]', '啖', '媽', '媽', '傳', '家', '菜', '、', '原', '住', '民', '傳', '統', '料', '理', '，', '戴', '愛', '玲', '還', '揪', '團', '到', '部', '落', '投', '幣', '式', '[MASK]', 'T', 'V', '歡', '唱', '。', '5', '月', '2', '3', '日', '將', '首', '度', '站', '上', '台', '北', '國', '際', '會', '議', '中', '心', '舉', '辦', '《', '愛', '戴', '2', '0', '2', '0', '》', '演', '唱', '會', '，', '戴', '愛', '玲', '這', '次', '回', '鄉', '也', '趁', '機', '會', '向', '族', '人', '持', '續', '學', '習', '排', '灣', '[MASK]', '古', '調', '，', '希', '望', '有', '機', '會', '能', '在', '餮', '來', '音', '樂', '演', '出', '融', '合', '。', '至', '於', '廚', '藝', '是', '否', '[MASK]', '精', '進', '。', '她', '自', '招', '：', '「', '都', '是', '家', '人', '煮', '給', '我', '吃', '，', '目', '前', '對', '廚', '藝', '真', '的', '沒', '##酷', '趣', '，', '要', '遇', '到', '對', '的', '人', '才', '會', '願', '意', '下', '廚', '哈', '哈', '。', '」', '新', '春', '碰', '上', '武', '漢', '肺', '炎', '疫', '情', '來', '襲', '，', '這', '回', '她', '與', '家', '人', '到', '自', '家', '咖', '啡', '莊', '園', '，', '墾', '丁', '海', '邊', '走', '春', '全', '程', '配', '戴', '口', '罩', '防', '疫', '，', '而', '[SEP]', '謝', '龍', '介', '在', '選', '舉', '期', '間', '發', '下', '豪', '語', '，', '韓', '國', '瑜', '沒', '贏', '[MASK]', '0', '[MASK]', '票', '，', '就', '會', '##實', '台', '北', '、', '高', '[MASK]', '、', '台', '南', '發', '出', '9', '0', '0', '份', '雞', '排', '，', '許', '[MASK]', '網', '友', '覺', '得', '韓', '國', '瑜', '不', '可', '能', '贏', '這', '麼', '多', '，', '敲', '碗', '等', '著', '謝', '龍', '介', '發', '雞', '排', '，', '不', '過', '同', '時', '也', '引', '起', '質', '疑', '，', '是', '另', '一', '種', '買', '票', '的', '行', '為', '，', '謝', '龍', '介', '對', '此', '也', '改', '口', '，', '把', '雞', '排', '換', '成', '[MASK]', '款', '，', '要', '讓', '韓', '國', '瑜', '贏', '得', '乾', '乾', '淨', '淨', '。', '不', '過', '檢', '方', '也', '在', '這', '個', '時', '候', '收', '到', '民', '眾', '檢', '舉', '情', '[MASK]', '，', '對', '於', '雞', '排', '與', '選', '票', '之', '間', '的', '關', '係', '覺', '得', '很', '曖', '昧', '，', '由', '於', '相', '關', '法', '[MASK]', '並', '未', '明', '確', '指', '定', '選', '舉', '小', '物', '的', '金', '額', '，', '導', '致', '在', '選', '舉', '期', '間', '許', '多', '參', '選', '人', '[MASK]', '贈', '品', '容', '易', '被', '檢', '舉', '賄', '選', '[MASK]', '例', '[MASK]', '標', '示', '名', '字', '的', '礦', '泉', '水', '、', '面', '紙', '，', '都', '成', '為', '檢', '舉', '的', '對', '象', '。', '[SEP]']\n",
      "['[CLS]', '人', '團', '聚', '，', '讓', '她', '笑', '說', '：', '「', '回', '去', '當', '然', '是', '好', '好', '當', '公', '主', '呀', '。', '媽', '媽', '、', '阿', '姨', '們', '輪', '流', '煮', '飯', '給', '我', '吃', '，', '還', '得', '與', '大', '家', '夜', '聊', '到', '天', '明', '，', '真', '的', '比', '工', '作', '還', '累', '。', '」', '孝', '順', '的', '她', '更', '樂', '當', '財', '神', '爺', '，', '發', '出', '超', '過', '3', '0', '包', '紅', '包', '給', '親', '戚', '家', '人', '。', '除', '了', '回', '鄉', '大', '啖', '媽', '媽', '傳', '家', '菜', '、', '原', '住', '民', '傳', '統', '料', '理', '，', '戴', '愛', '玲', '還', '揪', '團', '到', '部', '落', '投', '幣', '式', 'K', 'T', 'V', '歡', '唱', '。', '5', '月', '2', '3', '日', '將', '首', '度', '站', '上', '台', '北', '國', '際', '會', '議', '中', '心', '舉', '辦', '《', '愛', '戴', '2', '0', '2', '0', '》', '演', '唱', '會', '，', '戴', '愛', '玲', '這', '次', '回', '鄉', '也', '趁', '機', '會', '向', '族', '人', '持', '續', '學', '習', '排', '灣', '族', '古', '調', '，', '希', '望', '有', '機', '會', '能', '在', '未', '來', '音', '樂', '演', '出', '融', '合', '。', '至', '於', '廚', '藝', '是', '否', '有', '精', '進', '。', '她', '自', '招', '：', '「', '都', '是', '家', '人', '煮', '給', '我', '吃', '，', '目', '前', '對', '廚', '藝', '真', '的', '沒', '興', '趣', '，', '要', '遇', '到', '對', '的', '人', '才', '會', '願', '意', '下', '廚', '哈', '哈', '。', '」', '新', '春', '碰', '上', '武', '漢', '肺', '炎', '疫', '情', '來', '襲', '，', '這', '回', '她', '與', '家', '人', '到', '自', '家', '咖', '啡', '莊', '園', '，', '墾', '丁', '海', '邊', '走', '春', '全', '程', '配', '戴', '口', '罩', '防', '疫', '，', '而', '[SEP]', '謝', '龍', '介', '在', '選', '舉', '期', '間', '發', '下', '豪', '語', '，', '韓', '國', '瑜', '沒', '贏', '9', '0', '萬', '票', '，', '就', '會', '在', '台', '北', '、', '高', '雄', '、', '台', '南', '發', '出', '9', '0', '0', '份', '雞', '排', '，', '許', '多', '網', '友', '覺', '得', '韓', '國', '瑜', '不', '可', '能', '贏', '這', '麼', '多', '，', '敲', '碗', '等', '著', '謝', '龍', '介', '發', '雞', '排', '，', '不', '過', '同', '時', '也', '引', '起', '質', '疑', '，', '是', '另', '一', '種', '買', '票', '的', '行', '為', '，', '謝', '龍', '介', '對', '此', '也', '改', '口', '，', '把', '雞', '排', '換', '成', '捐', '款', '，', '要', '讓', '韓', '國', '瑜', '贏', '得', '乾', '乾', '淨', '淨', '。', '不', '過', '檢', '方', '也', '在', '這', '個', '時', '候', '收', '到', '民', '眾', '檢', '舉', '情', '資', '，', '對', '於', '雞', '排', '與', '選', '票', '之', '間', '的', '關', '係', '覺', '得', '很', '曖', '昧', '，', '由', '於', '相', '關', '法', '令', '並', '未', '明', '確', '指', '定', '選', '舉', '小', '物', '的', '金', '額', '，', '導', '致', '在', '選', '舉', '期', '間', '許', '多', '參', '選', '人', '的', '贈', '品', '容', '易', '被', '檢', '舉', '賄', '選', '，', '例', '如', '標', '示', '名', '字', '的', '礦', '泉', '水', '、', '面', '紙', '，', '都', '成', '為', '檢', '舉', '的', '對', '象', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(pretrain_instance[0].tokens)\n",
    "print(pretrain_instance[0].original_tokens)\n",
    "# print(pretrain_instance[0].attention_mask)\n",
    "# print(pretrain_instance[0].segment_ids)\n",
    "# print(pretrain_instance[0].is_random_next)\n",
    "# print(pretrain_instance[0].masked_lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, pretrain_instance,tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.pretrain_instance = pretrain_instance\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer.encode(self.pretrain_instance[idx].tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        original_input_ids = self.tokenizer.encode(self.pretrain_instance[idx].original_tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        masked_lm_labels_ids = self.tokenizer.encode(self.pretrain_instance[idx].masked_lm_labels,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "\n",
    "        token_type_ids = torch.tensor(self.pretrain_instance[idx].segment_ids)\n",
    "        attention_mask = torch.tensor(self.pretrain_instance[idx].attention_mask)\n",
    "        is_random_next = torch.tensor(self.pretrain_instance[idx].is_random_next)\n",
    "        return input_ids, token_type_ids, attention_mask,  is_random_next , original_input_ids\n",
    "    def __len__(self):\n",
    "        return len(self.pretrain_instance)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "trainset = PreTrainDataset(pretrain_instance,tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in trainloader:\n",
    "#     input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "#     input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "#     masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "#     print(masked_lm_labels.size())\n",
    "#     print(masked_lm_labels)\n",
    "# #     masked_lm_labels_ids = torch.reshape(masked_lm_labels_ids,(masked_lm_labels_ids.size()[0],masked_lm_labels_ids.size()[2]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ./chinese_wwm_pytorch/ and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] 11060/15322 Loss: 0.3172 totaloss: 206.94249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining\n",
    "model = BertForPreTraining.from_pretrained(pretrain_model_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for data in trainloader:\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "        input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "        masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         print(input_ids.size())\n",
    "#         print(token_type_ids.size())\n",
    "#         print(attention_mask.size())\n",
    "#         print(is_random_next.size())\n",
    "#         print(masked_lm_labels.size())\n",
    "        i += (input_ids.size()[0])\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels = masked_lm_labels,\n",
    "                        next_sentence_label = is_random_next.long())\n",
    "\n",
    "        loss = outputs[0]\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {loss.item():.4f} totaloss: {running_loss:.4f}', end='')\n",
    "    model.save_pretrained('./bert_pretrain_news/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
