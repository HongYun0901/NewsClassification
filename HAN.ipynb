{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[0 0 2 ... 3 6 2]\n",
      "(35546, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector,Input\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Add,Concatenate,BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#參數\n",
    "MAX_SENT_LENGTH = 8\n",
    "MAX_SENTS = 128\n",
    "EMBEDDING_DIM = 250\n",
    "\n",
    "\n",
    "## label data\n",
    "\n",
    "print (keras.__version__)\n",
    "\n",
    "column_names = ['type','title','text']\n",
    "df = pd.read_csv('./all_after_mapping.tsv',sep='\\t',names=column_names)\n",
    "\n",
    "labels = df['type'].values\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "labels = np_utils.to_categorical(labels)\n",
    "print(labels.shape)\n",
    "tokenlizeword = np.load('./tokenlizeword0221.npy',allow_pickle=True)\n",
    "u_tokenlizeword = np.load('./tokenlizeword0226_udn_for_mct.npy',allow_pickle=True)\n",
    "#串起來\n",
    "tokenlizeword=np.concatenate((tokenlizeword, u_tokenlizeword), axis=0)\n",
    "\n",
    "wmodel = Word2Vec(tokenlizeword, size=EMBEDDING_DIM, window=10, min_count=20)\n",
    "# wmodel=Word2Vec.load('./word2vec.model')\n",
    "# wmodel.most_similar('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32947\n"
     ]
    }
   ],
   "source": [
    "vocab = wmodel.wv.vocab\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46237\n",
      "embedding done\n"
     ]
    }
   ],
   "source": [
    "#x_train=文章數*最大句數*最大字數*300維\n",
    "x_train =[] #label data\n",
    "##文章數\n",
    "for k in range(tokenlizeword.shape[0]):\n",
    "    embedding_matrix=np.zeros((MAX_SENTS,MAX_SENT_LENGTH,EMBEDDING_DIM))\n",
    "    word_count=0 ##計算目前位置\n",
    "    ##句子數\n",
    "    for i in range(MAX_SENTS):\n",
    "        if word_count >= len(tokenlizeword[k]):\n",
    "            break\n",
    "        embedding_vector=np.zeros((MAX_SENT_LENGTH,EMBEDDING_DIM))\n",
    "        ##最大字數\n",
    "        for j in range(MAX_SENT_LENGTH):\n",
    "            if tokenlizeword[k][word_count] == '，' or tokenlizeword[k][word_count] == '。' or tokenlizeword[k][word_count] == '！' or tokenlizeword[k][word_count] == '？':\n",
    "                word_count=word_count+1\n",
    "                break\n",
    "            else:\n",
    "                if tokenlizeword[k][word_count] in vocab: ##在字典裡\n",
    "                    embedding_vector[j]=wmodel[tokenlizeword[k][word_count]]\n",
    "                word_count=word_count+1\n",
    "                if word_count >= len(tokenlizeword[k]):\n",
    "                    break\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    x_train.append(embedding_matrix)\n",
    "\n",
    "print(len(x_train))\n",
    "print(\"embedding done\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 128, 8, 250)\n",
      "(5000, 128, 8, 250)\n",
      "(25546, 128, 8, 250)\n",
      "60%\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train,dtype = 'float32')\n",
    "\n",
    "\n",
    "dev_x = x_train[:5000]\n",
    "dev_y = labels[:5000]\n",
    "\n",
    "test_x = x_train[5000:10000]\n",
    "test_y = labels[5000:10000]\n",
    "#到35546為止為label data\n",
    "train_x = x_train[10000:35546]\n",
    "\n",
    "train_y = labels[10000:]\n",
    "# !ls\n",
    "print(dev_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_x.shape)\n",
    "\n",
    "print(\"60%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意層\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = 50\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], 1)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "            \n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'attention_25/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_25/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_25/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'attention_26/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_26/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_26/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 128, 8, 250)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_38 (TimeDis (None, 128, 100)          230900    \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 128, 200)          120600    \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, 128, 100)          20100     \n",
      "_________________________________________________________________\n",
      "attention_26 (Attention)     (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 372,507\n",
      "Trainable params: 372,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model fitting - Hierachical attention network\n",
      "Train on 25546 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "25546/25546 [==============================] - 228s 9ms/step - loss: 0.4328 - acc: 0.8515 - val_loss: 0.7242 - val_acc: 0.8026\n",
      "Epoch 2/3\n",
      "25546/25546 [==============================] - 226s 9ms/step - loss: 0.2644 - acc: 0.9092 - val_loss: 0.7264 - val_acc: 0.7982\n",
      "Epoch 3/3\n",
      "25546/25546 [==============================] - 227s 9ms/step - loss: 0.2211 - acc: 0.9217 - val_loss: 0.7759 - val_acc: 0.7878\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "l_dense = TimeDistributed(Dense(100))(l_lstm)\n",
    "l_att = Attention()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(100))(l_lstm_sent)\n",
    "l_att_sent = Attention()(l_dense_sent)\n",
    "preds = Dense(7, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "adam = keras.optimizers.Adam(lr=4e-4, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "filepath = \"./HAN2e_4.h5\"\n",
    " \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.summary()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(train_x, train_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=32,callbacks = callbacks_list)\n",
    "# model.save_weights('HAN2_para.h5') ##存參數\n",
    "model.save('HAN3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('HAN1.h5')\n",
    "# sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "# l_lstm = Bidirectional(GRU(300, return_sequences=True))(sentence_input)\n",
    "# l_att = AttLayer(300)(l_lstm)\n",
    "# sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "# review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "# review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# # l_drop1 = Dropout(0.3)(review_encoder)\n",
    "# # l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "# dropout = Dropout(0.2)(review_encoder)\n",
    "# filter_sizes = [3,4,5]\n",
    "# convs = []\n",
    "# for filter_size in filter_sizes:\n",
    "#     conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n",
    "#     pool = MaxPooling1D(filter_size)(conv)\n",
    "#     convs.append(pool)\n",
    "        \n",
    "# concatenate = Concatenate(axis=1)(convs)\n",
    "# drop_out = Dropout(0.1)(concatenate)\n",
    "# l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(drop_out)\n",
    "# preds = Dense(7, activation='softmax')(l_lstm_sent)\n",
    "# l_att_sent = AttLayer(100)(preds)\n",
    "\n",
    "# model = Model(review_input, l_att_sent)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='Adam',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "\n",
    "# filepath = \"best_HAHNN.h5\"\n",
    " \n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "# print(\"model fitting - Hierachical attention network\")\n",
    "# model.fit(test_x, test_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=4,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(test_x,test_y,model):\n",
    "    test_y = np.array(test_y,dtype ='int64')\n",
    "    total = len(test_x)\n",
    "    logits=model.predict(test_x,batch_size=50)\n",
    "    pred=np.argmax(logits, axis=1)\n",
    "    correct=0 #對的有幾個\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    total_labels = np.array(total_labels)\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    for i in range(len(test_x)):\n",
    "        total_labels += test_y[i]\n",
    "        if test_y[i][pred[i]]==1.0:\n",
    "            correct = correct+1\n",
    "            correct_labels[pred[i]] += 1\n",
    "    for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "    print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vote_acc(test_x,test_y,model,model2,model3):\n",
    "    test_y = np.array(test_y,dtype ='int64')\n",
    "    total = len(test_x)\n",
    "    logits=model.predict(test_x,batch_size=50)\n",
    "    logits2=model2.predict(test_x,batch_size=50)\n",
    "    logits3=model3.predict(test_x,batch_size=50)\n",
    "    pred=np.argmax(logits, axis=1)\n",
    "    pred2=np.argmax(logits2, axis=1)\n",
    "    pred3=np.argmax(logits3, axis=1)\n",
    "    correct=0 #對的有幾個\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    total_labels = np.array(total_labels)\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    for i in range(len(test_x)):\n",
    "        count = 0\n",
    "        total_labels += test_y[i]\n",
    "        if test_y[i][pred[i]]==1.0:\n",
    "            count+=1\n",
    "        if test_y[i][pred2[i]]==1.0:\n",
    "            count+=1\n",
    "        if test_y[i][pred3[i]]==1.0:\n",
    "            count+=1\n",
    "        if count>=2:\n",
    "            correct = correct+1\n",
    "            if pred[i] == pred2[i]:\n",
    "                correct_labels[pred[i]] += 1\n",
    "            elif pred[i] == pred3[i]:\n",
    "                correct_labels[pred[i]] += 1\n",
    "            elif pred2[i] == pred3[i]:\n",
    "                correct_labels[pred2[i]] += 1\n",
    "    for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "    print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 有bug\n",
    "def test_acc_two_ans(test_x,test_y,model):\n",
    "    test_y = np.array(test_y,dtype ='int64')\n",
    "    total = len(test_x)\n",
    "    logits=model.predict(test_x,batch_size=50)\n",
    "    pred=np.argmax(logits, axis=1)\n",
    "    for i in range(len(logits)):\n",
    "        logits[i][pred[i]] = 0.0\n",
    "    pred_sec = np.argmax(logits,axis=1)\n",
    "    correct=0 #對的有幾個\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    total_labels = np.array(total_labels)\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    for i in range(len(test_x)):\n",
    "        total_labels += test_y[i]\n",
    "        if test_y[i][pred[i]]==1.0 or test_y[i][pred_sec[i]]==1.0:\n",
    "            correct = correct+1\n",
    "            correct_labels+= test_y[i]\n",
    "    for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "    print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'attention_21_3/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_21_3/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_21_3/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'attention_22_3/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_22_3/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_22_3/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'attention_23_2/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_23_2/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_23_2/Variable_2:0' shape=(50, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'attention_24_2/Variable:0' shape=(100, 1) dtype=float32> W\n",
      "tracking <tf.Variable 'attention_24_2/Variable_1:0' shape=(50,) dtype=float32> b\n",
      "tracking <tf.Variable 'attention_24_2/Variable_2:0' shape=(50, 1) dtype=float32> u\n"
     ]
    }
   ],
   "source": [
    "# model2 = load_model('HAN3e_4.h5')\n",
    "model2 = load_model('HAN1.h5',custom_objects={'Attention': \n",
    "Attention})\n",
    "model3 = load_model('HAN2.h5',custom_objects={'Attention': \n",
    "Attention})\n",
    "# model3 = Model(review_input, preds)\n",
    "# model3 = model3.load_weights('HAN3e_4_decay0_para.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "政治 : 809 / 1104 Acc : 0.7327898550724637\n",
      "生活 : 895 / 1107 Acc : 0.8084914182475158\n",
      "國際 : 354 / 424 Acc : 0.8349056603773585\n",
      "體育 : 353 / 359 Acc : 0.9832869080779945\n",
      "娛樂 : 632 / 653 Acc : 0.9678407350689127\n",
      "社會 : 346 / 453 Acc : 0.7637969094922737\n",
      "財經 : 786 / 900 Acc : 0.8733333333333333\n",
      "Total： 0.835\n"
     ]
    }
   ],
   "source": [
    "test_vote_acc(test_x,test_y,model,model2,model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "政治 : 832 / 1104 Acc : 0.7536231884057971\n",
      "生活 : 830 / 1107 Acc : 0.7497741644083108\n",
      "國際 : 352 / 424 Acc : 0.8301886792452831\n",
      "體育 : 355 / 359 Acc : 0.9888579387186629\n",
      "娛樂 : 640 / 653 Acc : 0.9800918836140888\n",
      "社會 : 353 / 453 Acc : 0.7792494481236203\n",
      "財經 : 748 / 900 Acc : 0.8311111111111111\n",
      "Total： 0.822\n"
     ]
    }
   ],
   "source": [
    "print('model1')\n",
    "test_acc(test_x,test_y,model)\n",
    "# print('model2')\n",
    "# test_acc(test_x,test_y,model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits=model.predict(test_x,batch_size=50)\n",
    "pred=np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4104\n",
      "0.8208\n"
     ]
    }
   ],
   "source": [
    "#驗證還沒加入unlabel data的表現\n",
    "correct=0 #對的有幾個\n",
    "for i in range(5000):\n",
    "    if test_y[i][pred[i]]==1.0:\n",
    "        correct = correct+1\n",
    "print(correct)\n",
    "acc=correct/5000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##預測\n",
    "u_train = x_train[35546:]\n",
    "u_pred = model.predict(u_train,batch_size=50)\n",
    "ary =[] ##準備刪除的所有資料位置\n",
    "for i in range(u_train.shape[0]):\n",
    "    ans = u_pred[i][0]\n",
    "    for j in range(7):\n",
    "        if u_pred[i][j]>ans:\n",
    "            ans=u_pred[i][j]\n",
    "    if ans<acc: ##比模型準確率高\n",
    "        ary.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3992579e-01 2.1731112e-02 8.2282134e-04 7.1054637e-01 4.6229051e-04\n",
      " 2.6327234e-02 1.8443231e-04]\n",
      "(10691, 7)\n",
      "[8.8439238e-01 5.6384900e-04 5.2494352e-06 9.5021096e-06 1.8242728e-06\n",
      " 1.1501102e-01 1.6204893e-05]\n",
      "(6374, 7)\n"
     ]
    }
   ],
   "source": [
    "print(u_pred[3])\n",
    "print(u_pred.shape)\n",
    "u_pred=np.delete(u_pred,ary,axis=0) ##刪除所有資料不夠好的位置\n",
    "print(u_pred[3])\n",
    "print(u_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 44.9 GiB for an array with shape (39218, 64, 8, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a76b29b958a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mary_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m35546\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mary_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4415\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4417\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 44.9 GiB for an array with shape (39218, 64, 8, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "ary_label = []##刪除x_train用ㄉ\n",
    "\n",
    "for i in range(len(ary)):\n",
    "    ary_label.append(ary[i]+35546)\n",
    "x_train = np.delete(x_train,ary_label,axis=0)\n",
    "print(x_train.shape)\n",
    "u_pred = np.argmax(u_pred, axis=1)\n",
    "##轉one-hot\n",
    "u_pred = np.array(u_pred)\n",
    "u_pred = np_utils.to_categorical(u_pred)\n",
    "print(u_pred.shape)\n",
    "print(u_pred[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = np.argmax(u_pred, axis=1)\n",
    "##轉one-hot\n",
    "u_pred = np.array(u_pred)\n",
    "u_pred = np_utils.to_categorical(u_pred)\n",
    "print(u_pred.shape)\n",
    "print(u_pred[7])\n",
    "train_y=labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#串起來\n",
    "train_y=np.concatenate((train_y,u_pred),axis=0)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再train一個model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(sentence_input)\n",
    "dropout = Dropout(0.2)(l_lstm)\n",
    "filter_sizes = [3,4,5]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n",
    "    pool = MaxPooling1D(filter_size)(conv)\n",
    "    convs.append(pool)\n",
    "        \n",
    "concatenate = Concatenate(axis=1)(convs)\n",
    "drop_out = Dropout(0.1)(concatenate)\n",
    "l_att = AttLayer(100)(drop_out)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# l_drop1 = Dropout(0.3)(review_encoder)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(7, activation='softmax')(l_att_sent)\n",
    "u_model = Model(review_input, preds)\n",
    "\n",
    "u_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "filepath = \"best_HAN_with_unlabel.h5\"\n",
    " \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "train_x=x_train[10000:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "u_model.summary()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "u_model.fit(train_x, train_y, validation_data=(dev_x,dev_y),epochs=3, batch_size=4,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "pred=u_model.predict(test_x,batch_size=50)\n",
    "pred=np.argmax(pred, axis=1)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176\n",
      "0.8352\n"
     ]
    }
   ],
   "source": [
    "#加入unlabel data後的表現\n",
    "correct=0 #對的有幾個\n",
    "for i in range(5000):\n",
    "    if test_y[i][pred[i]]==1.0:\n",
    "        correct = correct+1\n",
    "print(correct)\n",
    "acc=correct/5000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
