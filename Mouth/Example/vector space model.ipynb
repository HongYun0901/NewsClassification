{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file():\n",
    "    query = {}\n",
    "    doc = {}\n",
    "# ntust-ir-2020\n",
    "    with open('./query_list.txt', 'r') as file:\n",
    "        query_list = file.read()\n",
    "        for file_name in query_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'./queries/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    query[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    with open('./doc_list.txt', 'r') as file:\n",
    "        doc_list = file.read()\n",
    "        for file_name in doc_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'./docs/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    doc[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    print(len(query))\n",
    "    print(len(doc))\n",
    "    \n",
    "    return query, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_df(doc_dict):\n",
    "    word_list = {}\n",
    "    all_word = []\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        word_list[doc_name] = {}\n",
    "        for word in set(doc.split(' ')):\n",
    "            word_list[doc_name][word] = ''\n",
    "            all_word.append(word)\n",
    "        all_word.extend(list(set(word_list)))\n",
    "\n",
    "    df_dict = {}\n",
    "    for word in list(set(all_word)):      # 計算 df\n",
    "        freq = 0\n",
    "        for doc_name in word_list.keys():\n",
    "            if word_list[doc_name].get(word, ''):\n",
    "                freq = freq + 1\n",
    "\n",
    "        df_dict[word] = freq\n",
    "        \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(df_dict, doc_dict, N):\n",
    "    tf_dict = {}   # tf[doc][word]\n",
    "    idf_dict = {}  # idf[word]\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        tf_dict[doc_name] = {}\n",
    "        for word in doc.split(' '):\n",
    "            if tf_dict[doc_name].get(word, 0):  # 計算 tf\n",
    "                tf_dict[doc_name][word] += 1\n",
    "            else:\n",
    "                tf_dict[doc_name][word] = 1\n",
    "    \n",
    "    for word, df_freq in df_dict.items():  # 計算 idf\n",
    "        idf_dict[word] = math.log(((N+1) / (df_freq+1)) + 1)  # smooth 公式\n",
    "        \n",
    "    return tf_dict, idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tf_idf(tf, idf, doc_dict):\n",
    "    tf_idf_dict = {}  # tf_idf[doc][word]\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        tf_idf_dict[doc_name] = {}\n",
    "        for word in doc.split(' '):\n",
    "            tf_idf_dict[doc_name][word] = (1 + math.log(tf[doc_name][word])) * idf[word]\n",
    "    # L2 normalize\n",
    "#     tf_idf_dict = normalize( )\n",
    "    return tf_idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vectorSpaceModel(query, doc_dict, tf_idf, idf):\n",
    "    query_vec = [idf[word] if idf.get(word, 0) else 0 for word in query.split(' ')] # 將 word 轉成 score\n",
    "    score_dict = {}\n",
    "#     print(query_vec)\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        doc_vec = [tf_idf[doc_name][word] if tf_idf[doc_name].get(word, 0) else 0 for word in query.split(' ')] # 將 word 轉成 score\n",
    "        score_dict[doc_name] = np.dot(query_vec, doc_vec) / len(query_vec)\n",
    "#         print(doc_vec)\n",
    "#         score_dict[doc_name] = cosine_similarity([query_vec], [doc_vec])\n",
    "#         print(score_dict[doc_name])\n",
    "    \n",
    "    rank = sorted(score_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './queries/.txt'\n",
      "[Errno 2] No such file or directory: './docs/.txt'\n",
      "50\n",
      "4191\n",
      "['intern', 'organ', 'crime']\n"
     ]
    }
   ],
   "source": [
    "query_dict, doc_dict = open_file()\n",
    "print(query_dict['301'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "all_voc = {} ## 字典\n",
    "all_split_list = [] ##所有斷詞後的結果\n",
    "for i in (doc_dict.values()):\n",
    "    all_split_list.append(i.split())\n",
    "    \n",
    "## 初始化字典\n",
    "for i in range(len(all_split_list)):\n",
    "    for j in range(len(all_split_list[i])):\n",
    "        all_voc[all_split_list[i][j]] = 0.0\n",
    "print(all_voc['crime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = {}\n",
    "all_word = []\n",
    "all_voc = {}\n",
    "for doc_name, doc in doc_dict.items():\n",
    "    word_list[doc_name] = {}\n",
    "    for word in set(doc.split(' ')):\n",
    "        word_list[doc_name][word] = ''\n",
    "        all_word.append(word)\n",
    "    all_word.extend(list(set(word_list)))\n",
    "print(all_word)\n",
    "df_dict = {}\n",
    "for word in list(set(all_word)):      # 計算 df\n",
    "    freq = 0\n",
    "    for doc_name in word_list.keys():\n",
    "        if word_list[doc_name].get(word, ''):\n",
    "            freq = freq + 1\n",
    "\n",
    "    df_dict[word] = freq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df = cal_df(doc_dict)\n",
    "print(df['crime'])\n",
    "tf, idf = cal(df, doc_dict, len(doc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = cal_tf_idf(tf, idf, doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "f = open('ans.txt', 'w')\n",
    "string = 'Query,RetrievedDocuments\\n'\n",
    "\n",
    "for _id, _query in query_dict.items():\n",
    "    rank = vectorSpaceModel(_query, doc_dict, tf_idf, idf)\n",
    "\n",
    "    string += _id + ','\n",
    "    for doc in rank:\n",
    "        string += doc[0] + ' '\n",
    "    string += '\\n'\n",
    "    \n",
    "f.write(string)\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
