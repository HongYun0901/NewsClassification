{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW \n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "test = pd.read_csv('./sample_submission.csv')\n",
    "model_path = './bert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download()\n",
    "words = stopwords.words('english')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_csv('./documents.csv')\n",
    "doc_list = doc['doc_id'].tolist()\n",
    "doc_text = doc['doc_text'].tolist()\n",
    "doc_dict = {doc_list[i]: i for i in range(len(doc_list))}\n",
    "# print(len(doc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_q = pd.read_csv('./train_queries.csv')\n",
    "train_query_data = [t.lower() for t in train_q['query_text']]\n",
    "train_top_1000 = [t.split() for t in train_q['bm25_top1000']]\n",
    "train_pos_list = [t.split() for t in train_q['pos_doc_ids']]\n",
    "train_neg_list = [t[:] for t in train_top_1000] \n",
    "train_pos_list = np.array(train_pos_list)\n",
    "train_top_1000 = np.array(train_top_1000)\n",
    "train_neg_list = np.array(train_neg_list) \n",
    "# print(train_q)\n",
    "# print(train_top_1000[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 裡面現在存的是pos doc idx\n",
    "for pos_docs in range(train_pos_list.shape[0]):\n",
    "    for idx in range(len(train_pos_list[pos_docs])):\n",
    "        train_pos_list[pos_docs][idx] = doc_dict[train_pos_list[pos_docs][idx]]\n",
    "        \n",
    "for pos_docs in range(train_top_1000.shape[0]):\n",
    "    for idx in range(len(train_top_1000[pos_docs])):\n",
    "        train_top_1000[pos_docs][idx] = doc_dict[train_top_1000[pos_docs][idx]]\n",
    "        \n",
    "for pos_docs in range(train_neg_list.shape[0]):\n",
    "    for idx in range(len(train_neg_list[pos_docs])):\n",
    "        train_neg_list[pos_docs][idx] = doc_dict[train_neg_list[pos_docs][idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    result = \"\"\n",
    "    content = content.replace('\\n','.').replace('\\t',',')\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content=  content.lower()\n",
    "    stopword = {}\n",
    "    for i in stopwords.words('english'):\n",
    "        stopword[i] = 0\n",
    "    for word in content.split():\n",
    "        if word not in stopword.keys():\n",
    "            result += word\n",
    "            result += \" \"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return input_id, tokentype, attentionmask, y\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_question = []\n",
    "train_choice = []\n",
    "train_y = []\n",
    "N = 3 ## 錯的文章數量\n",
    "for i in range(len(train_query_data)):\n",
    "    if len(train_pos_list[i]) <50:\n",
    "        N = 10\n",
    "    else:\n",
    "        N = 3\n",
    "    for j in range(len(train_pos_list[i])): ##幾篇POS\n",
    "        if type(doc_text[train_pos_list[i][j]]) != float:\n",
    "            train_y.append(1)\n",
    "            train_question.append(train_query_data[i])\n",
    "            train_choice.append(clean_string(doc_text[train_pos_list[i][j]])) ##對的\n",
    "#             upper_bound = (N+1)*len(train_pos_list[i])\n",
    "#             if upper_bound>=999:\n",
    "#                 upper_bound = 999\n",
    "            l = random.sample(range(0,999), N)\n",
    "            count = -1\n",
    "            while(count < N): \n",
    "                if count != -1:\n",
    "                    l = random.sample(range(0,999), N)\n",
    "                count = 0 \n",
    "                for check in l:\n",
    "                    if int(train_neg_list[i][check]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][check])]) != float: ##不是POS\n",
    "                        count += 1    \n",
    "            for idx in l:\n",
    "                train_y.append(0)\n",
    "                train_question.append(train_query_data[i])\n",
    "                train_choice.append(clean_string(doc_text[int(train_neg_list[i][idx])]))\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_question = []\n",
    "# train_choice = []\n",
    "# train_y = []\n",
    "# R = 1 ##偽相關的數量\n",
    "# N = 2 ## 錯的文章數量\n",
    "# for i in range(len(train_query_data)):\n",
    "# #     print(i)\n",
    "#     for j in range(len(train_pos_list[i])): ##幾篇POS\n",
    "#         if type(doc_text[train_pos_list[i][j]]) != float:\n",
    "#             train_y.append(2) ##代表相關\n",
    "#             train_question.append(train_query_data[i])\n",
    "#             train_choice.append(clean_string(doc_text[train_pos_list[i][j]])) ##對的\n",
    "# #             upper_bound = (N+1)*len(train_pos_list[i])\n",
    "# #             if upper_bound>=999:\n",
    "# #                 upper_bound = 999\n",
    "#             n = random.sample(range(0,99),R)\n",
    "#             flag = True\n",
    "#             while(flag):\n",
    "#                 if int(train_neg_list[i][n[0]]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][n[0]])]) != float: ##不是POS\n",
    "#                     flag = False  \n",
    "#                 else:\n",
    "#                     n = random.sample(range(0,99),R)\n",
    "#             train_y.append(1)\n",
    "#             train_question.append(train_query_data[i])\n",
    "#             train_choice.append(clean_string(doc_text[int(train_neg_list[i][n[0]])]))\n",
    "#             l = random.sample(range(100,999), N)\n",
    "#             count = -1\n",
    "#             while(count < N): \n",
    "#                 if count != -1:\n",
    "#                     l = random.sample(range(100,999), N)\n",
    "#                 count = 0 \n",
    "#                 for check in l:\n",
    "#                     if int(train_neg_list[i][check]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][check])]) != float: ##不是POS\n",
    "#                         count += 1    \n",
    "#             for idx in l:\n",
    "#                 train_y.append(0)\n",
    "#                 train_question.append(train_query_data[i])\n",
    "#                 train_choice.append(clean_string(doc_text[int(train_neg_list[i][idx])]))\n",
    "# train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poliomyelitis and post-polio\n",
      "46743\n",
      "46743\n",
      "[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
      "(46743,)\n"
     ]
    }
   ],
   "source": [
    "print(train_question[0])\n",
    "print(len(train_question))\n",
    "print(len(train_choice))\n",
    "print(train_y[:100])\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 備份\n",
    "# train_question = []\n",
    "# train_choice = []\n",
    "# train_y = []\n",
    "# N = 3 ## 錯的文章數量\n",
    "# for i in range(len(train_query_data)):\n",
    "#     for j in range(len(train_pos_list[i])):\n",
    "#         if type(doc_text[train_pos_list[i][j]]) != float:\n",
    "#             query = [\"\",\"\",\"\",\"\"]\n",
    "#             choice = [\"\",\"\",\"\",\"\"]\n",
    "#             ans_pos = random.randint(0,3) ##隨機給正確答案的位置\n",
    "#             train_y.append(ans_pos)\n",
    "#             query[ans_pos] = train_query_data[i]\n",
    "#             choice[ans_pos] = clean_string(doc_text[train_pos_list[i][j]])\n",
    "#             l = random.sample(range(0,100000), N)\n",
    "#             count = -1\n",
    "#             while(count<3): ##有抽到前1000的\n",
    "#                 if count != -1:\n",
    "#                     l = random.sample(range(0,100000), N)\n",
    "#                 count = 0 \n",
    "#                 for check in l:\n",
    "#                     if check not in train_top_1000[i] or type(doc_text[check]) != float: ##不是top 1000\n",
    "#                         count += 1        \n",
    "#             for idx in range(N+1):\n",
    "#                 if idx != ans_pos:\n",
    "#                     query[idx] = train_query_data[i]\n",
    "#                     choice[idx] = clean_string(doc_text[idx])\n",
    "#             query = json.dumps(query)\n",
    "#             choice = json.dumps(choice)\n",
    "#             train_question.append(query)\n",
    "#             train_choice.append(choice)\n",
    "# #             train_question.append(json.dumps(query))\n",
    "# #             train_choice.append(json.dumps(choice))\n",
    "# # train_question = np.array(train_question)\n",
    "# # train_choice = np.array(train_choice)\n",
    "# print(train_question[0])\n",
    "# train_y = np.array(train_y)\n",
    "# # print(train_question.shape)\n",
    "# # print(train_choice.shape)\n",
    "# print(train_y.shape)http://localhost:8787/notebooks/NewsClassify/Mouth/HW6.ipynb#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## model 各參數\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "# model.load_state_dict(torch.load('./HW6_model/HW6_baseline_0.pkl'))\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "EVAL = 9000\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ## model 各參數\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# num_labels = 2\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased',num_labels = num_labels)\n",
    "# # model.load_state_dict(torch.load('./HW6_model/HW6_baseline_0.pkl'))\n",
    "# model = model.to(device)\n",
    "# model.train()\n",
    "# EVAL = 7500\n",
    "# EPOCHS = 20\n",
    "# optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "9000\n",
      "9000\n",
      "poliomyelitis and post-polio\n",
      "language: <f p=105> chinese <f> article type:cso <f p=106> [article zhao zhuliang 6392 4554 5328) central <f> china teachers college edited xu honghai 6079 1347 3189): state chinas physically challenged persons course modernization] [excerpt] [passage omitted] recent years along development chinas physically challenged persons cause condition large special group improved certain extent current state physically challenged areas economics education jobs recovery generally remains far behind average level society whole 1 threat-to-survival crisis statistics show 49 percent chinas population physically challenged many 20 million struggling survive subsistence line 70 percent physically challenged existing support family relatives state collective relief study reported 1993 2 issue zhongguo canji ren [chinas disabled persons] says that: many physically challenged remain great hardship basically enough food clothing instance shaoyang city hunan province economic conditions better average income 1991 54519 yuan rural population 26464 yuan physically challenged less one-half shaoyangs xinning county poverty-stricken physically challenged make 644 percent physically challenged 172 percent exceptional poverty document put china coalition disabled 1992 document 126 coalition disabled) contains following item: poverty relief carried certain developed developed areas 70-80 percent poverty-striken population local subsistence line physically challenged shunyi county 200 poverty-stricken families 97 percent families physically challenged one village chengde living thatched houses physically challenged poverty-stricken population 1991 physically challenged made 80 percent qingdao 70 percent jiangsu 50 percent yunnan guizhou sichuan 2 inequality-of-education crisis national rate illiteracy semi-literacy 206 percent 1987 156 percent end 1991 illiteracy physically challenged stood 68 percent national school-entrance rate school-age children 971 percent 1987 978 percent end 1991 rate 1987 27 percent blind children 55 percent deaf dumb children 033 percent low-intelligence children end 1991 school-entrance rate blind deaf low-intelligence children less 10 percent gap even larger education high-school level 1984 almost none chinas physically challenged went school high-school level since 1984 physical-examination criteria amended enabling 1700 physically challenged year meet political-examination grades physical-examination standards college admission nearly 900 year still admitted 1992 little 5000 physically challenged throughout china received higher education university level poverty ignorance always go together approximately 70 percent chinas physically challenged dependent support family relatives state collective relief figure alarmingly close rate illiteracy semi-literacy among physically challenged warns us longstanding lack education makes hard physically challenged overcome dependence others society leaving regretable plight social burden 3 job-obstacle puzzle jobs means livelihood terms individuals display talent grow way everyone contribute society jobs situation among chinas physically challenged far desirable recent years chinas urban unemployment rate around 3 percent 26 percent 1991 49 percent physically challenged unemployed rural areas nearly 30 percent employable partially employable physically challenged without jobs physically challenged choose jobs range free job-choice small sample survey shows job situation physically challenged china following features: 1 job classification 812 percent work farming forestry livestock raising fishery; 98 percent production transportation post telecommunications; 55 percent commercial service; 21 percent specialized technicians; 14 percent government collective-enterprise officials office workers jobs 2 manual mental labor 966 percent physically challenged work manual jobs agriculture industry commerce 34 percent work mental jobs specialized technicians government enterprise officials lower national ratio mental laborers population 8 percent 4 health-care-security deficiency 1987 national sample survey showed crucial need 50 percent physically challenged treatment recovery necessary supplementary appliances 1992 sample survey 11 provinces found 97 percent physically challenged needed supplementary appliances special articles nearly 90 percent could obtain china 49 million people gone blind cataracts 177 million deaf people 171 million children age 14 740000 infants age 7) every year bringing 30000 new deaf children 124 million suffering aftermath polio end 1992 little 700000 cataract surgery little 280000 corrective surgery polio [infantile paralysis] slightly 29000 deaf-mutes trained sign data shows health-care-security deficiency keeps many physically challenged bed long years cut world unable take equal part social activities 5 marriage-and-family misfortune 1987 sample survey showed 43 million adult physically challenged throughout china 2021 million 47 percent adult physically challenged spouses national unmarried rate adults roughly 8 percent; family divorce rate physically challenged high 146 percent almost triple sound-family divorce rate 055 percent; widowed rate physically challenged 3217 percent quadruple healthy widowed rate 761 percent physical defects psychological barriers social economic status limitations turn marriage family matter physically challenged troublesome problem face short within general setting chinese peoples march toward becoming comfortably well-off two-thirds physically challenged still cannot support nearly 20 million enough food clothing; physically challenged high 68-percent illiteracy rate almost 90 percent blind deaf low-intelligence children school; quite employable physically challenged jobs high 49-percent unemployment rate; physically challenged obtained care needed recovery leaving enormous shocking social problem harshly confronting society physically challenged [passage omitted] \n",
      "46743\n"
     ]
    }
   ],
   "source": [
    "test_q_set = train_question[:EVAL]\n",
    "test_c_set = train_choice[:EVAL]\n",
    "train_q_set = train_question[:]\n",
    "train_c_set = train_choice[:]\n",
    "\n",
    "test_y_set = train_y[:EVAL]\n",
    "train_y_set = train_y[:]\n",
    "print(len(test_q_set))\n",
    "print(len(test_c_set))\n",
    "print(len(test_y_set))\n",
    "\n",
    "print(train_q_set[2])\n",
    "print(train_c_set[0])\n",
    "print(len(train_y_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health and computer terminals\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(test_q_set[7499])\n",
    "for i in range(120):\n",
    "    if train_query_data[i] == test_q_set[7499]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_test_acc(model, testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                           token_type_ids = segment_tensors,\n",
    "                           attention_mask = masks_tensors,\n",
    "                           labels = labels)\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_test_score(model, testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    result_batch = [] ## all label 1's score\n",
    "    correct = 0 ##correct_num\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                           token_type_ids = segment_tensors,\n",
    "                           attention_mask = masks_tensors,\n",
    "                           labels = labels)\n",
    "            score = outputs[1]\n",
    "            for idx in range(score.shape[0]):\n",
    "                result_batch.append(score[idx][1])\n",
    "#             result_score = 0\n",
    "#             for num in score:\n",
    "#                 for result in num:\n",
    "#                     result_score += result\n",
    "#             print(result_score)\n",
    "    return result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# def get_test_score(model, testloader):\n",
    "#     model.eval()  ##test mode\n",
    "#     total = 0 ##total_num\n",
    "#     result_batch = [] ## all label 1's score\n",
    "#     correct = 0 ##correct_num\n",
    "#     with torch.no_grad():   ##eval不計算gradient \n",
    "#         for data in testloader:\n",
    "#             tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "#             outputs = model(input_ids = tokens_tensors,\n",
    "#                            token_type_ids = segment_tensors,\n",
    "#                            attention_mask = masks_tensors,\n",
    "#                            labels = labels)\n",
    "#             score = outputs[1]\n",
    "#             for idx in range(score.shape[0]):\n",
    "#                 result_batch.append(score[idx][2]-score[idx][1]) ##相關扣掉偽相關\n",
    "#     return result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MAP():\n",
    "#     top_1000 = [t.split() for t in train_q['bm25_top1000']]\n",
    "#     pos_ans = [t.split() for t in train_q['pos_doc_ids']]\n",
    "#     num = 0\n",
    "#     a = 0 \n",
    "#     num_query = 120 ## 25 拿來test \n",
    "#     all_score = 0.0\n",
    "#     for i in range(int(num_query)):\n",
    "#         score = 0.0\n",
    "#         find_list = []## all list\n",
    "#         pred_list = [] ## all pred score\n",
    "#         pos_list = [] ##all score changed pos \n",
    "#         relevant_list = {} ## all relevant\n",
    "# #         all_find = input() ##所有query結果\n",
    "#         all_find_split = top_1000[i] ## list all find\n",
    "#         for j in top_1000[i]:\n",
    "#             find_list.append(j) ## 建list\n",
    "#             pred_list.append(0)\n",
    "# #         relevant = input()\n",
    "#         all_relevant_split = pos_ans[i] ## list all find\n",
    "#         print(len(all_relevant_split))\n",
    "#         num+=len(all_relevant_split)\n",
    "#         if len(all_relevant_split)>80:\n",
    "#             a += 1\n",
    "#         for j in pos_ans[i]:\n",
    "#             relevant_list[j] = 0 ## 建dict 比較好查找\n",
    "#         now_pos = 0\n",
    "#         correct = 0 ##對幾個\n",
    "#         count = 0\n",
    "#         for j in range(len(find_list)):\n",
    "#             count += 1\n",
    "#             if(find_list[j] in relevant_list):\n",
    "#                 correct += 1\n",
    "#                 for k in range(count-now_pos):\n",
    "#                     pred_list[now_pos+k] = correct/count\n",
    "#                 now_pos = j+1\n",
    "#                 pos_list.append(j)\n",
    "#         for j in range(len(pos_list)):\n",
    "#             score += pred_list[pos_list[j]]\n",
    "#         if len(pos_list)!=0:\n",
    "#             score /= len(all_relevant_split) \n",
    "#         all_score += score\n",
    "#     all_score/=int(num_query)\n",
    "#     print(num)\n",
    "#     print(a)\n",
    "#     return (round(all_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_a = [t.split() for t in train_q['bm25_top1000']]\n",
    "# MAP() ##要傳25x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "train_input_dict = tokenizer(train_q_set, train_c_set, \n",
    "                             padding=True, \n",
    "                             truncation=True, \n",
    "                             return_tensors=\"pt\",\n",
    "                             return_token_type_ids = True,\n",
    "                             max_length = 512)\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "trainset = TrainDataset(train_input_dict, train_y_set) ##trainset參數如init\n",
    "trainloader = DataLoader(trainset, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test \n",
    "test_input_dict = tokenizer(test_q_set, test_c_set, \n",
    "                             padding=True, \n",
    "                             truncation=True, \n",
    "                             return_tensors=\"pt\",\n",
    "                             return_token_type_ids = True,\n",
    "                             max_length = 512)\n",
    "TEST_BATCH_SIZE = 64\n",
    "testset = TrainDataset(test_input_dict, test_y_set) ##trainset參數如init\n",
    "testloader = DataLoader(testset, batch_size = TEST_BATCH_SIZE, shuffle = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_trainloader():\n",
    "    train_question = []\n",
    "    train_choice = []\n",
    "    train_y = []\n",
    "    N = 3 ## 錯的文章數量\n",
    "    EVAL = 9000\n",
    "    for i in range(len(train_query_data)):\n",
    "        if len(train_pos_list[i]) <50:\n",
    "            N = 10\n",
    "        else:\n",
    "            N = 3\n",
    "        for j in range(len(train_pos_list[i])): ##幾篇POS\n",
    "            if type(doc_text[train_pos_list[i][j]]) != float:\n",
    "                train_y.append(1)\n",
    "                train_question.append(train_query_data[i])\n",
    "                train_choice.append(clean_string(doc_text[train_pos_list[i][j]])) ##對的\n",
    "    #             upper_bound = (N+1)*len(train_pos_list[i])\n",
    "    #             if upper_bound>=999:\n",
    "    #                 upper_bound = 999\n",
    "                l = random.sample(range(0,999), N)\n",
    "                count = -1\n",
    "                while(count < N): \n",
    "                    if count != -1:\n",
    "                        l = random.sample(range(0,999), N)\n",
    "                    count = 0 \n",
    "                    for check in l:\n",
    "                        if int(train_neg_list[i][check]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][check])]) != float: ##不是POS\n",
    "                            count += 1    \n",
    "                for idx in l:\n",
    "                    train_y.append(0)\n",
    "                    train_question.append(train_query_data[i])\n",
    "                    train_choice.append(clean_string(doc_text[int(train_neg_list[i][idx])]))\n",
    "    train_y = np.array(train_y)\n",
    "     ###切割驗證 訓練\n",
    "    test_q_set = train_question[:EVAL]\n",
    "    test_c_set = train_choice[:EVAL]\n",
    "    train_q_set = train_question[:]\n",
    "    train_c_set = train_choice[:]\n",
    "\n",
    "    test_y_set = train_y[:EVAL]\n",
    "    train_y_set = train_y[:]\n",
    "    ### tokenizer\n",
    "    input_dict = tokenizer(train_q_set, train_c_set, \n",
    "                             padding=True, \n",
    "                             truncation=True, \n",
    "                             return_tensors=\"pt\",\n",
    "                             return_token_type_ids = True,\n",
    "                             max_length = 512)\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    data = TrainDataset(input_dict, train_y_set) ##trainset參數如init\n",
    "    loader = DataLoader(data, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def new_trainloader():\n",
    "#     train_question = []\n",
    "#     train_choice = []\n",
    "#     train_y = []\n",
    "#     R = 1 ##偽相關的數量\n",
    "#     EVAL = 7500\n",
    "#     N = 2 ## 錯的文章數量\n",
    "#     for i in range(len(train_query_data)):\n",
    "#         for j in range(len(train_pos_list[i])): ##幾篇POS\n",
    "#             if type(doc_text[train_pos_list[i][j]]) != float:\n",
    "#                 train_y.append(2) ##代表相關\n",
    "#                 train_question.append(train_query_data[i])\n",
    "#                 train_choice.append(clean_string(doc_text[train_pos_list[i][j]])) ##對的\n",
    "#     #             upper_bound = (N+1)*len(train_pos_list[i])\n",
    "#     #             if upper_bound>=999:\n",
    "#     #                 upper_bound = 999\n",
    "#                 n = random.sample(range(0,99),R)\n",
    "#                 flag = True\n",
    "#                 while(flag):\n",
    "#                     if int(train_neg_list[i][n[0]]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][n[0]])]) != float: ##不是POS\n",
    "#                         flag = False  \n",
    "#                     else:\n",
    "#                         n = random.sample(range(0,99),R)\n",
    "#                 train_y.append(1)\n",
    "#                 train_question.append(train_query_data[i])\n",
    "#                 train_choice.append(clean_string(doc_text[int(train_neg_list[i][n[0]])]))\n",
    "#                 l = random.sample(range(100,999), N)\n",
    "#                 count = -1\n",
    "#                 while(count < N): \n",
    "#                     if count != -1:\n",
    "#                         l = random.sample(range(100,999), N)\n",
    "#                     count = 0 \n",
    "#                     for check in l:\n",
    "#                         if int(train_neg_list[i][check]) not in train_pos_list[i] and type(doc_text[int(train_neg_list[i][check])]) != float: ##不是POS\n",
    "#                             count += 1    \n",
    "#                 for idx in l:\n",
    "#                     train_y.append(0)\n",
    "#                     train_question.append(train_query_data[i])\n",
    "#                     train_choice.append(clean_string(doc_text[int(train_neg_list[i][idx])]))\n",
    "#     train_y = np.array(train_y)\n",
    "#      ###切割驗證 訓練\n",
    "#     test_q_set = train_question[:EVAL]\n",
    "#     test_c_set = train_choice[:EVAL]\n",
    "#     train_q_set = train_question[EVAL:]\n",
    "#     train_c_set = train_choice[EVAL:]\n",
    "\n",
    "#     test_y_set = train_y[:EVAL]\n",
    "#     train_y_set = train_y[EVAL:]\n",
    "#     ### tokenizer\n",
    "#     input_dict = tokenizer(train_q_set, train_c_set, \n",
    "#                              padding=True, \n",
    "#                              truncation=True, \n",
    "#                              return_tensors=\"pt\",\n",
    "#                              return_token_type_ids = True,\n",
    "#                              max_length = 512)\n",
    "#     TRAIN_BATCH_SIZE = 4\n",
    "#     data = TrainDataset(input_dict, train_y_set) ##trainset參數如init\n",
    "#     loader = DataLoader(data, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  \n",
    "#     return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# num_labels = 3\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "# model = model.to(device)\n",
    "# # model.load_state_dict(torch.load('./HW6_model/HW6_1e5_new_train_data_0.pkl'))\n",
    "# EPOCHS = 5 #ten model 兩個一組\n",
    "# optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
    "# # model.eval()\n",
    "# # test_acc = get_test_acc(model,testloader)\n",
    "# # print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] 9422/9423 Loss: 3145.1042 Acc : 0.8556   test acc: 0.8891111111111111\n",
      "Epoch [3/10] 11685/11686 Loss: 3685.1519 Acc : 0.8800   test acc: 0.935\n",
      "Epoch [4/10] 11685/11686 Loss: 2431.7421 Acc : 0.9236   test acc: 0.9528888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] 10811/11686 Loss: 3289.8735 Acc : 0.8795"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] 11685/11686 Loss: 3445.2270 Acc : 0.8824   test acc: 0.9208888888888889\n",
      "Epoch [10/10] 2363/11686 Loss: 477.8781 Acc : 0.9240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e = 0.1 ##label smooth的參數\n",
    "EPOCHS = 10\n",
    "# smooth_loss_fnc = F.CrossEntropyLoss()\n",
    "type_weight = torch.FloatTensor([1.2e+00,6.0e+00]).to(device)\n",
    "type_loss_func = F.CrossEntropyLoss(weight=type_weight)\n",
    "origin_lr = 1e-5\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if epoch != 0:\n",
    "        if (epoch % 2 == 0):\n",
    "            num_labels = 2\n",
    "            model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels) ##重新init model\n",
    "            model = model.to(device)\n",
    "            model.train()\n",
    "            optimizer = AdamW(model.parameters(),lr = origin_lr) \n",
    "        trainloader = new_trainloader() ##重新選random\n",
    "        optimizer = AdamW(model.parameters(),lr = origin_lr *(0.7)**(epoch%3)) ##隨epoch下降lr\n",
    "    for (i,data) in enumerate(trainloader):\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , labels  = [t.to(device) for t in data]\n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                             token_type_ids=segments_tensors, \n",
    "                             attention_mask=masks_tensors,\n",
    "                             labels = labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = bert_outputs[1]\n",
    "        pred = torch.argmax(logits,dim = -1)\n",
    "\n",
    "        loss = type_loss_func(logits,labels)\n",
    "#         loss = bert_outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += pred.size()[0]\n",
    "        correct += (pred == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {running_loss:.4f} Acc : {(correct/total):.4f}', end='')\n",
    "    test_acc = get_test_acc(model,testloader)\n",
    "    ##all save\n",
    "    torch.save(model.state_dict(),'./HW6_model/HW6_bert_new_balance_' + str(epoch) + '.pkl')\n",
    "    print('   test acc:' , test_acc)\n",
    "    with open('./hw6_test_score_1000'+str(epoch)+'.txt','w') as f:\n",
    "        f.write(str(test_acc))\n",
    "        f.write('\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./HW6_model/HW6_bert_balance_12.pkl')) ##先拿中間ㄉ\n",
    "model.eval()\n",
    "# test_acc = get_test_acc(model,testloader)\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels = 2\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "# model = model.to(device)\n",
    "# model.load_state_dict(torch.load('./HW6_model/HW6_smooth_0.pkl'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international organized crime\n"
     ]
    }
   ],
   "source": [
    "test_q = pd.read_csv('./test_queries.csv')\n",
    "test_query_list = test_q['query_id'].tolist()\n",
    "test_query_data = [t.lower() for t in test_q['query_text']]\n",
    "rerank_idx_list = [t.split() for t in test_q['bm25_top1000']]\n",
    "rerank_score_list = [t.split() for t in test_q['bm25_top1000_scores']]\n",
    "result_score_list = []\n",
    "print(test_query_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBIS3-21770\n"
     ]
    }
   ],
   "source": [
    "print(rerank_idx_list[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_question = []\n",
    "# test_choice = []\n",
    "# test_y = []\n",
    "# for i in range(len(test_query_data)):\n",
    "#     for j in range(len(rerank_idx_list[i])):\n",
    "#         idx = doc_dict[rerank_idx_list[i][j]]\n",
    "#         if type(doc_text[idx]) != float:\n",
    "#             test_y.append(1)\n",
    "#             test_question.append(test_query_data[i])\n",
    "#             test_choice.append(clean_string(doc_text[idx]))\n",
    "\n",
    "# # train_question = np.array(train_question)\n",
    "# # train_choice = np.array(train_choice)\n",
    "# # train_question = json.dumps(train_question)\n",
    "# test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score_list = np.zeros([80,1000]) ##分數\n",
    "bert_score_list = np.zeros([80,1000])\n",
    "for i in range(len(rerank_score_list)):\n",
    "    for j in range(len(rerank_score_list[0])):\n",
    "        result_score_list[i][j] = float(rerank_score_list[i][j])\n",
    "after_add_list = np.zeros([80,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.23352808\n"
     ]
    }
   ],
   "source": [
    "print(result_score_list[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "all_ans_list_1 = []\n",
    "all_ans_list_2 = []\n",
    "all_ans_list_3 = []\n",
    "bert_ans_list = []\n",
    "for i in range(80):\n",
    "    print(i)\n",
    "    bert_ans = []\n",
    "    ans_list = []\n",
    "#     ans_list2 = []\n",
    "#     ans_list3 = []\n",
    "    idx_list = rerank_idx_list[i]\n",
    "    bert_score = bert_score_list[i].copy()\n",
    "    score_list = result_score_list[i].copy()\n",
    "    score_list2 = result_score_list[i].copy()\n",
    "    score_list3 = result_score_list[i].copy()\n",
    "#     denominator = score_list[0]/score_list[999]\n",
    "#     score_list /= denominator\n",
    "#     print(score_list)\n",
    "    input_q = []\n",
    "    input_c = []\n",
    "    no_content_list = []\n",
    "    for j in range(len(idx_list)):\n",
    "        find = doc_dict[idx_list[j]] ## idx \n",
    "        if type(doc_text[find]) != float:\n",
    "            text = clean_string(doc_text[find])\n",
    "            input_q.append(test_query_data[i])\n",
    "            input_c.append(text)\n",
    "        else:\n",
    "            no_content_list.append(j) ##空的\n",
    "    batch_test_input_dict = tokenizer(input_q, input_c, \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length = 512)\n",
    "    TEST_BATCH_SIZE = 64\n",
    "    batch_testset = TrainDataset(batch_test_input_dict,torch.zeros([len(idx_list)], dtype=torch.long)) ##trainset參數如init 給label沒差\n",
    "    batch_testloader = DataLoader(batch_testset, batch_size = TEST_BATCH_SIZE, shuffle = False)  \n",
    "    score = get_test_score(model,batch_testloader)\n",
    "    no_content_count = 0\n",
    "    for k in range(len(score)):\n",
    "#         print(k)\n",
    "#         print(score[k])\n",
    "        if k not in no_content_list:\n",
    "            bert_score[k+no_content_count] += score[k]\n",
    "            score_list[k+no_content_count] +=  2 * score[k]\n",
    "#             score_list2[k+no_content_count] +=  2 * score[k]\n",
    "#             score_list3[k+no_content_count] +=  3 * score[k]\n",
    "        else:\n",
    "            no_content_count += 1\n",
    "    after_add_list[i] = score_list\n",
    "    bert_score_list[i] = bert_score\n",
    "    sort_bert = sorted(range(1000),reverse = True,key = lambda k : bert_score[k])\n",
    "    ans_idx_list = sorted(range(1000),reverse = True,key = lambda k : score_list[k])\n",
    "#     ans_idx_list2 = sorted(range(1000),reverse = True,key = lambda k : score_list2[k])\n",
    "#     ans_idx_list3 = sorted(range(1000),reverse = True,key = lambda k : score_list3[k])\n",
    "    for j in range(1000):\n",
    "        bert_ans.append(rerank_idx_list[i][sort_bert[j]])\n",
    "        ans_list.append(rerank_idx_list[i][ans_idx_list[j]])\n",
    "#         ans_list2.append(rerank_idx_list[i][ans_idx_list2[j]])\n",
    "#         ans_list3.append(rerank_idx_list[i][ans_idx_list3[j]])\n",
    "    bert_ans_list.append(bert_ans)\n",
    "    all_ans_list_1.append(ans_list)\n",
    "#     all_ans_list_2.append(ans_list2)\n",
    "#     all_ans_list_3.append(ans_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./HW6_model/bert_score_6',bert_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_list = np.zeros([25,1000]) ##分數\n",
    "train_top_1000 = [t.split() for t in train_q['bm25_top1000']]\n",
    "train_pos_list = [t.split() for t in train_q['pos_doc_ids']]\n",
    "train_1000_score = [t.split() for t in train_q['bm25_top1000_scores']]\n",
    "bert_test = np.zeros([25,1000])\n",
    "for i in range(len(test_score_list)):\n",
    "    for j in range(len(test_score_list[0])):\n",
    "        test_score_list[i][j] = float(train_1000_score[i][j])\n",
    "        \n",
    "top_1000 = [t.split() for t in train_q['bm25_top1000']]\n",
    "pos_list = [t.split() for t in train_q['pos_doc_ids']]\n",
    "for i in range(25):\n",
    "    print(i)\n",
    "    idx_list = top_1000[i]\n",
    "    bert_score = bert_test[i].copy()\n",
    "    input_q = []\n",
    "    input_c = []\n",
    "    no_content_list = []\n",
    "    for j in range(len(idx_list)):\n",
    "        find = doc_dict[idx_list[j]] ## idx \n",
    "        if type(doc_text[find]) != float:\n",
    "            text = clean_string(doc_text[find])\n",
    "            input_q.append(test_query_data[i])\n",
    "            input_c.append(text)\n",
    "        else:\n",
    "            no_content_list.append(j) ##空的\n",
    "    batch_test_input_dict = tokenizer(input_q, input_c, \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length = 512)\n",
    "    TEST_BATCH_SIZE = 64\n",
    "    batch_testset = TrainDataset(batch_test_input_dict,torch.zeros([len(idx_list)], dtype=torch.long)) ##trainset參數如init 給label沒差\n",
    "    batch_testloader = DataLoader(batch_testset, batch_size = TEST_BATCH_SIZE, shuffle = False)  \n",
    "    score = get_test_score(model,batch_testloader)\n",
    "    no_content_count = 0\n",
    "    for k in range(len(score)):\n",
    "        if k not in no_content_list:\n",
    "            bert_score[k+no_content_count] += score[k]\n",
    "        else:\n",
    "            no_content_count += 1\n",
    "    bert_test[i] = bert_score ##塞回去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_list = np.zeros([25,1000]) ##分數\n",
    "train_top_1000 = [t.split() for t in train_q['bm25_top1000']]\n",
    "train_pos_list = [t.split() for t in train_q['pos_doc_ids']]\n",
    "train_1000_score = [t.split() for t in train_q['bm25_top1000_scores']]\n",
    "for i in range(len(test_score_list)):\n",
    "    for j in range(len(test_score_list[0])):\n",
    "        test_score_list[i][j] = float(train_1000_score[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: -1.99   MAP: 0.2541\n",
      "a: -1.98   MAP: 0.2543\n",
      "a: -1.97   MAP: 0.2543\n",
      "a: -1.96   MAP: 0.2543\n",
      "a: -1.95   MAP: 0.2544\n",
      "a: -1.94   MAP: 0.2545\n",
      "a: -1.93   MAP: 0.2545\n",
      "a: -1.92   MAP: 0.2545\n",
      "a: -1.91   MAP: 0.2544\n",
      "a: -1.9   MAP: 0.2544\n",
      "a: -1.89   MAP: 0.2545\n",
      "a: -1.88   MAP: 0.2545\n",
      "a: -1.8699999999999999   MAP: 0.2546\n",
      "a: -1.8599999999999999   MAP: 0.2546\n",
      "a: -1.8499999999999999   MAP: 0.2547\n",
      "a: -1.8399999999999999   MAP: 0.2548\n",
      "a: -1.8299999999999998   MAP: 0.2548\n",
      "a: -1.8199999999999998   MAP: 0.2563\n",
      "a: -1.8099999999999998   MAP: 0.2563\n",
      "a: -1.7999999999999998   MAP: 0.2556\n",
      "a: -1.7899999999999998   MAP: 0.2557\n",
      "a: -1.7799999999999998   MAP: 0.2557\n",
      "a: -1.7699999999999998   MAP: 0.2558\n",
      "a: -1.7599999999999998   MAP: 0.2552\n",
      "a: -1.7499999999999998   MAP: 0.2553\n",
      "a: -1.7399999999999998   MAP: 0.2553\n",
      "a: -1.7299999999999998   MAP: 0.2553\n",
      "a: -1.7199999999999998   MAP: 0.2554\n",
      "a: -1.7099999999999997   MAP: 0.2553\n",
      "a: -1.6999999999999997   MAP: 0.2554\n",
      "a: -1.6899999999999997   MAP: 0.2554\n",
      "a: -1.6799999999999997   MAP: 0.2554\n",
      "a: -1.6699999999999997   MAP: 0.2555\n",
      "a: -1.6599999999999997   MAP: 0.2556\n",
      "a: -1.6499999999999997   MAP: 0.2556\n",
      "a: -1.6399999999999997   MAP: 0.2559\n",
      "a: -1.6299999999999997   MAP: 0.2559\n",
      "a: -1.6199999999999997   MAP: 0.2559\n",
      "a: -1.6099999999999997   MAP: 0.256\n",
      "a: -1.5999999999999996   MAP: 0.2561\n",
      "a: -1.5899999999999996   MAP: 0.2564\n",
      "a: -1.5799999999999996   MAP: 0.2565\n",
      "a: -1.5699999999999996   MAP: 0.2566\n",
      "a: -1.5599999999999996   MAP: 0.2568\n",
      "a: -1.5499999999999996   MAP: 0.2575\n",
      "a: -1.5399999999999996   MAP: 0.2575\n",
      "a: -1.5299999999999996   MAP: 0.2576\n",
      "a: -1.5199999999999996   MAP: 0.2576\n",
      "a: -1.5099999999999996   MAP: 0.2576\n",
      "a: -1.4999999999999996   MAP: 0.2577\n",
      "a: -1.4899999999999995   MAP: 0.2577\n",
      "a: -1.4799999999999995   MAP: 0.2577\n",
      "a: -1.4699999999999995   MAP: 0.2578\n",
      "a: -1.4599999999999995   MAP: 0.2573\n",
      "a: -1.4499999999999995   MAP: 0.2569\n",
      "a: -1.4399999999999995   MAP: 0.2569\n",
      "a: -1.4299999999999995   MAP: 0.257\n",
      "a: -1.4199999999999995   MAP: 0.257\n",
      "a: -1.4099999999999995   MAP: 0.257\n",
      "a: -1.3999999999999995   MAP: 0.257\n",
      "a: -1.3899999999999995   MAP: 0.257\n",
      "a: -1.3799999999999994   MAP: 0.257\n",
      "a: -1.3699999999999994   MAP: 0.2572\n",
      "a: -1.3599999999999994   MAP: 0.2572\n",
      "a: -1.3499999999999994   MAP: 0.2572\n",
      "a: -1.3399999999999994   MAP: 0.2572\n",
      "a: -1.3299999999999994   MAP: 0.2572\n",
      "a: -1.3199999999999994   MAP: 0.2573\n",
      "a: -1.3099999999999994   MAP: 0.2574\n",
      "a: -1.2999999999999994   MAP: 0.2573\n",
      "a: -1.2899999999999994   MAP: 0.2567\n",
      "a: -1.2799999999999994   MAP: 0.2568\n",
      "a: -1.2699999999999994   MAP: 0.2569\n",
      "a: -1.2599999999999993   MAP: 0.2569\n",
      "a: -1.2499999999999993   MAP: 0.2569\n",
      "a: -1.2399999999999993   MAP: 0.2571\n",
      "a: -1.2299999999999993   MAP: 0.257\n",
      "a: -1.2199999999999993   MAP: 0.257\n",
      "a: -1.2099999999999993   MAP: 0.257\n",
      "a: -1.1999999999999993   MAP: 0.2571\n",
      "a: -1.1899999999999993   MAP: 0.2571\n",
      "a: -1.1799999999999993   MAP: 0.2571\n",
      "a: -1.1699999999999993   MAP: 0.2574\n",
      "a: -1.1599999999999993   MAP: 0.2575\n",
      "a: -1.1499999999999992   MAP: 0.2576\n",
      "a: -1.1399999999999992   MAP: 0.2576\n",
      "a: -1.1299999999999992   MAP: 0.2575\n",
      "a: -1.1199999999999992   MAP: 0.2575\n",
      "a: -1.1099999999999992   MAP: 0.2576\n",
      "a: -1.0999999999999992   MAP: 0.2576\n",
      "a: -1.0899999999999992   MAP: 0.2577\n",
      "a: -1.0799999999999992   MAP: 0.2582\n",
      "a: -1.0699999999999992   MAP: 0.2583\n",
      "a: -1.0599999999999992   MAP: 0.2583\n",
      "a: -1.0499999999999992   MAP: 0.2584\n",
      "a: -1.0399999999999991   MAP: 0.2583\n",
      "a: -1.0299999999999991   MAP: 0.2584\n",
      "a: -1.0199999999999991   MAP: 0.2587\n",
      "a: -1.0099999999999991   MAP: 0.2586\n",
      "a: -0.9999999999999991   MAP: 0.2587\n",
      "a: -0.9899999999999991   MAP: 0.2587\n",
      "a: -0.9799999999999991   MAP: 0.2587\n",
      "a: -0.9699999999999991   MAP: 0.2588\n",
      "a: -0.9599999999999991   MAP: 0.2587\n",
      "a: -0.9499999999999991   MAP: 0.259\n",
      "a: -0.9399999999999991   MAP: 0.2591\n",
      "a: -0.929999999999999   MAP: 0.259\n",
      "a: -0.919999999999999   MAP: 0.259\n",
      "a: -0.909999999999999   MAP: 0.259\n",
      "a: -0.899999999999999   MAP: 0.259\n",
      "a: -0.889999999999999   MAP: 0.2591\n",
      "a: -0.879999999999999   MAP: 0.2591\n",
      "a: -0.869999999999999   MAP: 0.2591\n",
      "a: -0.859999999999999   MAP: 0.2591\n",
      "a: -0.849999999999999   MAP: 0.2592\n",
      "a: -0.839999999999999   MAP: 0.2592\n",
      "a: -0.829999999999999   MAP: 0.2591\n",
      "a: -0.819999999999999   MAP: 0.2591\n",
      "a: -0.8099999999999989   MAP: 0.2591\n",
      "a: -0.7999999999999989   MAP: 0.2591\n",
      "a: -0.7899999999999989   MAP: 0.2591\n",
      "a: -0.7799999999999989   MAP: 0.2592\n",
      "a: -0.7699999999999989   MAP: 0.2593\n",
      "a: -0.7599999999999989   MAP: 0.2593\n",
      "a: -0.7499999999999989   MAP: 0.2592\n",
      "a: -0.7399999999999989   MAP: 0.2592\n",
      "a: -0.7299999999999989   MAP: 0.2584\n",
      "a: -0.7199999999999989   MAP: 0.2581\n",
      "a: -0.7099999999999989   MAP: 0.2575\n",
      "a: -0.6999999999999988   MAP: 0.2576\n",
      "a: -0.6899999999999988   MAP: 0.2576\n",
      "a: -0.6799999999999988   MAP: 0.2575\n",
      "a: -0.6699999999999988   MAP: 0.2575\n",
      "a: -0.6599999999999988   MAP: 0.2576\n",
      "a: -0.6499999999999988   MAP: 0.2575\n",
      "a: -0.6399999999999988   MAP: 0.2572\n",
      "a: -0.6299999999999988   MAP: 0.2573\n",
      "a: -0.6199999999999988   MAP: 0.2574\n",
      "a: -0.6099999999999988   MAP: 0.2574\n",
      "a: -0.5999999999999988   MAP: 0.2574\n",
      "a: -0.5899999999999987   MAP: 0.2574\n",
      "a: -0.5799999999999987   MAP: 0.2574\n",
      "a: -0.5699999999999987   MAP: 0.2575\n",
      "a: -0.5599999999999987   MAP: 0.2577\n",
      "a: -0.5499999999999987   MAP: 0.2577\n",
      "a: -0.5399999999999987   MAP: 0.2577\n",
      "a: -0.5299999999999987   MAP: 0.2578\n",
      "a: -0.5199999999999987   MAP: 0.2575\n",
      "a: -0.5099999999999987   MAP: 0.2576\n",
      "a: -0.49999999999999867   MAP: 0.258\n",
      "a: -0.48999999999999866   MAP: 0.258\n",
      "a: -0.47999999999999865   MAP: 0.2578\n",
      "a: -0.46999999999999864   MAP: 0.2578\n",
      "a: -0.45999999999999863   MAP: 0.2578\n",
      "a: -0.4499999999999986   MAP: 0.2578\n",
      "a: -0.4399999999999986   MAP: 0.2577\n",
      "a: -0.4299999999999986   MAP: 0.2578\n",
      "a: -0.4199999999999986   MAP: 0.2578\n",
      "a: -0.4099999999999986   MAP: 0.2578\n",
      "a: -0.3999999999999986   MAP: 0.2578\n",
      "a: -0.38999999999999857   MAP: 0.2578\n",
      "a: -0.37999999999999856   MAP: 0.2578\n",
      "a: -0.36999999999999855   MAP: 0.2579\n",
      "a: -0.35999999999999854   MAP: 0.2579\n",
      "a: -0.34999999999999853   MAP: 0.2579\n",
      "a: -0.3399999999999985   MAP: 0.258\n",
      "a: -0.3299999999999985   MAP: 0.258\n",
      "a: -0.3199999999999985   MAP: 0.258\n",
      "a: -0.3099999999999985   MAP: 0.2583\n",
      "a: -0.2999999999999985   MAP: 0.2588\n",
      "a: -0.2899999999999985   MAP: 0.2589\n",
      "a: -0.2799999999999985   MAP: 0.2589\n",
      "a: -0.26999999999999846   MAP: 0.2588\n",
      "a: -0.25999999999999845   MAP: 0.2588\n",
      "a: -0.24999999999999845   MAP: 0.2591\n",
      "a: -0.23999999999999844   MAP: 0.2591\n",
      "a: -0.22999999999999843   MAP: 0.2592\n",
      "a: -0.21999999999999842   MAP: 0.2592\n",
      "a: -0.2099999999999984   MAP: 0.259\n",
      "a: -0.1999999999999984   MAP: 0.2591\n",
      "a: -0.1899999999999984   MAP: 0.2592\n",
      "a: -0.17999999999999838   MAP: 0.2592\n",
      "a: -0.16999999999999837   MAP: 0.2592\n",
      "a: -0.15999999999999837   MAP: 0.2594\n",
      "a: -0.14999999999999836   MAP: 0.2593\n",
      "a: -0.13999999999999835   MAP: 0.2593\n",
      "a: -0.12999999999999834   MAP: 0.2593\n",
      "a: -0.11999999999999834   MAP: 0.2593\n",
      "a: -0.10999999999999835   MAP: 0.2593\n",
      "a: -0.09999999999999835   MAP: 0.2594\n",
      "a: -0.08999999999999836   MAP: 0.2594\n",
      "a: -0.07999999999999836   MAP: 0.2594\n",
      "a: -0.06999999999999837   MAP: 0.2592\n",
      "a: -0.05999999999999837   MAP: 0.2592\n",
      "a: -0.049999999999998365   MAP: 0.2592\n",
      "a: -0.03999999999999836   MAP: 0.2592\n",
      "a: -0.02999999999999836   MAP: 0.2592\n",
      "a: -0.01999999999999836   MAP: 0.2593\n",
      "a: -0.00999999999999836   MAP: 0.2593\n",
      "a: 1.6410484082740595e-15   MAP: 0.2599\n",
      "a: 0.010000000000001641   MAP: 0.2599\n",
      "a: 0.02000000000000164   MAP: 0.26\n",
      "a: 0.030000000000001643   MAP: 0.26\n",
      "a: 0.040000000000001645   MAP: 0.2601\n",
      "a: 0.05000000000000165   MAP: 0.2601\n",
      "a: 0.06000000000000165   MAP: 0.2601\n",
      "a: 0.07000000000000164   MAP: 0.2601\n",
      "a: 0.08000000000000164   MAP: 0.2602\n",
      "a: 0.09000000000000163   MAP: 0.2602\n",
      "a: 0.10000000000000163   MAP: 0.2602\n",
      "a: 0.11000000000000162   MAP: 0.2602\n",
      "a: 0.12000000000000162   MAP: 0.2602\n",
      "a: 0.13000000000000161   MAP: 0.26\n",
      "a: 0.14000000000000162   MAP: 0.26\n",
      "a: 0.15000000000000163   MAP: 0.26\n",
      "a: 0.16000000000000164   MAP: 0.26\n",
      "a: 0.17000000000000165   MAP: 0.26\n",
      "a: 0.18000000000000166   MAP: 0.26\n",
      "a: 0.19000000000000167   MAP: 0.26\n",
      "a: 0.20000000000000168   MAP: 0.2599\n",
      "a: 0.21000000000000169   MAP: 0.2599\n",
      "a: 0.2200000000000017   MAP: 0.2598\n",
      "a: 0.2300000000000017   MAP: 0.2599\n",
      "a: 0.2400000000000017   MAP: 0.2599\n",
      "a: 0.2500000000000017   MAP: 0.2599\n",
      "a: 0.26000000000000173   MAP: 0.2595\n",
      "a: 0.27000000000000174   MAP: 0.2596\n",
      "a: 0.28000000000000175   MAP: 0.2602\n",
      "a: 0.29000000000000176   MAP: 0.2602\n",
      "a: 0.30000000000000177   MAP: 0.2599\n",
      "a: 0.3100000000000018   MAP: 0.2598\n",
      "a: 0.3200000000000018   MAP: 0.2596\n",
      "a: 0.3300000000000018   MAP: 0.2597\n",
      "a: 0.3400000000000018   MAP: 0.2597\n",
      "a: 0.3500000000000018   MAP: 0.2597\n",
      "a: 0.3600000000000018   MAP: 0.2596\n",
      "a: 0.3700000000000018   MAP: 0.2596\n",
      "a: 0.38000000000000184   MAP: 0.2596\n",
      "a: 0.39000000000000185   MAP: 0.2593\n",
      "a: 0.40000000000000185   MAP: 0.26\n",
      "a: 0.41000000000000186   MAP: 0.26\n",
      "a: 0.42000000000000187   MAP: 0.2597\n",
      "a: 0.4300000000000019   MAP: 0.2597\n",
      "a: 0.4400000000000019   MAP: 0.2598\n",
      "a: 0.4500000000000019   MAP: 0.2598\n",
      "a: 0.4600000000000019   MAP: 0.2597\n",
      "a: 0.4700000000000019   MAP: 0.2597\n",
      "a: 0.4800000000000019   MAP: 0.2597\n",
      "a: 0.49000000000000193   MAP: 0.2597\n",
      "a: 0.5000000000000019   MAP: 0.2589\n",
      "a: 0.5100000000000019   MAP: 0.2602\n",
      "a: 0.5200000000000019   MAP: 0.2602\n",
      "a: 0.5300000000000019   MAP: 0.2602\n",
      "a: 0.5400000000000019   MAP: 0.2601\n",
      "a: 0.5500000000000019   MAP: 0.2601\n",
      "a: 0.5600000000000019   MAP: 0.2602\n",
      "a: 0.570000000000002   MAP: 0.2602\n",
      "a: 0.580000000000002   MAP: 0.2599\n",
      "a: 0.590000000000002   MAP: 0.2599\n",
      "a: 0.600000000000002   MAP: 0.2599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.610000000000002   MAP: 0.2599\n",
      "a: 0.620000000000002   MAP: 0.2599\n",
      "a: 0.630000000000002   MAP: 0.2599\n",
      "a: 0.640000000000002   MAP: 0.2599\n",
      "a: 0.650000000000002   MAP: 0.2599\n",
      "a: 0.660000000000002   MAP: 0.2599\n",
      "a: 0.670000000000002   MAP: 0.2594\n",
      "a: 0.680000000000002   MAP: 0.259\n",
      "a: 0.6900000000000021   MAP: 0.259\n",
      "a: 0.7000000000000021   MAP: 0.259\n",
      "a: 0.7100000000000021   MAP: 0.259\n",
      "a: 0.7200000000000021   MAP: 0.259\n",
      "a: 0.7300000000000021   MAP: 0.2583\n",
      "a: 0.7400000000000021   MAP: 0.2582\n",
      "a: 0.7500000000000021   MAP: 0.2582\n",
      "a: 0.7600000000000021   MAP: 0.2581\n",
      "a: 0.7700000000000021   MAP: 0.2582\n",
      "a: 0.7800000000000021   MAP: 0.2581\n",
      "a: 0.7900000000000021   MAP: 0.2579\n",
      "a: 0.8000000000000022   MAP: 0.2579\n",
      "a: 0.8100000000000022   MAP: 0.2578\n",
      "a: 0.8200000000000022   MAP: 0.2577\n",
      "a: 0.8300000000000022   MAP: 0.2577\n",
      "a: 0.8400000000000022   MAP: 0.2577\n",
      "a: 0.8500000000000022   MAP: 0.2577\n",
      "a: 0.8600000000000022   MAP: 0.2577\n",
      "a: 0.8700000000000022   MAP: 0.2576\n",
      "a: 0.8800000000000022   MAP: 0.2576\n",
      "a: 0.8900000000000022   MAP: 0.2575\n",
      "a: 0.9000000000000022   MAP: 0.2574\n",
      "a: 0.9100000000000023   MAP: 0.2573\n",
      "a: 0.9200000000000023   MAP: 0.2573\n",
      "a: 0.9300000000000023   MAP: 0.2572\n",
      "a: 0.9400000000000023   MAP: 0.2572\n",
      "a: 0.9500000000000023   MAP: 0.257\n",
      "a: 0.9600000000000023   MAP: 0.2569\n",
      "a: 0.9700000000000023   MAP: 0.2569\n",
      "a: 0.9800000000000023   MAP: 0.2569\n",
      "a: 0.9900000000000023   MAP: 0.2566\n",
      "a: 1.0000000000000022   MAP: 0.2565\n",
      "a: 1.0100000000000022   MAP: 0.2564\n",
      "a: 1.0200000000000022   MAP: 0.2564\n",
      "a: 1.0300000000000022   MAP: 0.2563\n",
      "a: 1.0400000000000023   MAP: 0.2562\n",
      "a: 1.0500000000000023   MAP: 0.2563\n",
      "a: 1.0600000000000023   MAP: 0.2563\n",
      "a: 1.0700000000000023   MAP: 0.256\n",
      "a: 1.0800000000000023   MAP: 0.2559\n",
      "a: 1.0900000000000023   MAP: 0.2559\n",
      "a: 1.1000000000000023   MAP: 0.2557\n",
      "a: 1.1100000000000023   MAP: 0.2556\n",
      "a: 1.1200000000000023   MAP: 0.2555\n",
      "a: 1.1300000000000023   MAP: 0.2553\n",
      "a: 1.1400000000000023   MAP: 0.2552\n",
      "a: 1.1500000000000024   MAP: 0.2551\n",
      "a: 1.1600000000000024   MAP: 0.2549\n",
      "a: 1.1700000000000024   MAP: 0.2548\n",
      "a: 1.1800000000000024   MAP: 0.2548\n",
      "a: 1.1900000000000024   MAP: 0.2547\n",
      "a: 1.2000000000000024   MAP: 0.2545\n",
      "a: 1.2100000000000024   MAP: 0.2543\n",
      "a: 1.2200000000000024   MAP: 0.2541\n",
      "a: 1.2300000000000024   MAP: 0.2541\n",
      "a: 1.2400000000000024   MAP: 0.2539\n",
      "a: 1.2500000000000024   MAP: 0.2536\n",
      "a: 1.2600000000000025   MAP: 0.2535\n",
      "a: 1.2700000000000025   MAP: 0.2533\n",
      "a: 1.2800000000000025   MAP: 0.2532\n",
      "a: 1.2900000000000025   MAP: 0.2529\n",
      "a: 1.3000000000000025   MAP: 0.2529\n",
      "a: 1.3100000000000025   MAP: 0.2528\n",
      "a: 1.3200000000000025   MAP: 0.2527\n",
      "a: 1.3300000000000025   MAP: 0.2526\n",
      "a: 1.3400000000000025   MAP: 0.2544\n",
      "a: 1.3500000000000025   MAP: 0.2542\n",
      "a: 1.3600000000000025   MAP: 0.2541\n",
      "a: 1.3700000000000025   MAP: 0.2539\n",
      "a: 1.3800000000000026   MAP: 0.2538\n",
      "a: 1.3900000000000026   MAP: 0.2537\n",
      "a: 1.4000000000000026   MAP: 0.2534\n",
      "a: 1.4100000000000026   MAP: 0.2533\n",
      "a: 1.4200000000000026   MAP: 0.2531\n",
      "a: 1.4300000000000026   MAP: 0.253\n",
      "a: 1.4400000000000026   MAP: 0.2528\n",
      "a: 1.4500000000000026   MAP: 0.2525\n",
      "a: 1.4600000000000026   MAP: 0.2522\n",
      "a: 1.4700000000000026   MAP: 0.2519\n",
      "a: 1.4800000000000026   MAP: 0.2518\n",
      "a: 1.4900000000000027   MAP: 0.2517\n",
      "a: 1.5000000000000027   MAP: 0.2516\n",
      "a: 1.5100000000000027   MAP: 0.2515\n",
      "a: 1.5200000000000027   MAP: 0.2513\n",
      "a: 1.5300000000000027   MAP: 0.2511\n",
      "a: 1.5400000000000027   MAP: 0.2511\n",
      "a: 1.5500000000000027   MAP: 0.251\n",
      "a: 1.5600000000000027   MAP: 0.2507\n",
      "a: 1.5700000000000027   MAP: 0.2504\n",
      "a: 1.5800000000000027   MAP: 0.2504\n",
      "a: 1.5900000000000027   MAP: 0.2506\n",
      "a: 1.6000000000000028   MAP: 0.2504\n",
      "a: 1.6100000000000028   MAP: 0.2501\n",
      "a: 1.6200000000000028   MAP: 0.25\n",
      "a: 1.6300000000000028   MAP: 0.2498\n",
      "a: 1.6400000000000028   MAP: 0.2497\n",
      "a: 1.6500000000000028   MAP: 0.2494\n",
      "a: 1.6600000000000028   MAP: 0.2492\n",
      "a: 1.6700000000000028   MAP: 0.2488\n",
      "a: 1.6800000000000028   MAP: 0.2485\n",
      "a: 1.6900000000000028   MAP: 0.2483\n",
      "a: 1.7000000000000028   MAP: 0.248\n",
      "a: 1.7100000000000029   MAP: 0.2478\n",
      "a: 1.7200000000000029   MAP: 0.2476\n",
      "a: 1.7300000000000029   MAP: 0.2473\n",
      "a: 1.7400000000000029   MAP: 0.247\n",
      "a: 1.7500000000000029   MAP: 0.2468\n",
      "a: 1.760000000000003   MAP: 0.2466\n",
      "a: 1.770000000000003   MAP: 0.2465\n",
      "a: 1.780000000000003   MAP: 0.2463\n",
      "a: 1.790000000000003   MAP: 0.2463\n",
      "a: 1.800000000000003   MAP: 0.2461\n",
      "a: 1.810000000000003   MAP: 0.2459\n",
      "a: 1.820000000000003   MAP: 0.2458\n",
      "a: 1.830000000000003   MAP: 0.2455\n",
      "a: 1.840000000000003   MAP: 0.2455\n",
      "a: 1.850000000000003   MAP: 0.2452\n",
      "a: 1.860000000000003   MAP: 0.2451\n",
      "a: 1.870000000000003   MAP: 0.2449\n",
      "a: 1.880000000000003   MAP: 0.2448\n",
      "a: 1.890000000000003   MAP: 0.2447\n",
      "a: 1.900000000000003   MAP: 0.2445\n",
      "a: 1.910000000000003   MAP: 0.2443\n",
      "a: 1.920000000000003   MAP: 0.2425\n",
      "a: 1.930000000000003   MAP: 0.2425\n",
      "a: 1.940000000000003   MAP: 0.2424\n",
      "a: 1.950000000000003   MAP: 0.2423\n",
      "a: 1.960000000000003   MAP: 0.242\n",
      "a: 1.970000000000003   MAP: 0.2419\n",
      "a: 1.980000000000003   MAP: 0.2418\n",
      "a: 1.990000000000003   MAP: 0.2414\n",
      "a: 2.000000000000003   MAP: 0.2413\n",
      "a: 2.010000000000003   MAP: 0.241\n",
      "a: 2.0200000000000027   MAP: 0.2406\n",
      "a: 2.0300000000000025   MAP: 0.2404\n",
      "a: 2.0400000000000023   MAP: 0.2402\n",
      "a: 2.050000000000002   MAP: 0.2401\n",
      "a: 2.060000000000002   MAP: 0.2399\n",
      "a: 2.0700000000000016   MAP: 0.2396\n",
      "a: 2.0800000000000014   MAP: 0.2394\n",
      "a: 2.090000000000001   MAP: 0.2391\n",
      "a: 2.100000000000001   MAP: 0.2387\n",
      "a: 2.1100000000000008   MAP: 0.2385\n",
      "a: 2.1200000000000006   MAP: 0.2384\n",
      "a: 2.1300000000000003   MAP: 0.2382\n",
      "a: 2.14   MAP: 0.2379\n",
      "a: 2.15   MAP: 0.2378\n",
      "a: 2.1599999999999997   MAP: 0.2375\n",
      "a: 2.1699999999999995   MAP: 0.2373\n",
      "a: 2.1799999999999993   MAP: 0.237\n",
      "a: 2.189999999999999   MAP: 0.2368\n",
      "a: 2.199999999999999   MAP: 0.2364\n",
      "a: 2.2099999999999986   MAP: 0.2362\n",
      "a: 2.2199999999999984   MAP: 0.236\n",
      "a: 2.229999999999998   MAP: 0.2359\n",
      "a: 2.239999999999998   MAP: 0.2357\n",
      "a: 2.249999999999998   MAP: 0.2355\n",
      "a: 2.2599999999999976   MAP: 0.2351\n",
      "a: 2.2699999999999974   MAP: 0.2349\n",
      "a: 2.279999999999997   MAP: 0.2347\n",
      "a: 2.289999999999997   MAP: 0.2335\n",
      "a: 2.2999999999999967   MAP: 0.2333\n",
      "a: 2.3099999999999965   MAP: 0.233\n",
      "a: 2.3199999999999963   MAP: 0.2329\n",
      "a: 2.329999999999996   MAP: 0.2328\n",
      "a: 2.339999999999996   MAP: 0.2325\n",
      "a: 2.3499999999999956   MAP: 0.2323\n",
      "a: 2.3599999999999954   MAP: 0.232\n",
      "a: 2.369999999999995   MAP: 0.2319\n",
      "a: 2.379999999999995   MAP: 0.2317\n",
      "a: 2.389999999999995   MAP: 0.2315\n",
      "a: 2.3999999999999946   MAP: 0.2315\n",
      "a: 2.4099999999999944   MAP: 0.2313\n",
      "a: 2.419999999999994   MAP: 0.2311\n",
      "a: 2.429999999999994   MAP: 0.2309\n",
      "a: 2.4399999999999937   MAP: 0.2311\n",
      "a: 2.4499999999999935   MAP: 0.231\n",
      "a: 2.4599999999999933   MAP: 0.2308\n",
      "a: 2.469999999999993   MAP: 0.2305\n",
      "a: 2.479999999999993   MAP: 0.2304\n",
      "a: 2.4899999999999927   MAP: 0.2302\n",
      "a: 2.4999999999999925   MAP: 0.23\n",
      "a: 2.5099999999999922   MAP: 0.2298\n",
      "a: 2.519999999999992   MAP: 0.2296\n",
      "a: 2.529999999999992   MAP: 0.2291\n",
      "a: 2.5399999999999916   MAP: 0.2287\n",
      "a: 2.5499999999999914   MAP: 0.2285\n",
      "a: 2.559999999999991   MAP: 0.2283\n",
      "a: 2.569999999999991   MAP: 0.2281\n",
      "a: 2.5799999999999907   MAP: 0.2278\n",
      "a: 2.5899999999999905   MAP: 0.2275\n",
      "a: 2.5999999999999903   MAP: 0.2271\n",
      "a: 2.60999999999999   MAP: 0.2269\n",
      "a: 2.61999999999999   MAP: 0.2268\n",
      "a: 2.6299999999999897   MAP: 0.2267\n",
      "a: 2.6399999999999895   MAP: 0.2265\n",
      "a: 2.6499999999999893   MAP: 0.2262\n",
      "a: 2.659999999999989   MAP: 0.2261\n",
      "a: 2.669999999999989   MAP: 0.2258\n",
      "a: 2.6799999999999886   MAP: 0.2254\n",
      "a: 2.6899999999999884   MAP: 0.2252\n",
      "a: 2.699999999999988   MAP: 0.225\n",
      "a: 2.709999999999988   MAP: 0.2249\n",
      "a: 2.7199999999999878   MAP: 0.2246\n",
      "a: 2.7299999999999875   MAP: 0.2246\n",
      "a: 2.7399999999999873   MAP: 0.2245\n",
      "a: 2.749999999999987   MAP: 0.2241\n",
      "a: 2.759999999999987   MAP: 0.2237\n",
      "a: 2.7699999999999867   MAP: 0.2235\n",
      "a: 2.7799999999999865   MAP: 0.2231\n",
      "a: 2.7899999999999863   MAP: 0.223\n",
      "a: 2.799999999999986   MAP: 0.2228\n",
      "a: 2.809999999999986   MAP: 0.2226\n",
      "a: 2.8199999999999856   MAP: 0.2224\n",
      "a: 2.8299999999999854   MAP: 0.222\n",
      "a: 2.839999999999985   MAP: 0.2218\n",
      "a: 2.849999999999985   MAP: 0.2216\n",
      "a: 2.8599999999999848   MAP: 0.2215\n",
      "a: 2.8699999999999846   MAP: 0.2212\n",
      "a: 2.8799999999999844   MAP: 0.2209\n",
      "a: 2.889999999999984   MAP: 0.2207\n",
      "a: 2.899999999999984   MAP: 0.2205\n",
      "a: 2.9099999999999837   MAP: 0.2202\n",
      "a: 2.9199999999999835   MAP: 0.2201\n",
      "a: 2.9299999999999833   MAP: 0.2198\n",
      "a: 2.939999999999983   MAP: 0.2196\n",
      "a: 2.949999999999983   MAP: 0.2194\n",
      "a: 2.9599999999999826   MAP: 0.2191\n",
      "a: 2.9699999999999824   MAP: 0.219\n",
      "a: 2.979999999999982   MAP: 0.2188\n",
      "a: 2.989999999999982   MAP: 0.2185\n",
      "a: 2.999999999999982   MAP: 0.2183\n",
      "a: 3.0099999999999816   MAP: 0.2181\n",
      "a: 3.0199999999999814   MAP: 0.218\n",
      "a: 3.029999999999981   MAP: 0.2178\n",
      "a: 3.039999999999981   MAP: 0.2176\n",
      "a: 3.0499999999999807   MAP: 0.2174\n",
      "a: 3.0599999999999805   MAP: 0.2172\n",
      "a: 3.0699999999999803   MAP: 0.217\n",
      "a: 3.07999999999998   MAP: 0.2167\n",
      "a: 3.08999999999998   MAP: 0.2163\n",
      "a: 3.0999999999999797   MAP: 0.216\n",
      "a: 3.1099999999999794   MAP: 0.2159\n",
      "a: 3.1199999999999792   MAP: 0.2155\n",
      "a: 3.129999999999979   MAP: 0.2153\n",
      "a: 3.139999999999979   MAP: 0.2151\n",
      "a: 3.1499999999999786   MAP: 0.2149\n",
      "a: 3.1599999999999784   MAP: 0.2147\n",
      "a: 3.169999999999978   MAP: 0.2146\n",
      "a: 3.179999999999978   MAP: 0.2144\n",
      "a: 3.1899999999999777   MAP: 0.2141\n",
      "a: 3.1999999999999775   MAP: 0.2138\n",
      "a: 3.2099999999999773   MAP: 0.2136\n",
      "a: 3.219999999999977   MAP: 0.2133\n",
      "a: 3.229999999999977   MAP: 0.213\n",
      "a: 3.2399999999999767   MAP: 0.2128\n",
      "a: 3.2499999999999765   MAP: 0.2125\n",
      "a: 3.2599999999999763   MAP: 0.2124\n",
      "a: 3.269999999999976   MAP: 0.2123\n",
      "a: 3.279999999999976   MAP: 0.2121\n",
      "a: 3.2899999999999756   MAP: 0.2118\n",
      "a: 3.2999999999999754   MAP: 0.2116\n",
      "a: 3.309999999999975   MAP: 0.2114\n",
      "a: 3.319999999999975   MAP: 0.2112\n",
      "a: 3.3299999999999748   MAP: 0.2109\n",
      "a: 3.3399999999999745   MAP: 0.2107\n",
      "a: 3.3499999999999743   MAP: 0.2104\n",
      "a: 3.359999999999974   MAP: 0.2103\n",
      "a: 3.369999999999974   MAP: 0.2101\n",
      "a: 3.3799999999999737   MAP: 0.2098\n",
      "a: 3.3899999999999735   MAP: 0.2095\n",
      "a: 3.3999999999999733   MAP: 0.2094\n",
      "a: 3.409999999999973   MAP: 0.2092\n",
      "a: 3.419999999999973   MAP: 0.2091\n",
      "a: 3.4299999999999726   MAP: 0.2088\n",
      "a: 3.4399999999999724   MAP: 0.2086\n",
      "a: 3.449999999999972   MAP: 0.2084\n",
      "a: 3.459999999999972   MAP: 0.2082\n",
      "a: 3.4699999999999718   MAP: 0.2079\n",
      "a: 3.4799999999999716   MAP: 0.2076\n",
      "a: 3.4899999999999713   MAP: 0.2074\n",
      "a: 3.499999999999971   MAP: 0.2073\n",
      "a: 3.509999999999971   MAP: 0.2071\n",
      "a: 3.5199999999999707   MAP: 0.2069\n",
      "a: 3.5299999999999705   MAP: 0.2067\n",
      "a: 3.5399999999999703   MAP: 0.2066\n",
      "a: 3.54999999999997   MAP: 0.2064\n",
      "a: 3.55999999999997   MAP: 0.2062\n",
      "a: 3.5699999999999696   MAP: 0.2061\n",
      "a: 3.5799999999999694   MAP: 0.2059\n",
      "a: 3.589999999999969   MAP: 0.2053\n",
      "a: 3.599999999999969   MAP: 0.2051\n",
      "a: 3.609999999999969   MAP: 0.2048\n",
      "a: 3.6199999999999686   MAP: 0.2048\n",
      "a: 3.6299999999999684   MAP: 0.2046\n",
      "a: 3.639999999999968   MAP: 0.2043\n",
      "a: 3.649999999999968   MAP: 0.2041\n",
      "a: 3.6599999999999677   MAP: 0.2038\n",
      "a: 3.6699999999999675   MAP: 0.2037\n",
      "a: 3.6799999999999673   MAP: 0.2035\n",
      "a: 3.689999999999967   MAP: 0.2031\n",
      "a: 3.699999999999967   MAP: 0.2028\n",
      "a: 3.7099999999999667   MAP: 0.2027\n",
      "a: 3.7199999999999664   MAP: 0.2023\n",
      "a: 3.7299999999999662   MAP: 0.202\n",
      "a: 3.739999999999966   MAP: 0.2018\n",
      "a: 3.749999999999966   MAP: 0.2016\n",
      "a: 3.7599999999999656   MAP: 0.2015\n",
      "a: 3.7699999999999654   MAP: 0.2013\n",
      "a: 3.779999999999965   MAP: 0.2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 3.789999999999965   MAP: 0.2009\n",
      "a: 3.7999999999999647   MAP: 0.2007\n",
      "a: 3.8099999999999645   MAP: 0.2005\n",
      "a: 3.8199999999999643   MAP: 0.2004\n",
      "a: 3.829999999999964   MAP: 0.2\n",
      "a: 3.839999999999964   MAP: 0.1997\n",
      "a: 3.8499999999999637   MAP: 0.1996\n",
      "a: 3.8599999999999635   MAP: 0.1993\n",
      "a: 3.8699999999999632   MAP: 0.1991\n",
      "a: 3.879999999999963   MAP: 0.199\n",
      "a: 3.889999999999963   MAP: 0.1987\n",
      "a: 3.8999999999999626   MAP: 0.1984\n",
      "a: 3.9099999999999624   MAP: 0.1983\n",
      "a: 3.919999999999962   MAP: 0.1982\n",
      "a: 3.929999999999962   MAP: 0.198\n",
      "a: 3.9399999999999618   MAP: 0.1978\n",
      "a: 3.9499999999999615   MAP: 0.1977\n",
      "a: 3.9599999999999613   MAP: 0.1975\n",
      "a: 3.969999999999961   MAP: 0.1974\n",
      "a: 3.979999999999961   MAP: 0.1971\n",
      "a: 3.9899999999999607   MAP: 0.1969\n",
      "a: 3.9999999999999605   MAP: 0.1966\n",
      "a: 4.009999999999961   MAP: 0.1964\n",
      "a: 4.0199999999999605   MAP: 0.1962\n",
      "a: 4.02999999999996   MAP: 0.196\n",
      "a: 4.03999999999996   MAP: 0.1959\n",
      "a: 4.04999999999996   MAP: 0.1956\n",
      "a: 4.05999999999996   MAP: 0.1955\n",
      "a: 4.069999999999959   MAP: 0.1953\n",
      "a: 4.079999999999959   MAP: 0.1951\n",
      "a: 4.089999999999959   MAP: 0.195\n",
      "a: 4.099999999999959   MAP: 0.1948\n",
      "a: 4.109999999999959   MAP: 0.1946\n",
      "a: 4.119999999999958   MAP: 0.1943\n",
      "a: 4.129999999999958   MAP: 0.1941\n",
      "a: 4.139999999999958   MAP: 0.1938\n",
      "a: 4.149999999999958   MAP: 0.1935\n",
      "a: 4.1599999999999575   MAP: 0.1931\n",
      "a: 4.169999999999957   MAP: 0.1928\n",
      "a: 4.179999999999957   MAP: 0.1925\n",
      "a: 4.189999999999957   MAP: 0.1923\n",
      "a: 4.199999999999957   MAP: 0.1921\n",
      "a: 4.209999999999956   MAP: 0.1919\n",
      "a: 4.219999999999956   MAP: 0.1917\n",
      "a: 4.229999999999956   MAP: 0.1916\n",
      "a: 4.239999999999956   MAP: 0.1913\n",
      "a: 4.249999999999956   MAP: 0.1912\n",
      "a: 4.259999999999955   MAP: 0.191\n",
      "a: 4.269999999999955   MAP: 0.1909\n",
      "a: 4.279999999999955   MAP: 0.1907\n",
      "a: 4.289999999999955   MAP: 0.1906\n",
      "a: 4.2999999999999545   MAP: 0.1905\n",
      "a: 4.309999999999954   MAP: 0.1905\n",
      "a: 4.319999999999954   MAP: 0.1903\n",
      "a: 4.329999999999954   MAP: 0.1901\n",
      "a: 4.339999999999954   MAP: 0.19\n",
      "a: 4.3499999999999535   MAP: 0.1899\n",
      "a: 4.359999999999953   MAP: 0.1897\n",
      "a: 4.369999999999953   MAP: 0.1888\n",
      "a: 4.379999999999953   MAP: 0.1886\n",
      "a: 4.389999999999953   MAP: 0.1884\n",
      "a: 4.399999999999952   MAP: 0.1881\n",
      "a: 4.409999999999952   MAP: 0.1911\n",
      "a: 4.419999999999952   MAP: 0.1908\n",
      "a: 4.429999999999952   MAP: 0.1905\n",
      "a: 4.4399999999999515   MAP: 0.1903\n",
      "a: 4.449999999999951   MAP: 0.1902\n",
      "a: 4.459999999999951   MAP: 0.1901\n",
      "a: 4.469999999999951   MAP: 0.1898\n",
      "a: 4.479999999999951   MAP: 0.1895\n",
      "a: 4.4899999999999505   MAP: 0.189\n",
      "a: 4.49999999999995   MAP: 0.1889\n",
      "a: 4.50999999999995   MAP: 0.1888\n",
      "a: 4.51999999999995   MAP: 0.1886\n",
      "a: 4.52999999999995   MAP: 0.1884\n",
      "a: 4.539999999999949   MAP: 0.1882\n",
      "a: 4.549999999999949   MAP: 0.1878\n",
      "a: 4.559999999999949   MAP: 0.1877\n",
      "a: 4.569999999999949   MAP: 0.187\n",
      "a: 4.579999999999949   MAP: 0.1866\n",
      "a: 4.589999999999948   MAP: 0.1864\n",
      "a: 4.599999999999948   MAP: 0.1863\n",
      "a: 4.609999999999948   MAP: 0.1862\n",
      "a: 4.619999999999948   MAP: 0.1858\n",
      "a: 4.6299999999999475   MAP: 0.1854\n",
      "a: 4.639999999999947   MAP: 0.1852\n",
      "a: 4.649999999999947   MAP: 0.185\n",
      "a: 4.659999999999947   MAP: 0.1847\n",
      "a: 4.669999999999947   MAP: 0.1846\n",
      "a: 4.679999999999946   MAP: 0.1845\n",
      "a: 4.689999999999946   MAP: 0.1842\n",
      "a: 4.699999999999946   MAP: 0.184\n",
      "a: 4.709999999999946   MAP: 0.1838\n",
      "a: 4.719999999999946   MAP: 0.1836\n",
      "a: 4.729999999999945   MAP: 0.1835\n",
      "a: 4.739999999999945   MAP: 0.1831\n",
      "a: 4.749999999999945   MAP: 0.1829\n",
      "a: 4.759999999999945   MAP: 0.1827\n",
      "a: 4.7699999999999445   MAP: 0.1826\n",
      "a: 4.779999999999944   MAP: 0.1825\n",
      "a: 4.789999999999944   MAP: 0.1824\n",
      "a: 4.799999999999944   MAP: 0.1821\n",
      "a: 4.809999999999944   MAP: 0.182\n",
      "a: 4.819999999999943   MAP: 0.1819\n",
      "a: 4.829999999999943   MAP: 0.1817\n",
      "a: 4.839999999999943   MAP: 0.1804\n",
      "a: 4.849999999999943   MAP: 0.1801\n",
      "a: 4.859999999999943   MAP: 0.18\n",
      "a: 4.869999999999942   MAP: 0.1797\n",
      "a: 4.879999999999942   MAP: 0.1796\n",
      "a: 4.889999999999942   MAP: 0.1794\n",
      "a: 4.899999999999942   MAP: 0.1792\n",
      "a: 4.9099999999999415   MAP: 0.179\n",
      "a: 4.919999999999941   MAP: 0.1788\n",
      "a: 4.929999999999941   MAP: 0.1786\n",
      "a: 4.939999999999941   MAP: 0.1784\n",
      "a: 4.949999999999941   MAP: 0.1781\n",
      "a: 4.9599999999999405   MAP: 0.1779\n",
      "a: 4.96999999999994   MAP: 0.1776\n",
      "a: 4.97999999999994   MAP: 0.1774\n",
      "a: 4.98999999999994   MAP: 0.1773\n",
      "a: 4.99999999999994   MAP: 0.1771\n",
      "a: 5.009999999999939   MAP: 0.177\n",
      "a: 5.019999999999939   MAP: 0.1768\n",
      "a: 5.029999999999939   MAP: 0.1766\n",
      "a: 5.039999999999939   MAP: 0.1764\n",
      "a: 5.0499999999999385   MAP: 0.1763\n",
      "a: 5.059999999999938   MAP: 0.1761\n",
      "a: 5.069999999999938   MAP: 0.176\n",
      "a: 5.079999999999938   MAP: 0.1759\n",
      "a: 5.089999999999938   MAP: 0.1757\n",
      "a: 5.0999999999999375   MAP: 0.1755\n",
      "a: 5.109999999999937   MAP: 0.1722\n",
      "a: 5.119999999999937   MAP: 0.1721\n",
      "a: 5.129999999999937   MAP: 0.172\n",
      "a: 5.139999999999937   MAP: 0.1718\n",
      "a: 5.149999999999936   MAP: 0.1717\n",
      "a: 5.159999999999936   MAP: 0.1715\n",
      "a: 5.169999999999936   MAP: 0.1712\n",
      "a: 5.179999999999936   MAP: 0.1711\n",
      "a: 5.1899999999999356   MAP: 0.1709\n",
      "a: 5.199999999999935   MAP: 0.1709\n",
      "a: 5.209999999999935   MAP: 0.1707\n",
      "a: 5.219999999999935   MAP: 0.1706\n",
      "a: 5.229999999999935   MAP: 0.1707\n",
      "a: 5.2399999999999345   MAP: 0.1706\n",
      "a: 5.249999999999934   MAP: 0.1704\n",
      "a: 5.259999999999934   MAP: 0.1703\n",
      "a: 5.269999999999934   MAP: 0.1659\n",
      "a: 5.279999999999934   MAP: 0.1658\n",
      "a: 5.289999999999933   MAP: 0.1657\n",
      "a: 5.299999999999933   MAP: 0.1656\n",
      "a: 5.309999999999933   MAP: 0.1655\n",
      "a: 5.319999999999933   MAP: 0.1653\n",
      "a: 5.329999999999933   MAP: 0.1652\n",
      "a: 5.339999999999932   MAP: 0.1632\n",
      "a: 5.349999999999932   MAP: 0.1631\n",
      "a: 5.359999999999932   MAP: 0.1629\n",
      "a: 5.369999999999932   MAP: 0.1628\n",
      "a: 5.3799999999999315   MAP: 0.1627\n",
      "a: 5.389999999999931   MAP: 0.1626\n",
      "a: 5.399999999999931   MAP: 0.1625\n",
      "a: 5.409999999999931   MAP: 0.1604\n",
      "a: 5.419999999999931   MAP: 0.1602\n",
      "a: 5.42999999999993   MAP: 0.1601\n",
      "a: 5.43999999999993   MAP: 0.16\n",
      "a: 5.44999999999993   MAP: 0.1598\n",
      "a: 5.45999999999993   MAP: 0.1597\n",
      "a: 5.46999999999993   MAP: 0.1596\n",
      "a: 5.479999999999929   MAP: 0.1595\n",
      "a: 5.489999999999929   MAP: 0.1593\n",
      "a: 5.499999999999929   MAP: 0.1592\n",
      "a: 5.509999999999929   MAP: 0.1591\n",
      "a: 5.5199999999999285   MAP: 0.159\n",
      "a: 5.529999999999928   MAP: 0.1589\n",
      "a: 5.539999999999928   MAP: 0.1587\n",
      "a: 5.549999999999928   MAP: 0.1586\n",
      "a: 5.559999999999928   MAP: 0.1585\n",
      "a: 5.5699999999999275   MAP: 0.1585\n",
      "a: 5.579999999999927   MAP: 0.1583\n",
      "a: 5.589999999999927   MAP: 0.1579\n",
      "a: 5.599999999999927   MAP: 0.1577\n",
      "a: 5.609999999999927   MAP: 0.1576\n",
      "a: 5.619999999999926   MAP: 0.1575\n",
      "a: 5.629999999999926   MAP: 0.1574\n",
      "a: 5.639999999999926   MAP: 0.1574\n",
      "a: 5.649999999999926   MAP: 0.1573\n",
      "a: 5.6599999999999255   MAP: 0.1559\n",
      "a: 5.669999999999925   MAP: 0.1557\n",
      "a: 5.679999999999925   MAP: 0.1556\n",
      "a: 5.689999999999925   MAP: 0.1555\n",
      "a: 5.699999999999925   MAP: 0.1554\n",
      "a: 5.7099999999999245   MAP: 0.1553\n",
      "a: 5.719999999999924   MAP: 0.1543\n",
      "a: 5.729999999999924   MAP: 0.1543\n",
      "a: 5.739999999999924   MAP: 0.1542\n",
      "a: 5.749999999999924   MAP: 0.154\n",
      "a: 5.759999999999923   MAP: 0.1534\n",
      "a: 5.769999999999923   MAP: 0.1533\n",
      "a: 5.779999999999923   MAP: 0.1532\n",
      "a: 5.789999999999923   MAP: 0.153\n",
      "a: 5.7999999999999226   MAP: 0.1529\n",
      "a: 5.809999999999922   MAP: 0.1528\n",
      "a: 5.819999999999922   MAP: 0.1527\n",
      "a: 5.829999999999922   MAP: 0.1526\n",
      "a: 5.839999999999922   MAP: 0.1525\n",
      "a: 5.8499999999999215   MAP: 0.1524\n",
      "a: 5.859999999999921   MAP: 0.1523\n",
      "a: 5.869999999999921   MAP: 0.1522\n",
      "a: 5.879999999999921   MAP: 0.1521\n",
      "a: 5.889999999999921   MAP: 0.152\n",
      "a: 5.89999999999992   MAP: 0.1519\n",
      "a: 5.90999999999992   MAP: 0.1518\n",
      "a: 5.91999999999992   MAP: 0.1517\n",
      "a: 5.92999999999992   MAP: 0.1516\n",
      "a: 5.93999999999992   MAP: 0.1516\n",
      "a: 5.949999999999919   MAP: 0.1515\n",
      "a: 5.959999999999919   MAP: 0.1513\n",
      "a: 5.969999999999919   MAP: 0.1511\n",
      "a: 5.979999999999919   MAP: 0.1511\n",
      "a: 5.9899999999999185   MAP: 0.151\n",
      "a: 5.999999999999918   MAP: 0.1508\n",
      "a: 6.009999999999918   MAP: 0.1507\n",
      "a: 6.019999999999918   MAP: 0.1505\n",
      "a: 6.029999999999918   MAP: 0.1505\n",
      "a: 6.039999999999917   MAP: 0.1503\n",
      "a: 6.049999999999917   MAP: 0.1501\n",
      "a: 6.059999999999917   MAP: 0.1501\n",
      "a: 6.069999999999917   MAP: 0.15\n",
      "a: 6.079999999999917   MAP: 0.1499\n",
      "a: 6.089999999999916   MAP: 0.1489\n",
      "a: 6.099999999999916   MAP: 0.1488\n",
      "a: 6.109999999999916   MAP: 0.1479\n",
      "a: 6.119999999999916   MAP: 0.1478\n",
      "a: 6.1299999999999155   MAP: 0.1476\n",
      "a: 6.139999999999915   MAP: 0.1474\n",
      "a: 6.149999999999915   MAP: 0.1473\n",
      "a: 6.159999999999915   MAP: 0.1472\n",
      "a: 6.169999999999915   MAP: 0.147\n",
      "a: 6.1799999999999145   MAP: 0.1469\n",
      "a: 6.189999999999914   MAP: 0.1468\n",
      "a: 6.199999999999914   MAP: 0.1467\n",
      "a: 6.209999999999914   MAP: 0.1466\n",
      "a: 6.219999999999914   MAP: 0.1464\n",
      "a: 6.229999999999913   MAP: 0.1463\n",
      "a: 6.239999999999913   MAP: 0.1462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 6.249999999999913   MAP: 0.1461\n",
      "a: 6.259999999999913   MAP: 0.1459\n",
      "a: 6.2699999999999125   MAP: 0.1458\n",
      "a: 6.279999999999912   MAP: 0.1456\n",
      "a: 6.289999999999912   MAP: 0.1455\n",
      "a: 6.299999999999912   MAP: 0.1454\n",
      "a: 6.309999999999912   MAP: 0.144\n",
      "a: 6.3199999999999115   MAP: 0.1438\n",
      "a: 6.329999999999911   MAP: 0.1437\n",
      "a: 6.339999999999911   MAP: 0.1435\n",
      "a: 6.349999999999911   MAP: 0.1435\n",
      "a: 6.359999999999911   MAP: 0.1434\n",
      "a: 6.36999999999991   MAP: 0.1432\n",
      "a: 6.37999999999991   MAP: 0.1431\n",
      "a: 6.38999999999991   MAP: 0.1429\n",
      "a: 6.39999999999991   MAP: 0.1428\n",
      "a: 6.4099999999999095   MAP: 0.1427\n",
      "a: 6.419999999999909   MAP: 0.1425\n",
      "a: 6.429999999999909   MAP: 0.1424\n",
      "a: 6.439999999999909   MAP: 0.1423\n",
      "a: 6.449999999999909   MAP: 0.1422\n",
      "a: 6.4599999999999085   MAP: 0.1421\n",
      "a: 6.469999999999908   MAP: 0.142\n",
      "a: 6.479999999999908   MAP: 0.1418\n",
      "a: 6.489999999999908   MAP: 0.1417\n",
      "a: 6.499999999999908   MAP: 0.1416\n",
      "a: 6.509999999999907   MAP: 0.1415\n",
      "a: 6.519999999999907   MAP: 0.1415\n",
      "a: 6.529999999999907   MAP: 0.1414\n",
      "a: 6.539999999999907   MAP: 0.1413\n",
      "a: 6.549999999999907   MAP: 0.1412\n",
      "a: 6.559999999999906   MAP: 0.1411\n",
      "a: 6.569999999999906   MAP: 0.141\n",
      "a: 6.579999999999906   MAP: 0.1406\n",
      "a: 6.589999999999906   MAP: 0.1405\n",
      "a: 6.5999999999999055   MAP: 0.1405\n",
      "a: 6.609999999999905   MAP: 0.1404\n",
      "a: 6.619999999999905   MAP: 0.1403\n",
      "a: 6.629999999999905   MAP: 0.1402\n",
      "a: 6.639999999999905   MAP: 0.1401\n",
      "a: 6.649999999999904   MAP: 0.1401\n",
      "a: 6.659999999999904   MAP: 0.14\n",
      "a: 6.669999999999904   MAP: 0.1399\n",
      "a: 6.679999999999904   MAP: 0.1399\n",
      "a: 6.689999999999904   MAP: 0.1398\n",
      "a: 6.699999999999903   MAP: 0.1397\n",
      "a: 6.709999999999903   MAP: 0.1396\n",
      "a: 6.719999999999903   MAP: 0.1394\n",
      "a: 6.729999999999903   MAP: 0.1393\n",
      "a: 6.7399999999999025   MAP: 0.1392\n",
      "a: 6.749999999999902   MAP: 0.139\n",
      "a: 6.759999999999902   MAP: 0.1369\n",
      "a: 6.769999999999902   MAP: 0.1368\n",
      "a: 6.779999999999902   MAP: 0.1367\n",
      "a: 6.7899999999999014   MAP: 0.1365\n",
      "a: 6.799999999999901   MAP: 0.1365\n",
      "a: 6.809999999999901   MAP: 0.1364\n",
      "a: 6.819999999999901   MAP: 0.1364\n",
      "a: 6.829999999999901   MAP: 0.135\n",
      "a: 6.8399999999999   MAP: 0.1349\n",
      "a: 6.8499999999999   MAP: 0.1348\n",
      "a: 6.8599999999999   MAP: 0.1346\n",
      "a: 6.8699999999999   MAP: 0.1345\n",
      "a: 6.8799999999998995   MAP: 0.1344\n",
      "a: 6.889999999999899   MAP: 0.1342\n",
      "a: 6.899999999999899   MAP: 0.1342\n",
      "a: 6.909999999999899   MAP: 0.1338\n",
      "a: 6.919999999999899   MAP: 0.1337\n",
      "a: 6.9299999999998985   MAP: 0.1336\n",
      "a: 6.939999999999898   MAP: 0.1335\n",
      "a: 6.949999999999898   MAP: 0.1334\n",
      "a: 6.959999999999898   MAP: 0.1333\n",
      "a: 6.969999999999898   MAP: 0.1332\n",
      "a: 6.979999999999897   MAP: 0.1331\n",
      "a: 6.989999999999897   MAP: 0.133\n",
      "a: 6.999999999999897   MAP: 0.1329\n",
      "a: 7.009999999999897   MAP: 0.1328\n",
      "a: 7.0199999999998965   MAP: 0.1327\n",
      "a: 7.029999999999896   MAP: 0.1326\n",
      "a: 7.039999999999896   MAP: 0.1326\n",
      "a: 7.049999999999896   MAP: 0.1325\n",
      "a: 7.059999999999896   MAP: 0.1317\n",
      "a: 7.0699999999998955   MAP: 0.1317\n",
      "a: 7.079999999999895   MAP: 0.1316\n",
      "a: 7.089999999999895   MAP: 0.1315\n",
      "a: 7.099999999999895   MAP: 0.1314\n",
      "a: 7.109999999999895   MAP: 0.1314\n",
      "a: 7.119999999999894   MAP: 0.1313\n",
      "a: 7.129999999999894   MAP: 0.1312\n",
      "a: 7.139999999999894   MAP: 0.1312\n",
      "a: 7.149999999999894   MAP: 0.131\n",
      "a: 7.159999999999894   MAP: 0.131\n",
      "a: 7.169999999999893   MAP: 0.1309\n",
      "a: 7.179999999999893   MAP: 0.1308\n",
      "a: 7.189999999999893   MAP: 0.1307\n",
      "a: 7.199999999999893   MAP: 0.1307\n",
      "a: 7.2099999999998925   MAP: 0.1306\n",
      "a: 7.219999999999892   MAP: 0.1304\n",
      "a: 7.229999999999892   MAP: 0.1304\n",
      "a: 7.239999999999892   MAP: 0.1303\n",
      "a: 7.249999999999892   MAP: 0.1302\n",
      "a: 7.259999999999891   MAP: 0.1299\n",
      "a: 7.269999999999891   MAP: 0.1298\n",
      "a: 7.279999999999891   MAP: 0.1297\n",
      "a: 7.289999999999891   MAP: 0.1296\n",
      "a: 7.299999999999891   MAP: 0.1295\n",
      "a: 7.30999999999989   MAP: 0.1293\n",
      "a: 7.31999999999989   MAP: 0.1292\n",
      "a: 7.32999999999989   MAP: 0.1291\n",
      "a: 7.33999999999989   MAP: 0.129\n",
      "a: 7.3499999999998895   MAP: 0.129\n",
      "a: 7.359999999999889   MAP: 0.1289\n",
      "a: 7.369999999999889   MAP: 0.1289\n",
      "a: 7.379999999999889   MAP: 0.1288\n",
      "a: 7.389999999999889   MAP: 0.1287\n",
      "a: 7.3999999999998884   MAP: 0.1287\n",
      "a: 7.409999999999888   MAP: 0.1286\n",
      "a: 7.419999999999888   MAP: 0.1285\n",
      "a: 7.429999999999888   MAP: 0.1284\n",
      "a: 7.439999999999888   MAP: 0.1283\n",
      "a: 7.449999999999887   MAP: 0.1283\n",
      "a: 7.459999999999887   MAP: 0.1282\n",
      "a: 7.469999999999887   MAP: 0.1281\n",
      "a: 7.479999999999887   MAP: 0.128\n",
      "a: 7.4899999999998865   MAP: 0.1277\n",
      "a: 7.499999999999886   MAP: 0.1276\n",
      "a: 7.509999999999886   MAP: 0.1275\n",
      "a: 7.519999999999886   MAP: 0.1274\n",
      "a: 7.529999999999886   MAP: 0.1268\n",
      "a: 7.5399999999998855   MAP: 0.1267\n",
      "a: 7.549999999999885   MAP: 0.1267\n",
      "a: 7.559999999999885   MAP: 0.1266\n",
      "a: 7.569999999999885   MAP: 0.1265\n",
      "a: 7.579999999999885   MAP: 0.1244\n",
      "a: 7.589999999999884   MAP: 0.1243\n",
      "a: 7.599999999999884   MAP: 0.1243\n",
      "a: 7.609999999999884   MAP: 0.1242\n",
      "a: 7.619999999999884   MAP: 0.1242\n",
      "a: 7.6299999999998835   MAP: 0.1241\n",
      "a: 7.639999999999883   MAP: 0.1241\n",
      "a: 7.649999999999883   MAP: 0.1239\n",
      "a: 7.659999999999883   MAP: 0.1238\n",
      "a: 7.669999999999883   MAP: 0.1237\n",
      "a: 7.6799999999998825   MAP: 0.1237\n",
      "a: 7.689999999999882   MAP: 0.1236\n",
      "a: 7.699999999999882   MAP: 0.1235\n",
      "a: 7.709999999999882   MAP: 0.1235\n",
      "a: 7.719999999999882   MAP: 0.1234\n",
      "a: 7.729999999999881   MAP: 0.1234\n",
      "a: 7.739999999999881   MAP: 0.1234\n",
      "a: 7.749999999999881   MAP: 0.1232\n",
      "a: 7.759999999999881   MAP: 0.1232\n",
      "a: 7.769999999999881   MAP: 0.123\n",
      "a: 7.77999999999988   MAP: 0.123\n",
      "a: 7.78999999999988   MAP: 0.1228\n",
      "a: 7.79999999999988   MAP: 0.1228\n",
      "a: 7.80999999999988   MAP: 0.1223\n",
      "a: 7.8199999999998795   MAP: 0.1223\n",
      "a: 7.829999999999879   MAP: 0.1222\n",
      "a: 7.839999999999879   MAP: 0.1221\n",
      "a: 7.849999999999879   MAP: 0.122\n",
      "a: 7.859999999999879   MAP: 0.122\n",
      "a: 7.869999999999878   MAP: 0.122\n",
      "a: 7.879999999999878   MAP: 0.1219\n",
      "a: 7.889999999999878   MAP: 0.1219\n",
      "a: 7.899999999999878   MAP: 0.1218\n",
      "a: 7.909999999999878   MAP: 0.1218\n",
      "a: 7.919999999999877   MAP: 0.1217\n",
      "a: 7.929999999999877   MAP: 0.1216\n",
      "a: 7.939999999999877   MAP: 0.1215\n",
      "a: 7.949999999999877   MAP: 0.1215\n",
      "a: 7.9599999999998765   MAP: 0.1214\n",
      "a: 7.969999999999876   MAP: 0.1212\n",
      "a: 7.979999999999876   MAP: 0.1211\n",
      "a: 7.989999999999876   MAP: 0.121\n",
      "a: 7.999999999999876   MAP: 0.121\n"
     ]
    }
   ],
   "source": [
    "a = -2 ## alpha\n",
    "n = 1000\n",
    "delta = 0.01\n",
    "top = [t.split() for t in train_q['bm25_top1000']]\n",
    "ans = [t.split() for t in train_q['pos_doc_ids']]\n",
    "for num in range(n): #每次加delta:\n",
    "    pred_list = []\n",
    "#     all_score = test_score_list + a * bert_test\n",
    "    ans_list = ans[:20]\n",
    "    for i in range(20):\n",
    "        bert_ans = []\n",
    "        all_score = test_score_list[i] + a * bert_test[i]\n",
    "        score = sorted(range(1000),reverse = True,key = lambda k : all_score[k])\n",
    "        for j in range(1000):\n",
    "            bert_ans.append(top[i][score[j]])\n",
    "        pred_list.append(bert_ans)\n",
    "#     print(pred_list)\n",
    "    result = MAP(pred_list) ##要傳25x1000\n",
    "    a += delta\n",
    "    print('a:',a,'  MAP:',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_result_BM25_Bert_a_2_84.txt','w') as f:\n",
    "    f.write('query_id,ranked_doc_ids\\n')\n",
    "    for i in range(80): \n",
    "        f.write(str(test_query_list[i]))\n",
    "        f.write(',')\n",
    "        for j in range(len(all_ans_list_1[0])):\n",
    "            f.write(all_ans_list_1[i][j])\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_result_BM25_Bert_a_2_all_epoch3.txt','w') as f:\n",
    "    f.write('query_id,ranked_doc_ids\\n')\n",
    "    for i in range(80): \n",
    "        f.write(str(test_query_list[i]))\n",
    "        f.write(',')\n",
    "        for j in range(len(all_ans_list_2[0])):\n",
    "            f.write(all_ans_list_2[i][j])\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_result_BM25_Bert_a_3_all_epoch3.txt','w') as f:\n",
    "    f.write('query_id,ranked_doc_ids\\n')\n",
    "    for i in range(80): \n",
    "        f.write(str(test_query_list[i]))\n",
    "        f.write(',')\n",
    "        for j in range(len(all_ans_list_3[0])):\n",
    "            f.write(all_ans_list_3[i][j])\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_only_Bert.txt','w') as f:\n",
    "    f.write('query_id,ranked_doc_ids\\n')\n",
    "    for i in range(80): \n",
    "        f.write(str(test_query_list[i]))\n",
    "        f.write(',')\n",
    "        for j in range(len(bert_ans_list[0])):\n",
    "            f.write(bert_ans_list[i][j])\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1 = np.load('./HW6_model/84_bert_score.npy')\n",
    "score_2 = np.load('./HW6_model/85_bert_score.npy')\n",
    "score_3 = np.load('./HW6_model/new_bert_score.npy')\n",
    "score_4 = np.load('./HW6_model/bert_score_3.npy')\n",
    "score_5 = np.load('./HW6_model/bert_score_4.npy')\n",
    "score_6 = np.load('./HW6_model/91_bert_score.npy')\n",
    "score_7 = np.load('./HW6_model/bert_score_6.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.2034130096435547\n",
      "2.0394632816314697\n"
     ]
    }
   ],
   "source": [
    "print(bert_score_list[0].min())\n",
    "print(bert_score_list[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19857574  0.2864981   0.30835772  0.91545808  0.44452697  0.56130213\n",
      " -0.76218355  0.93669212  0.93564975 -0.08939899]\n",
      "-2.9418857097625732\n",
      "0.977552592754364\n",
      "[-0.01611805  1.01507843  0.98575181  1.76120901  0.82554829  0.46683502\n",
      " -0.00744006  1.16104364  1.16643512 -0.38859686]\n",
      "-4.135344505310059\n",
      "2.5401198863983154\n",
      "[ 0.32602665  0.19839273  0.20481455  0.15634261  0.12310709 -0.04645513\n",
      " -0.44732541  0.44058132  0.44106436  0.29140231]\n",
      "-3.106045961380005\n",
      "0.808659553527832\n",
      "[ 0.46305868  1.10627103  1.37598395  2.11274195  0.34400618  0.68545663\n",
      "  1.26805615  1.45703733  1.4523679  -0.27640003]\n",
      "-3.957559823989868\n",
      "3.1810543537139893\n",
      "[0.20581445 0.57972723 0.58888334 0.65538132 0.24105221 0.56531078\n",
      " 0.170351   0.93586588 0.93555534 0.54967535]\n",
      "-2.6166515350341797\n",
      "0.9358658790588379\n",
      "[0.77312845 1.26064122 1.25226676 1.91743433 0.48077983 0.66892183\n",
      " 0.63924706 1.62464654 1.62473249 0.41933694]\n",
      "-3.815316677093506\n",
      "1.9174343347549438\n",
      "[1.39012039 1.87458003 1.90994799 1.96015906 1.34775984 1.23374295\n",
      " 0.39597648 1.89264166 1.89453042 1.51887655]\n",
      "-3.2034130096435547\n",
      "2.0394632816314697\n"
     ]
    }
   ],
   "source": [
    "# print(bert_score_list[0][:10])\n",
    "# print(bert_score_list[0].min())\n",
    "# print(bert_score_list[0].max())\n",
    "print(score_1[0][:10])\n",
    "print(score_1[0].min())\n",
    "print(score_1[0].max())\n",
    "print(score_2[0][:10])\n",
    "print(score_2[0].min())\n",
    "print(score_2[0].max())\n",
    "print(score_3[0][:10])\n",
    "print(score_3[0].min())\n",
    "print(score_3[0].max())\n",
    "print(score_4[0][:10])\n",
    "print(score_4[0].min())\n",
    "print(score_4[0].max())\n",
    "print(score_5[0][:10])\n",
    "print(score_5[0].min())\n",
    "print(score_5[0].max())\n",
    "print(score_6[0][:10])\n",
    "print(score_6[0].min())\n",
    "print(score_6[0].max())\n",
    "print(score_7[0][:10])\n",
    "print(score_7[0].min())\n",
    "print(score_7[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2/=2.5\n",
    "score_4/=3\n",
    "score_6/=1.9\n",
    "score_7/=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_score_list = np.zeros([80,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_score_list += score_1  + score_2 +score_3+score_4+score_5+score_6+score_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_score_list /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28289892  0.49145598  0.50986849  0.73216692  0.31149946  0.3520447\n",
      " -0.01214493  0.72351912  0.72362196  0.2120355   0.17472876  0.17615887\n",
      "  0.22143613  0.00382071  0.63152177  0.00795055 -0.13272919  0.06878403\n",
      "  0.06633442 -0.58717008  0.29697921  0.40608862  0.05354757  0.05366373\n",
      " -0.07297313  0.3504502   0.10580328 -1.31759286  0.13083852 -0.09528823\n",
      " -0.83777497 -0.83777497  0.37607381  0.40727685 -1.06522253 -1.04820595\n",
      "  0.22160451 -1.48810775  0.62143796 -0.8103536  -1.5762138  -1.29638347\n",
      " -1.08198174  0.49371712 -1.51573341 -0.44404663 -0.96617319 -1.71967043\n",
      " -1.85620761  0.07164099 -1.54416393 -0.50167333 -0.44353503 -1.25204521\n",
      "  0.25100582 -0.26132128 -1.36419886 -1.2461449   0.14055153 -0.62048992\n",
      " -1.69575786 -0.81687692  0.04527056 -1.62359103 -1.46975984  0.21796911\n",
      " -0.14963541 -0.58200839 -0.53468115 -0.14709225 -1.00771958  0.39037599\n",
      " -0.34777678 -0.40963636 -1.04727652 -1.65254352 -0.78371078 -0.62158299\n",
      " -0.65377924  0.20909841 -1.35314226 -1.35574977 -0.87140539 -1.45403897\n",
      " -0.37385829  0.15499264 -0.9646644  -1.12524263 -0.01161339 -0.17710364\n",
      " -1.73038319  0.03936473 -0.54708068 -1.19419751 -0.04955042 -0.1125149\n",
      " -0.26186951  0.01915452 -0.11020673 -1.69947207 -1.66874583 -1.67035081\n",
      "  0.20169883 -1.69717033  0.07890268 -1.16887556 -0.21399177 -1.62866565\n",
      " -0.21669763 -1.66352015 -0.23330963 -1.21289275 -1.78525051 -0.34261627\n",
      " -0.57431349 -1.628328   -0.91875878 -1.04461148 -1.2879008   0.14943781\n",
      " -1.28916183  0.05120039 -0.45414235 -1.04360006 -0.82106124 -1.28002181\n",
      "  0.07585136 -0.80234178  0.0623885  -1.01937717 -0.27839959 -1.27811909\n",
      " -1.16172637 -0.85135145 -0.37676289 -0.28289763 -0.59599538 -0.28521989\n",
      "  0.53655624 -1.07275498 -0.3356814  -0.36271813 -1.0434834   0.22669598\n",
      " -1.58086572  0.27391537 -0.5108233  -0.94546277 -0.29353475 -1.705835\n",
      " -0.36145142 -0.89362892  0.73961337 -0.82262353 -0.12423644 -1.54666402\n",
      " -1.80451471 -0.83538821 -1.02534469 -1.39006388  0.49944916 -1.2806817\n",
      " -0.79457679  0.34077818 -0.94041017 -0.50664945 -0.37605519 -1.15972082\n",
      " -1.05195171  0.31057177 -0.60742488 -0.95227615 -0.60742488 -1.55731579\n",
      "  0.56921064 -1.88321337 -0.17001528 -0.17228601  0.322836   -0.86936275\n",
      " -1.75938579 -0.76937276 -1.73359976  0.07881146 -0.83733308 -1.18620252\n",
      "  0.45201274 -0.72876132 -1.03113618 -1.28769844 -0.34677473  0.15266835\n",
      " -0.2640187  -0.38928064 -0.04699328 -1.15636322  0.11506764 -1.15690432\n",
      " -1.92259766 -1.76422003 -0.99638518 -1.64850798 -1.64114316 -1.70516278\n",
      " -0.98060151 -0.13125998 -0.04327205  0.14118637  0.33920965 -0.46247573\n",
      " -1.17476144 -1.19128627 -1.05263448 -1.38593803 -1.00016076 -0.16065685\n",
      " -1.35649692 -0.11821845 -1.41589448 -1.25309538 -1.57922847 -0.30930437\n",
      " -1.01758164 -0.56796797 -1.89599492 -0.2766641  -1.26662831 -1.24857912\n",
      "  0.16856887 -1.16899976 -2.11451572 -1.02380872 -0.3713267  -1.94182884\n",
      " -0.18547825 -1.25058015 -2.01998167 -1.13163083 -1.32638479 -0.70357455\n",
      " -1.58208096 -2.02163649 -0.42840641 -0.62926504 -1.09912335 -0.54019028\n",
      " -1.06598823 -0.64655321 -0.38242581 -2.08238132 -0.81751012 -0.91056766\n",
      " -0.07141203 -2.02576882 -0.49603196 -0.95966989 -0.30744573 -0.81059352\n",
      " -1.66173144 -1.56165955  0.22581488 -0.6368715  -0.55730888 -0.30990073\n",
      " -0.69261386 -0.28310375 -1.81011584 -0.06112809 -0.01985174 -1.51407238\n",
      " -1.29240649 -0.26662434 -1.17568848 -0.91521922 -1.0083756  -1.16508517\n",
      "  0.197877   -0.30209471 -0.62289516 -1.52003387 -0.56099961 -0.08508572\n",
      " -0.4885264  -0.73333186 -0.03947374 -0.96802818 -2.04003297 -1.90975185\n",
      " -0.84796582 -0.21239965  0.24058952 -1.8721754  -1.93104503 -0.20420496\n",
      " -0.64906987 -0.91289542 -2.00231386 -1.87637594 -2.0719427  -0.38027245\n",
      " -0.04162682 -0.89657438 -0.27895655 -2.01847225 -1.37040516  0.10081695\n",
      " -0.74245735 -1.89729893 -0.51441116 -0.43259221 -0.84690145 -1.27568935\n",
      " -1.56247633 -0.26791502 -0.02580014  0.06151236 -1.80699416 -0.24939897\n",
      " -0.01015738 -2.03716037 -1.61991915 -0.23666803 -0.37182506 -0.90008731\n",
      " -0.66329545 -1.72240808 -1.83345422 -1.69966959 -1.90470351 -0.93671856\n",
      " -0.81437149 -1.02721717 -1.93055486 -0.07236445 -0.99896118 -0.4892371\n",
      " -1.13142867 -1.81442714 -0.97946444 -0.38453906 -0.13260423 -1.97672585\n",
      " -2.1063093  -0.36328319 -0.97618949 -0.38810409 -1.94819635 -0.24885143\n",
      " -0.24885143 -0.97618949 -0.12309432 -0.90466412 -0.13254255 -0.7368635\n",
      " -0.1788711  -0.37162883 -0.34685919 -1.31765431 -0.39329086 -1.71582509\n",
      " -0.74830059 -0.65658545 -0.85215664 -0.53655481  0.2276887   0.25306382\n",
      " -0.76543612 -0.51449933 -0.51449933 -1.23808064 -2.00154527 -1.19274621\n",
      " -0.12636989 -0.5526052  -1.90224281 -1.04602389 -1.34489463 -0.68740871\n",
      " -0.47794438 -1.69071804 -0.89715823 -0.12152797 -0.70720086  0.11262\n",
      " -0.59422914 -0.6409382  -0.60705747 -0.71689563 -0.27633615 -1.12977784\n",
      " -1.08464569 -0.72078964 -1.76512623 -1.42639671 -2.08310871 -0.59372734\n",
      " -0.72491983 -2.02650823 -1.4962283  -0.82735174 -0.36332031 -1.79136777\n",
      " -0.20150251 -0.23209073 -0.70374439 -1.49904592 -0.09328738 -1.68414748\n",
      "  0.12599698 -0.13243141 -1.60659691 -1.7331109  -0.34701327 -0.34701327\n",
      " -2.04605731 -0.48870535 -0.48870535 -0.05509622 -1.59220284 -0.28556483\n",
      " -0.32688343 -0.36490913 -0.54142437 -1.26258948 -0.71135241 -1.91826906\n",
      " -0.38642575 -0.46346909 -0.51369164 -1.87926803 -1.37149375 -1.55333549\n",
      " -0.31101883 -0.84641507 -0.44590737 -0.72606299 -1.87387499 -1.17068987\n",
      " -1.50731325 -0.70016451 -0.54511764 -0.70994065 -1.03037013 -1.39145968\n",
      " -1.97593727 -1.57180011 -0.83942999 -0.38989044  0.00921051 -0.62280784\n",
      " -1.91524439 -0.41052387 -0.41052387 -0.59363244 -0.8391803  -0.9656249\n",
      " -1.42091391 -1.83137166 -1.87758966 -0.84348145 -1.07634334 -0.47520878\n",
      " -0.47520878 -1.72477451 -0.7628313  -1.2320821  -0.22465657 -1.26728733\n",
      " -0.77125855 -1.74012994 -1.8738761  -1.88949881 -0.81979432 -0.44090284\n",
      " -1.89736312 -0.72258569  0.15744952  0.16194045 -1.78738581 -0.35587534\n",
      " -1.93633542 -0.42904313 -1.69766287 -0.93946055 -0.53748579 -1.09549808\n",
      " -1.46129403 -0.84937166 -0.84937166 -1.10888313 -0.30120719 -1.14471037\n",
      " -1.35704745 -1.24781107 -1.5941506  -1.66637882 -1.82411875 -1.10277669\n",
      " -1.45362457 -0.95712091 -1.5385826  -1.57532135 -1.75851717 -0.18814407\n",
      " -1.0530815  -1.88635983  0.0025307  -0.61805636 -0.41706144 -1.29557638\n",
      " -0.33178899 -0.3987137  -1.96119132 -1.36139937 -1.06284363 -0.13159803\n",
      " -1.30062338 -0.13254902 -0.95534777 -1.4088737  -1.50080192 -1.64211613\n",
      "  0.06898985  0.06898985  0.07648414 -1.25549177 -0.12178432 -0.7260537\n",
      " -1.39462192 -0.57123204 -0.03850155 -0.13558252 -0.50140643 -0.98185921\n",
      " -0.72597654 -1.28430701 -1.42920591  0.13466097 -0.12557278 -1.2718767\n",
      " -1.09934865 -0.43275116 -1.47889719 -1.59639889 -1.52148643 -1.42076477\n",
      " -0.01762302  0.16652524 -0.8620638  -0.23392276 -1.2410213  -0.01430259\n",
      " -1.49520243 -2.06019396 -0.32545431 -0.45187355 -1.48512574 -1.95196807\n",
      " -0.69606119 -0.4561215  -1.1148288  -1.03957106 -0.61270556 -1.39921741\n",
      " -0.12838355 -0.3023681  -0.97566956 -0.57590286 -2.04959999 -0.28085236\n",
      " -0.87214677 -0.71250471 -0.26767577 -1.91225337 -0.86558993 -0.75922741\n",
      " -1.62324409 -1.83598387 -0.78218841 -0.97894788 -1.12240895 -1.05018882\n",
      " -0.70326082 -0.67955514 -1.25954232 -0.59530092 -1.17196263 -1.8815317\n",
      " -1.32606518 -0.30800552 -1.70884176 -1.08132587 -1.17079073 -1.09692905\n",
      " -1.78806165 -1.3992722  -0.8831335  -1.0160244  -0.7304     -0.65933655\n",
      " -0.90992909 -1.61833364 -0.8776916  -1.78466134 -1.4719846  -1.55288607\n",
      " -0.8397833  -0.87807994 -1.81455199 -0.87380036 -0.60781915 -0.03048263\n",
      " -0.39680444  0.15759855 -1.91025754 -2.12820896 -0.84000644 -0.0226556\n",
      " -1.37152048 -0.8510453  -0.69940638  0.096293   -1.98260459 -0.05351956\n",
      " -1.52294725 -1.86927896 -0.98183442  0.04025607 -0.19207276 -1.81550022\n",
      " -1.01471835 -0.94450513 -1.08750432 -1.24277065 -1.2111792  -1.08754894\n",
      " -1.72086019 -2.04564359 -0.63612421 -0.25793916 -0.30050222 -2.11431496\n",
      " -0.7161243  -1.00023836 -0.90854127  0.01837268  0.01624998 -0.3392002\n",
      " -0.3392002  -0.99294546 -0.8133678  -1.43626969 -0.19449224 -0.11465987\n",
      "  0.30667078 -1.68132055 -2.00186529 -1.76546642 -0.31215144 -1.41510538\n",
      " -1.19787684 -1.96847472 -0.70053465 -0.50377364 -1.71107262 -1.26552871\n",
      " -0.83788312 -0.53129282 -0.6581633  -1.74536227 -2.01627463 -0.78981287\n",
      " -1.52836006 -2.07046773 -0.47819391 -1.18610237 -1.25411997 -1.03049216\n",
      " -1.09909343 -1.30225337 -1.30484159 -1.70653297 -0.40762602 -1.63116969\n",
      " -0.88329217 -0.72941741 -0.90435858 -2.00037803 -0.45712769 -1.62053068\n",
      " -0.12860856 -0.11381799 -1.63967863 -1.65983387  0.16614604 -0.81403673\n",
      " -1.59039311 -0.77609339 -1.10461621 -1.94261832  0.19650402 -1.34329305\n",
      " -0.61602162  0.18291318 -0.03487904 -0.12556155 -2.13244763 -1.19700781\n",
      " -0.89223049 -1.59626627 -0.96104178 -0.37328545 -1.72528    -0.60890477\n",
      "  0.03739661 -0.79372247 -1.05006378 -1.54219283 -2.11052706 -1.5715555\n",
      " -1.25086235 -0.4432972  -1.02227913 -0.72079133 -0.84135566 -1.35533893\n",
      " -1.42292171 -1.5786425  -0.75557549 -1.28940004 -0.38492833 -0.09086008\n",
      " -0.17840719 -2.04939432 -1.20744205 -1.32040083 -1.34813963 -2.02857185\n",
      " -0.7537678  -0.69402482 -0.2126553  -1.05482983 -0.33741484 -1.41668191\n",
      " -0.95570609 -1.31880863 -1.24566199 -0.53075003 -1.46398462 -0.41743063\n",
      " -2.04459781 -1.07121162 -0.76969565 -1.78493762 -1.83897345 -1.71532459\n",
      " -0.66066658 -2.11439217 -1.11087584 -0.80224577 -1.78430786 -1.1570435\n",
      "  0.11855409 -1.95833203 -0.66897231 -1.87864466 -0.78801096 -1.18327318\n",
      " -1.25857732 -0.4942986  -1.97895674 -0.05441646 -0.94800203 -1.37709588\n",
      " -1.70609531 -1.25971312 -1.91965546 -0.11174344 -0.9218234  -1.52002928\n",
      " -1.43000122 -1.0587324  -1.17881701 -0.88329679 -1.83891423 -0.75988551\n",
      " -0.91067492 -1.22200618 -0.77713564 -1.91309429 -0.86146953 -1.36848988\n",
      " -1.37803477 -1.52100705 -1.86885465 -0.12089191 -0.14838036 -1.83588844\n",
      " -0.18757953 -1.09408495 -0.87900618 -1.51701152 -1.70791385 -1.22107833\n",
      " -2.13929855 -1.42768169 -1.5211677  -0.63272251 -0.8907033  -2.11821807\n",
      " -1.90043139 -0.14506762 -0.37629658 -1.66924031 -0.17267178 -1.28956643\n",
      " -0.63056338 -0.51831377 -1.81592699 -1.66819416 -2.11915502 -1.36853503\n",
      " -1.83649152 -1.9850783  -1.05886731 -0.398191   -1.26616083 -1.12356368\n",
      " -1.09703213 -1.73089084 -1.82894616 -1.7380262  -1.48198577 -2.13015589\n",
      " -0.10170718 -0.97452209 -1.68541746 -1.50390021 -0.11820747 -0.89004665\n",
      " -0.37553894  0.23156115 -0.56842356 -1.13114919 -1.76971853 -1.43480238\n",
      " -0.66198794 -1.16513026  0.06090635  0.09099975 -1.49058392 -0.88577868\n",
      " -0.45157528 -0.50197792 -1.79374922 -0.11593468 -1.05380326 -1.83304848\n",
      " -1.24696611 -1.59414845 -1.6336394  -1.02747614 -1.38211729 -1.1337589\n",
      " -1.07068405 -0.31920447 -0.12688704 -0.77998676 -1.6690647  -0.11094904\n",
      " -0.80398946 -1.88195703 -1.61743519 -1.50066925 -1.75936492 -0.10806614\n",
      " -1.20244228 -1.50600355  0.43731743 -1.56962136 -1.89705699 -1.63100284\n",
      " -1.38525672 -1.74446109 -0.9574808  -0.05209632 -1.65791773 -0.10982548\n",
      " -1.38505397 -1.46895964 -1.94948491 -1.30922935 -0.98694657 -1.05399924\n",
      " -1.07849129 -0.48716522 -1.86677266 -0.33377907 -0.3888187  -0.87479864\n",
      " -0.53860659 -0.07383246  0.28011212 -1.5257085  -0.56444379 -1.4152206\n",
      " -0.85006707 -0.66769711 -1.08042237 -1.52728862 -1.66290941 -0.86264637\n",
      " -0.86264637 -0.87789533 -1.00214843 -0.59564841 -1.04228069 -0.7758106\n",
      " -1.55416483 -0.21981301 -0.78278423 -1.8736651  -0.14747916 -0.32389709\n",
      " -0.28551974 -1.49488791 -1.53073911 -0.62713496 -0.72869824  0.05517099\n",
      " -1.17414277 -0.58660063 -1.84133996 -0.36526506 -1.02323089 -1.40691693\n",
      " -1.84928984 -0.0883886  -0.79187567 -1.19724796 -1.69192477 -1.52061895\n",
      " -1.23518307 -0.43342375 -0.912256   -1.29901068 -1.79414983 -1.58723856\n",
      " -1.83705478 -1.10822335 -1.87666439 -0.2521878  -0.78378567 -0.58823595\n",
      " -1.2611832  -1.53838787 -1.88181277 -1.55894043 -1.29482524 -1.64234576\n",
      " -1.68188835 -0.69309987 -0.91163519 -1.58319816 -0.66572548 -1.33163957\n",
      " -0.75482546 -2.08653153 -0.94623235 -0.71410448 -0.28044497 -1.22252777\n",
      " -0.20735764 -0.72660398 -0.80026982 -0.80681246 -1.4791049  -1.72378069\n",
      " -0.33991011 -1.78438074 -1.86472869 -1.88155437 -1.22758501 -1.25860344\n",
      " -1.67294992 -1.7309551  -1.26173678 -1.67645677 -1.96573387  0.05274061\n",
      "  0.02080934 -1.30599084 -0.06248178 -1.50928607 -1.6779181  -0.64142839\n",
      " -1.90576893 -1.83959958 -1.29359784 -1.8836375 ]\n",
      "[27.03814755 26.16712283 26.16712283 25.73222681 25.07271772 24.94901827\n",
      " 24.85960216 24.64764842 24.64764842 24.54355649 24.48749023 24.32225321\n",
      " 24.32225321 24.29220873 23.69311614 23.23352808 22.91041934 22.90816876\n",
      " 22.90816876 22.73366513 22.6561122  22.63024405 22.60739924 22.56145767\n",
      " 22.5005371  22.26959904 22.11763222 22.10561121 22.10539515 21.93515449\n",
      " 21.78812788 21.78812788 21.75714676 21.75714676 21.66948718 21.55412678\n",
      " 21.31796191 21.30755072 21.24164171 21.21505172 21.20509125 21.11881397\n",
      " 21.0446029  21.02132921 20.99726159 20.76887966 20.71095565 20.60934834\n",
      " 20.4695788  19.95837456 19.92981587 19.83070091 19.81781475 19.76964863\n",
      " 19.71108528 19.64141039 19.63107136 19.60889097 19.56396453 19.46306128\n",
      " 19.19438341 19.08358124 19.03216462 18.99249638 18.90713031 18.72538291\n",
      " 18.56946424 18.56124417 18.5099993  18.44308182 18.24799471 18.23567023\n",
      " 18.19883369 18.16692831 18.1113381  18.10656816 18.07192347 17.96259925\n",
      " 17.95573573 17.95573573 17.9307469  17.91511703 17.8719607  17.78857432\n",
      " 17.7616695  17.76106586 17.74314264 17.71803127 17.70782578 17.70782578\n",
      " 17.62012977 17.46835071 17.44770066 17.43687685 17.38978535 17.38329325\n",
      " 17.34896855 17.31521759 17.29915227 17.28686274 17.27578333 17.23129755\n",
      " 17.17903721 17.15914421 17.12810302 17.01623288 17.01291518 17.00611438\n",
      " 16.99481477 16.94682538 16.93409454 16.66490038 16.61236426 16.60169654\n",
      " 16.59182738 16.5547537  16.46562885 16.45051449 16.36629409 16.27947543\n",
      " 16.2720115  16.24226377 16.24066781 16.21848138 16.20210462 16.10477303\n",
      " 16.07492763 16.0197595  16.00425131 16.00050811 15.87841472 15.87280699\n",
      " 15.85817411 15.81281438 15.79929164 15.77765275 15.7637744  15.73523833\n",
      " 15.73270848 15.64328113 15.63725279 15.57061895 15.54052173 15.49944554\n",
      " 15.49940043 15.48868516 15.46238519 15.42008407 15.41123296 15.4090046\n",
      " 15.38429181 15.38069353 15.37029974 15.35890016 15.34825858 15.33573072\n",
      " 15.32931645 15.28611302 15.28077906 15.27603218 15.20716916 15.16811713\n",
      " 15.15286175 15.12583444 15.07860165 15.05458191 15.03890933 14.97430258\n",
      " 14.92707279 14.88908711 14.86975905 14.85706827 14.85671142 14.85379866\n",
      " 14.83907466 14.80922069 14.75958791 14.75958791 14.70039241 14.69085402\n",
      " 14.64040827 14.63067917 14.62565166 14.61701758 14.60303661 14.56663119\n",
      " 14.55590009 14.54578881 14.54578881 14.54419068 14.5312033  14.500894\n",
      " 14.48361801 14.48322856 14.48197231 14.43873689 14.41432837 14.40154562\n",
      " 14.38117265 14.38000099 14.3227614  14.31813872 14.31813872 14.30915881\n",
      " 14.26142077 14.23912253 14.23446631 14.22731718 14.21615375 14.20037717\n",
      " 14.18056899 14.18056899 14.17193667 14.160816   14.15546573 14.14337344\n",
      " 14.10188584 14.06596403 14.0628708  14.01803639 13.99807129 13.96463486\n",
      " 13.96196914 13.95656667 13.95111576 13.93977906 13.93459577 13.92800192\n",
      " 13.91635734 13.90782182 13.89556292 13.89037317 13.88527661 13.87226012\n",
      " 13.82527193 13.80686131 13.78334241 13.77699935 13.75432509 13.73998395\n",
      " 13.73975892 13.72192422 13.6945258  13.69285014 13.68844473 13.67706048\n",
      " 13.64753275 13.63916032 13.6292359  13.61680557 13.6132426  13.60076466\n",
      " 13.54974995 13.54064864 13.5362197  13.52658106 13.48132013 13.46674555\n",
      " 13.44892991 13.42792787 13.37000146 13.36055416 13.33407659 13.30095805\n",
      " 13.29739821 13.28967549 13.26926365 13.2243266  13.20530894 13.18724188\n",
      " 13.1674579  13.15315095 13.15000444 13.08539611 13.0517962  13.05100057\n",
      " 13.04739556 13.03506088 13.02793765 13.00171864 12.99553635 12.97413594\n",
      " 12.96891553 12.96854654 12.92716663 12.92710433 12.88339868 12.86119734\n",
      " 12.86109548 12.85329639 12.85103236 12.84909555 12.81855612 12.81671202\n",
      " 12.81243425 12.78908379 12.77503226 12.75656029 12.73531417 12.71929545\n",
      " 12.68060929 12.67744373 12.67640002 12.66165406 12.64590367 12.63892426\n",
      " 12.62214284 12.6136772  12.60300393 12.52489284 12.51865296 12.505894\n",
      " 12.49112306 12.48799062 12.48515906 12.48035898 12.45046794 12.42720162\n",
      " 12.42427051 12.42427051 12.40836998 12.40312346 12.39912268 12.38307458\n",
      " 12.36895063 12.36507707 12.35392758 12.35391961 12.33952373 12.33610868\n",
      " 12.33069868 12.32763975 12.31493731 12.30780598 12.30170683 12.28216632\n",
      " 12.27930917 12.26934541 12.26019198 12.25625033 12.22679937 12.22554167\n",
      " 12.21423488 12.21317622 12.20620313 12.1963578  12.17776875 12.17729362\n",
      " 12.17729362 12.15873153 12.15869884 12.15867589 12.15236294 12.14054331\n",
      " 12.13895713 12.13793903 12.12053095 12.11766739 12.11455181 12.11371596\n",
      " 12.1095438  12.09873011 12.08606045 12.07785022 12.07295078 12.05039937\n",
      " 12.03595332 12.02495202 12.02495202 12.01633002 12.01633002 12.01633002\n",
      " 12.01388746 12.00370015 11.98103432 11.97918946 11.94137254 11.93242466\n",
      " 11.92019988 11.91957785 11.8963844  11.88109336 11.8727311  11.87011581\n",
      " 11.8660017  11.83193998 11.82478872 11.81879315 11.79611747 11.79228199\n",
      " 11.79072438 11.78104932 11.77765396 11.77353903 11.77353903 11.76897241\n",
      " 11.75746938 11.7565717  11.74836183 11.73964788 11.73962963 11.73748976\n",
      " 11.72218317 11.7047885  11.69989304 11.68744537 11.68644802 11.68644326\n",
      " 11.66805754 11.61858377 11.60949213 11.59635107 11.59085835 11.59085835\n",
      " 11.58954999 11.58445637 11.58445637 11.56746775 11.55311417 11.5475919\n",
      " 11.52424353 11.51679952 11.51436867 11.50079821 11.49793481 11.49159562\n",
      " 11.48834441 11.47472559 11.47286076 11.46243751 11.45358838 11.45105701\n",
      " 11.44165531 11.42417142 11.4168604  11.4118017  11.41122299 11.40470943\n",
      " 11.4002824  11.39933346 11.38541318 11.38382779 11.37384899 11.35996793\n",
      " 11.3503188  11.34850414 11.34452131 11.33856777 11.33196681 11.30057174\n",
      " 11.2942052  11.2860993  11.2860993  11.27234819 11.27179825 11.24274783\n",
      " 11.24213432 11.19778988 11.19730638 11.17749439 11.16040042 11.15459611\n",
      " 11.15459611 11.13579591 11.12645907 11.12379262 11.11410449 11.11163407\n",
      " 11.10572983 11.08670011 11.08558449 11.08000243 11.07395849 11.04924868\n",
      " 11.04057571 11.02663317 11.01871796 11.01871796 11.01809388 11.01809388\n",
      " 11.01173927 11.00499414 11.00339155 10.99783022 10.97887091 10.97592874\n",
      " 10.9687227  10.95595776 10.95595776 10.95595776 10.95164577 10.94520053\n",
      " 10.92866555 10.92651743 10.92068828 10.92064994 10.91204171 10.909539\n",
      " 10.90771588 10.8991344  10.8917245  10.88171492 10.87018651 10.86630751\n",
      " 10.85800009 10.849688   10.83470951 10.8321489  10.83107678 10.82759883\n",
      " 10.81354444 10.80515543 10.80054293 10.79311218 10.7816654  10.77946093\n",
      " 10.77265188 10.77250722 10.76812855 10.76671293 10.76252161 10.76144946\n",
      " 10.73546495 10.73546495 10.73546495 10.72663481 10.71854059 10.71336319\n",
      " 10.71182361 10.70850847 10.68981154 10.6895944  10.68653522 10.67925059\n",
      " 10.67917282 10.67484167 10.67369872 10.67173407 10.66465522 10.65820385\n",
      " 10.6573348  10.6513636  10.64617405 10.62997416 10.627253   10.6271883\n",
      " 10.6263847  10.62464413 10.62362187 10.61915113 10.61416496 10.61038291\n",
      " 10.5953802  10.58835566 10.58587386 10.58474695 10.58227918 10.58227918\n",
      " 10.57494452 10.56778116 10.56125796 10.54945943 10.54868492 10.54676947\n",
      " 10.54529882 10.53177233 10.52034924 10.50181926 10.50055168 10.49289989\n",
      " 10.49234266 10.487053   10.487053   10.48163906 10.47475771 10.46212233\n",
      " 10.45995539 10.45410907 10.45085201 10.43669153 10.43340357 10.43340357\n",
      " 10.43319765 10.43319765 10.43035338 10.42743149 10.42093325 10.38783322\n",
      " 10.37592618 10.37398788 10.37317231 10.37097026 10.35464485 10.34667667\n",
      " 10.34523575 10.34481099 10.34289351 10.33871491 10.31226272 10.31148626\n",
      " 10.29554164 10.29229211 10.29054536 10.28285168 10.26611139 10.23073332\n",
      " 10.22703762 10.21874501 10.21417891 10.21012569 10.20671662 10.20552361\n",
      " 10.20552361 10.20234132 10.19862112 10.18351063 10.18140375 10.18022998\n",
      " 10.17218173 10.17094625 10.14487927 10.1408425  10.13981205 10.13507498\n",
      " 10.13448985 10.13295954 10.12930311 10.12389726 10.12389726 10.12382341\n",
      " 10.11645821 10.11317635 10.09829385 10.08095122 10.0778147  10.07269587\n",
      " 10.07092137 10.07025064 10.06709245 10.06637152 10.05843235 10.04788306\n",
      " 10.04384605 10.04343683 10.04079764 10.04070224 10.04070224 10.03670738\n",
      " 10.03670738 10.02395255 10.02110397 10.01632854 10.0112301  10.01087065\n",
      " 10.01086048 10.00691829  9.99213336  9.9902713   9.98975339  9.98232356\n",
      "  9.98074333  9.95222756  9.94914498  9.94809922  9.94291329  9.93905012\n",
      "  9.93606614  9.93556846  9.92942642  9.92399429  9.92306924  9.92306924\n",
      "  9.91634421  9.91592533  9.91351705  9.91060142  9.90972411  9.90972411\n",
      "  9.8925587   9.89137451  9.88991674  9.88434486  9.88213001  9.88048172\n",
      "  9.88048172  9.87675576  9.8656129   9.8656129   9.86131297  9.86070692\n",
      "  9.85946846  9.85946846  9.85707984  9.85707984  9.85187514  9.85175646\n",
      "  9.85170234  9.84872937  9.84674452  9.83748215  9.83662384  9.83231901\n",
      "  9.81197557  9.81148923  9.80241204  9.79455749  9.78487172  9.77108015\n",
      "  9.77011841  9.76711047  9.75132507  9.75063078  9.74845474  9.74549748\n",
      "  9.73870827  9.73609576  9.73127433  9.72751699  9.72686859  9.71291168\n",
      "  9.70879634  9.70702626  9.70334186  9.70122615  9.69356876  9.69276468\n",
      "  9.68034003  9.68034003  9.67834184  9.67768055  9.67195247  9.67039183\n",
      "  9.66766321  9.66687318  9.66231586  9.6568863   9.65613687  9.64990002\n",
      "  9.64780747  9.64165993  9.64144531  9.63944396  9.63675186  9.63456316\n",
      "  9.6331883   9.62694075  9.62029881  9.61928219  9.61493278  9.61316551\n",
      "  9.60993519  9.58962498  9.58404567  9.58016313  9.57107536  9.56485339\n",
      "  9.56165006  9.55366401  9.55259737  9.55231412  9.54947384  9.54646061\n",
      "  9.54001238  9.53975327  9.53680885  9.5302358   9.52493749  9.52443647\n",
      "  9.52195876  9.52195876  9.49673166  9.49056419  9.48676819  9.48172311\n",
      "  9.47974209  9.47939341  9.47334369  9.47066974  9.47033905  9.44355658\n",
      "  9.4422713   9.44016348  9.4363503   9.43163156  9.43121091  9.43121091\n",
      "  9.42258343  9.4168718   9.41418743  9.41326851  9.40843651  9.40730288\n",
      "  9.40266208  9.39955398  9.38873615  9.38777827  9.38322371  9.37794319\n",
      "  9.37520544  9.37200804  9.3690995   9.36799964  9.36712462  9.35987148\n",
      "  9.35643155  9.35205585  9.34970247  9.34547373  9.34368266  9.3405739\n",
      "  9.34053752  9.33860673  9.33663879  9.33630056  9.33583228  9.33501838\n",
      "  9.32977098  9.32783815  9.32783815  9.32547259  9.31944695  9.3128707\n",
      "  9.31039902  9.30337734  9.29536385  9.2907571   9.28408266  9.28408266\n",
      "  9.28374823  9.27053572  9.26083798  9.25480733  9.25033691  9.24131842\n",
      "  9.23481787  9.23288501  9.22731492  9.22731492  9.22671447  9.22662783\n",
      "  9.22662783  9.22662783  9.22662783  9.22351012  9.21492136  9.21459189\n",
      "  9.20887534  9.20856971  9.20770038  9.20770038  9.20316587  9.19918655\n",
      "  9.19779173  9.19253465  9.18925078  9.18293295  9.17683932  9.17631177\n",
      "  9.15524501  9.15475719  9.14712956  9.14618096  9.14281851  9.13050542\n",
      "  9.11812052  9.11684689  9.11478236  9.11149144  9.10841306  9.10231883\n",
      "  9.10159283  9.09927661  9.09563086  9.09044517  9.08798297  9.07796451\n",
      "  9.0713216   9.0713216   9.0675288   9.05524599  9.04900407  9.04743345\n",
      "  9.03926202  9.03868155  9.03818148  9.03652365  9.02993424  9.02103722\n",
      "  9.01864571  9.01348485  9.00850478  9.00526642  9.00526642  8.9979095\n",
      "  8.99729359  8.99514532  8.993515    8.9902273   8.98758716  8.98588865\n",
      "  8.98339138  8.98150963  8.97708788  8.97622297  8.9746761   8.96867259\n",
      "  8.96447639  8.96418249  8.96418249  8.96181968  8.96169358  8.96125665\n",
      "  8.96125665  8.9599685   8.95429983  8.95429983  8.95429983  8.95305765\n",
      "  8.95186332  8.94174922  8.94060335  8.94019819  8.93515301  8.93401587\n",
      "  8.92699883  8.92617675  8.92390832  8.91572497  8.90694437  8.90694437\n",
      "  8.87658896  8.87371167  8.86763597  8.8479787   8.84386135  8.83611625\n",
      "  8.82956692  8.82027235  8.81281146  8.81235945  8.80466941  8.80451901\n",
      "  8.79945002  8.79831574  8.79714291  8.78986803  8.78932952  8.78497882\n",
      "  8.78466654  8.78152998  8.77374428  8.77284617  8.76164476  8.75941002\n",
      "  8.75046979  8.75046979  8.74615747  8.74580512  8.73375354  8.73215673\n",
      "  8.72731846  8.71738179  8.70767772  8.70428931  8.69708402  8.69663991\n",
      "  8.69663991  8.69011768  8.68912193  8.68800676  8.68440258  8.68333557\n",
      "  8.68114292  8.6784479   8.67619919  8.66953508  8.66891004  8.66617628\n",
      "  8.65938464  8.65859369  8.65859369  8.65408615  8.65102436  8.64741222\n",
      "  8.64346826  8.63926073  8.63744889  8.63592534  8.63357914  8.63096733\n",
      "  8.63096733  8.63063177  8.62996679  8.62781764  8.62445172  8.62099077\n",
      "  8.61552297  8.61337536  8.61058938  8.60588485]\n"
     ]
    }
   ],
   "source": [
    "print(mix_score_list[0])\n",
    "print(result_score_list[0])\n",
    "all_ans = []\n",
    "for i in range(80):\n",
    "    ans = []\n",
    "    score = result_score_list[i] + (8 * mix_score_list[i])\n",
    "    idx = sorted(range(1000),reverse = True,key = lambda k : score[k])\n",
    "    for j in range(1000):\n",
    "        ans.append(rerank_idx_list[i][idx[j]])\n",
    "    all_ans.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw6_new_best_91.txt','w') as f:\n",
    "    f.write('query_id,ranked_doc_ids\\n')\n",
    "    for i in range(80): \n",
    "        f.write(str(test_query_list[i]))\n",
    "        f.write(',')\n",
    "        for j in range(len(all_ans[0])):\n",
    "            f.write(all_ans[i][j])\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
