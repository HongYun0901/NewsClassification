{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5360, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "df_train = pd.read_csv('./tbrain/only_have_names_data.csv')\n",
    "df_train = df_train.fillna('[\\'\\]')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "#     cc = OpenCC('t2s')\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "#     content = cc.convert(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(name, content):\n",
    "    # +1 for [CLS]\n",
    "    pos_list = [m.start()+1 for m in re.finditer(name, content)]\n",
    "    count = len(pos_list)\n",
    "    return pos_list , count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orgi_2_array(names, contents):\n",
    "    x = []\n",
    "    binary_y = []\n",
    "    BIO_labels = []\n",
    "    nFound_count = 0\n",
    "    name_count = 0\n",
    "    \n",
    "    for i in range(len(contents)):\n",
    "        content = contents[i]\n",
    "        content = clean_string(content)\n",
    "\n",
    "        # record names\n",
    "        # name = names[i] # single\n",
    "        name_list = names[i]\n",
    "        names_label = ast.literal_eval(name_list) # string to list\n",
    "        # debug\n",
    "        \n",
    "\n",
    "        # init pos label arr\n",
    "        BIO_label = np.full((512), 2) # initial to all 2 (outside)\n",
    "        \n",
    "        # no AML person\n",
    "        if(name_list == '[]'):\n",
    "            binary_y.append(0)\n",
    "            x.append(content)\n",
    "            BIO_label[0] = 0 # first position 0(begin)\n",
    "            BIO_labels.append(BIO_label)\n",
    "\n",
    "        else:\n",
    "            # initial position list\n",
    "            start_pos = []\n",
    "            end_pos = []\n",
    "\n",
    "            # if (True): # single\n",
    "            for name in names_label:\n",
    "              temp, count = find_all(name, content)\n",
    "              if(temp == []):\n",
    "  #                 print(name + ' find error in data', i)\n",
    "                  nFound_count += 1\n",
    "                  continue\n",
    "              for j in range(count):\n",
    "                start_pos.append(temp[j])\n",
    "                end_pos.append(temp[j] + len(name))\n",
    "\n",
    "#                  01234\n",
    "#                B 00100\n",
    "#                I 00011\n",
    "#                O 11000\n",
    "            for j in range(len(start_pos)):\n",
    "                if(start_pos[j] < 512 and end_pos[j] < 512):\n",
    "                    BIO_label[start_pos[j]] = 0\n",
    "                    BIO_label[start_pos[j]+1 : end_pos[j]] = 1\n",
    "            binary_y.append(1)\n",
    "            x.append(content)\n",
    "            BIO_labels.append(BIO_label)\n",
    "            \n",
    "\n",
    "    x = np.array(x)\n",
    "    binary_y = np.array(binary_y)\n",
    "    BIO_labels = np.array(BIO_labels)\n",
    "    \n",
    "    print('nFound: ', nFound_count)\n",
    "    print('name_count:', name_count)\n",
    "    print(x.shape)\n",
    "    print(binary_y.shape)\n",
    "#     print(begin_pos_labels.shape)\n",
    "#     print(inside_pos_labels.shape)\n",
    "#     print(outside_pos_labels.shape)\n",
    "    print(BIO_labels.shape)\n",
    "    return x, binary_y, BIO_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(5360,)\n",
      "(5360,)\n",
      "(5360, 512)\n"
     ]
    }
   ],
   "source": [
    "names =  df_train['name']\n",
    "contents = np.array(df_train['full_content'].tolist())\n",
    "train_x, train_binary_y, train_bio_labels = orgi_2_array(names, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y , bio_labels):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "        self.bio_labels = bio_labels\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        bio_label = self.bio_labels[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, y , bio_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask, \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "PRETRAINED_MODEL_NAME = './chinese_xlnet_mid_pytorch/'\n",
    "\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "PRETRAINED_MODEL_NAME = './chinese_xlnet_mid_pytorch/'\n",
    "\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# 把input轉換成bert格式\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model budling \"\"\"\n",
    "from transformers import BertModel, XLNetModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "class AMLPredictModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(AMLPredictModel, self).__init__()\n",
    "        self.bert = XLNetModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        self.bert.output_hidden_states = True\n",
    "        self.BIO_classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 3),\n",
    "        ) # BIO tagging\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "#         position_ids=None,\n",
    "#         head_mask=None,\n",
    "#         inputs_embeds=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        \n",
    "        BIO = self.BIO_classifier(outputs[0]) # 512*HIDDENSIZE word vectors\n",
    "        BIO = self.softmax(BIO)\n",
    "        \n",
    "#         flag = 1\n",
    "        # debug\n",
    "#         if (flag):\n",
    "#             flag = 0\n",
    "#             print(\"forward output\")\n",
    "#             print(BIO)\n",
    "#             print(BIO_out)\n",
    "#             print(arg)\n",
    "#             print(\"---\")\n",
    "        \n",
    "        outputs = (BIO,) + outputs[2:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    predictions_withoutmax = None\n",
    "    binary_correct = 0\n",
    "    total = 0\n",
    "    bio_correct = 0\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0] # haveAML(binary classification)\n",
    "            after_softmax = nn.functional.softmax(logits.data, dim=1)\n",
    "            _, binary_pred = torch.max(after_softmax, 1)\n",
    "\n",
    "            temp = outputs[1]\n",
    "            bio_preds = torch.empty(temp.shape[0], 3, 512)\n",
    "            \n",
    "            for i in range(temp.shape[0]):  # run batchsize times\n",
    "                arg = temp[i].argmax(1) # 3*512 into class label\n",
    "                bio_preds[i] = arg\n",
    "\n",
    "            bio_preds = np.array(bio_preds)\n",
    "\n",
    "            # debug\n",
    "            print(\"get pred\")\n",
    "            print(\"b_pred \", binary_pred)\n",
    "            # print(binary_pred.shape)\n",
    "            # print(\"-----\")\n",
    "            # print(\"b_label \", data[3])\n",
    "            # print(data[3].shape)\n",
    "            print(\"BIO_labels \", data[4])\n",
    "            print(data[4].shape)\n",
    "            # print(\"---\")\n",
    "            print(\"BIO_pred \",bio_preds)\n",
    "            # print(bio_preds.shape)\n",
    "            # break\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                binary_labels = data[3]\n",
    "                total += binary_labels.size(0)\n",
    "                binary_correct += (binary_pred == binary_labels).sum().item()\n",
    "                bio_labels = data[4]\n",
    "                bio_correct += (bio_preds == bio_labels).sum().item()\n",
    "                # print(binary_correct)\n",
    "                # break\n",
    "\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = binary_pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, binary_pred))\n",
    "                \n",
    "            if predictions_withoutmax is None:\n",
    "                predictions_withoutmax = after_softmax\n",
    "            else:\n",
    "                predictions_withoutmax = torch.cat((predictions_withoutmax,after_softmax))\n",
    "    \n",
    "    if compute_acc:\n",
    "        binary_acc = binary_correct / total\n",
    "        bio_acc = bio_correct / total\n",
    "        return predictions, binary_acc, bio_acc\n",
    "    return predictions_withoutmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:word_embedding\n",
      "bert:layer\n",
      "bert:dropout\n",
      "BIO_classifier  Sequential(\n",
      "  (0): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n",
      "softmax         Softmax(dim=-1)\n",
      "sequence_summary SequenceSummary(\n",
      "  (summary): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (first_dropout): Identity()\n",
      "  (last_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model setting (training)\"\"\"\n",
    "from transformers import AdamW , XLNetConfig\n",
    "\n",
    "\n",
    "config = XLNetConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "BATCH_SIZE = 3\n",
    "trainSet = TrainDataset(train_input_dict, train_binary_y, train_bio_labels)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = AMLPredictModel(config)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) # AdamW = BertAdam\n",
    "binary_loss_fct = nn.CrossEntropyLoss()\n",
    "weight = torch.FloatTensor([500,450,1]).cuda()\n",
    "BIO_loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "#             print(_)\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 09:52:51.202201+08:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danny/.local/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AMLPredictModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 10:13:03.009536+08:00\t[epoch 1] loss: 1714.969\n",
      "2020-07-25 10:33:18.619415+08:00\t[epoch 2] loss: 1695.551\n",
      "2020-07-25 10:53:34.516089+08:00\t[epoch 3] loss: 1720.024\n",
      "2020-07-25 11:13:50.270268+08:00\t[epoch 4] loss: 1669.619\n",
      "2020-07-25 11:34:01.315622+08:00\t[epoch 5] loss: 1637.370\n",
      "2020-07-25 11:54:21.723540+08:00\t[epoch 6] loss: 1578.820\n",
      "2020-07-25 12:14:38.323781+08:00\t[epoch 7] loss: 1530.072\n",
      "2020-07-25 12:34:56.384024+08:00\t[epoch 8] loss: 1468.636\n",
      "2020-07-25 12:55:09.844126+08:00\t[epoch 9] loss: 1421.870\n",
      "2020-07-25 13:15:21.700096+08:00\t[epoch 10] loss: 1379.033\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timezone,timedelta\n",
    "\"\"\" training \"\"\"\n",
    "model = model.to(device)\n",
    "model.train() ##########################\n",
    "\n",
    "EPOCHS = 10\n",
    "dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "print(dt2)\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    binary_running_loss = 0.0\n",
    "    BIO_running_loss = 0.0\n",
    "    for data in trainLoader:\n",
    "    # data = testSet[21] # test model\n",
    "    # if(True):\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, \\\n",
    "        labels, BIO_label = [t.to(device) for t in data]\n",
    "\n",
    "      # tokens_tensors, segments_tensors, masks_tensors, labels, BIO_label = data\n",
    "      # tokens_tensors, segments_tensors, masks_tensors = data\n",
    "      # tokens_tensors = tokens_tensors.reshape((1,512)).to(device)\n",
    "      # segments_tensors = segments_tensors.reshape((1,512)).to(device)\n",
    "      # masks_tensors = masks_tensors.reshape((1,512)).to(device)\n",
    "      # labels = torch.tensor(labels).reshape((1)).to(device)\n",
    "      # BIO_label = torch.tensor(BIO_label).reshape((1,512)).to(device)\n",
    "\n",
    "      # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                      token_type_ids=segments_tensors, \n",
    "                      attention_mask=masks_tensors)\n",
    "\n",
    "        BIO_pred = outputs[0]\n",
    "        BIO_pred = torch.transpose(BIO_pred, 1, 2)\n",
    "      \n",
    "      # debug\n",
    "      # print(\"epoch output\")\n",
    "      # BIO_label[0][0] = 500\n",
    "      # BIO_label = BIO_label.squeeze()\n",
    "      # BIO_pred = BIO_pred.squeeze()\n",
    "      # print(BIO_label)\n",
    "      # print(BIO_label.shape)\n",
    "      # print(BIO_pred)\n",
    "      # print(BIO_pred.shape)\n",
    "      # print(outputs[0].shape)\n",
    "      # print(labels.shape)\n",
    "      # print(BIO_pred[0][0])\n",
    "      # print(BIO_pred[0][1])\n",
    "      # print(BIO_pred[0][2])\n",
    "      # break\n",
    "\n",
    "\n",
    "      # print(BIO_pred.shape)\n",
    "      # print(BIO_label.shape)\n",
    "        BIO_loss = BIO_loss_fct(BIO_pred, BIO_label)\n",
    "      # print(binary_loss, BIO_loss)\n",
    "        loss = BIO_loss\n",
    "      # print(loss)\n",
    "      # break\n",
    "      \n",
    "      # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    CHECKPOINT_NAME = './model/XLNet_bio_EPOCHES_' + str(epoch) + '.pkl'\n",
    "    torch.save(model, CHECKPOINT_NAME)\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    # _, binary_acc, bio_acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print('%s\\t[epoch %d] loss: %.3f' %\n",
    "          (dt2, epoch + 1, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "df_test = pd.read_csv('./tbrain/tbrain_test.csv')\n",
    "temp = df_test['name'].tolist()\n",
    "ans = []\n",
    "for i in range(len(temp)):\n",
    "    t = ast.literal_eval(temp[i])\n",
    "    if (len(t) == 0):\n",
    "        t.append('')\n",
    "    ans.append(t)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(491,)\n",
      "(491,)\n",
      "(491, 512)\n"
     ]
    }
   ],
   "source": [
    "names =  df_test['name']\n",
    "contents = np.array(df_test['full_content'].tolist())\n",
    "test_x, test_binary_y, test_bio_labels = orgi_2_array(names, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_2_string(tokens_tensors, have_AML, BIO_tagging, ckip_result):\n",
    "    result = []\n",
    "    if (have_AML.item() == 0):\n",
    "        result.append('')\n",
    "    else:\n",
    "        for j in range(1, 512):\n",
    "            if (BIO_tagging[j] == 0):\n",
    "                start = j\n",
    "                end = j + 1\n",
    "                while (end < 512 and BIO_tagging[end] == 1):\n",
    "                    end += 1\n",
    "                if (end > start + 1):\n",
    "                    s = tokenizer.decode(token_ids = tokens_tensors[start : end], skip_special_tokens = True)\n",
    "                    s = s.replace(' ', '')\n",
    "                    for k in range(len(ckip_result)):\n",
    "                        found = s.find(ckip_result[k])\n",
    "                        if (found != 1):\n",
    "                            result.append(ckip_result[k])\n",
    "    if (len(result) == 0):\n",
    "        result.append('')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, testLoader, BATCH_SIZE):\n",
    "  result = []\n",
    "  total_count = 0 # 第n筆data\n",
    "  with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "      # 將所有 tensors 移到 GPU 上\n",
    "      if next(model.parameters()).is_cuda:\n",
    "        data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "      \n",
    "      # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "      # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "      tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors)\n",
    "      \n",
    "      # print(tokens_tensors, tokens_tensors.shape)\n",
    "      # print(outputs[0], outputs[0].shape)\n",
    "      # print(outputs[1], outputs[1].shape)\n",
    "      \n",
    "      count = min(outputs[0].shape[0], BATCH_SIZE)\n",
    "      for i in range(count):  # run batchsize times\n",
    "        BIO_pred = outputs[0][i].argmax(1) # 3*512 into class label\n",
    "        text_token = tokens_tensors[i]\n",
    "        ckip_names = df_test.loc[total_count, 'ckip_names']\n",
    "        ckip_names_list = ast.literal_eval(ckip_names) # string to list\n",
    "        r = bio_2_string(text_token, test_binary_y[total_count], BIO_pred, ckip_names_list)\n",
    "        # print(r)\n",
    "        result.append(r)\n",
    "        total_count += 1\n",
    "      \n",
    "        # print(text_token, text_token.shape)\n",
    "        # print(have_AML, have_AML.shape)\n",
    "        # print(BIO_pred, BIO_pred.shape)\n",
    "        # print(\"recover\", tokenizer.decode(token_ids = tokens_tensors[0][1:5], skip_special_tokens = True))\n",
    "      # break\n",
    "    # print(result)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"testing\"\"\"\n",
    "import torch\n",
    "from transformers import BertConfig , XLNetConfig\n",
    "\n",
    "PRETRAINED_MODEL_NAME = './chinese_xlnet_mid_pytorch/'\n",
    "config = XLNetConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "model = AMLPredictModel(config)\n",
    "model = torch.load('./model/XLNet_bio_EPOCHES_9.pkl')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "testSet = TestDataset(test_input_dict)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "predictions = get_predictions(model, testLoader, BATCH_SIZE)\n",
    "\n",
    "# pred = predictions.cpu().data.numpy()\n",
    "# pred = np.argmax(pred, axis=1)\n",
    "# accuracy = (pred == test_binary_y).mean()\n",
    "# print('Your test accuracy is %.6f' % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ckip = pd.read_csv('./tbrain/ckip.csv')\n",
    "ckip_name = df_ckip.loc[df_ckip['ans'] == 1, 'name'].tolist()\n",
    "# ckip_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "ckip_name = set(ckip_name)\n",
    "for i in range(len(predictions)):\n",
    "  temp = set(predictions[i])\n",
    "  r = list(ckip_name & temp)\n",
    "  if (len(r) == 0):\n",
    "    r.append('')\n",
    "  result.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816142219400876"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_all(result, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
