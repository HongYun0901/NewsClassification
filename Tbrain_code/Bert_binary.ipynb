{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops, stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import BertTokenizer , BertConfig , BertModel , XLNetTokenizer, XLNetConfig , XLNetModel\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"./tbrain/tbrain_train_split.csv\")\n",
    "train_df = pd.read_csv(\"./tbrain/tbrain_train.csv\")\n",
    "\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "# test_df = pd.read_csv(\"./tbrain/tbrain_test_split.csv\")\n",
    "test_df = pd.read_csv(\"./tbrain/tbrain_test.csv\")\n",
    "\n",
    "train_df = pd.concat([train_df,test_df])\n",
    "\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "#     cc = OpenCC('t2s')\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "#     content = cc.convert(content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < 510:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太常\n",
    "            if string == '':\n",
    "                n = 510\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917,)\n",
      "(4917,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "names =  train_df['name'].tolist()\n",
    "contents = np.array(train_df['full_content'].tolist())\n",
    "ckip_names = train_df['ckip_names'].tolist()\n",
    "x = []\n",
    "binary_y = []\n",
    "names_y  = []\n",
    "content_all_names = []\n",
    "start_pos_labels = []\n",
    "end_pos_labels = []\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "\n",
    "    if(content=='nan'):\n",
    "        continue\n",
    "\n",
    "    \n",
    "    name_li  = ast.literal_eval(names[i])\n",
    "    if(len(name_li) == 0 ):\n",
    "#         content切句 \n",
    "#         split_content = cut_sent(content)\n",
    "#         chunks = combine_sentence(split_content)\n",
    "        \n",
    "#         for chunk in chunks:\n",
    "#             start_pos_label = np.zeros(512)\n",
    "#             end_pos_label = np.zeros(512)\n",
    "#             binary_y.append(0)\n",
    "#             start_pos_label[0] = 1\n",
    "#             end_pos_label[0] = 1\n",
    "#             x.append(chunk)\n",
    "#             start_pos_labels.append(start_pos_label)\n",
    "#             end_pos_labels.append(end_pos_label)\n",
    "#             content_all_names.append(content_ckip_names)\n",
    "        binary_y.append(0)\n",
    "        x.append(content)\n",
    "        \n",
    "    else:\n",
    "        binary_y.append(1)\n",
    "        x.append(content)\n",
    "        \n",
    "#         for name in name_li:\n",
    "        \n",
    "#             name = name.replace('\\n','').replace('\\t','').replace(' ','')\n",
    "#             #  content切句 \n",
    "            \n",
    "#             _pos = 0\n",
    "#             while True:\n",
    "#                 start_pos = content.find(name,_pos)\n",
    "#                 if(start_pos == -1):\n",
    "#                     break\n",
    "#                 start_pos += 1\n",
    "#                 _pos = start_pos\n",
    "#                 end_pos = start_pos + len(name)\n",
    "\n",
    "#                 if(start_pos < 512 and end_pos < 512 ):\n",
    "#                     binary_y.append(1)\n",
    "#                     x.append(content)\n",
    "\n",
    "#                     start_pos_label = np.zeros(512)\n",
    "#                     end_pos_label = np.zeros(512)\n",
    "#                     start_pos_label[start_pos] = 1\n",
    "#                     end_pos_label[end_pos] = 1\n",
    "\n",
    "#                     start_pos_labels.append(start_pos_label)\n",
    "#                     end_pos_labels.append(end_pos_label)\n",
    "#                     content_all_names.append(content_ckip_names)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "train_x = np.array(x)\n",
    "train_binary_y = np.array(binary_y)\n",
    "train_start_pos_labels = np.array(start_pos_labels)\n",
    "train_end_pos_labels = np.array(end_pos_labels)\n",
    "train_ckip_names = np.array(content_all_names)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_binary_y.shape)\n",
    "print(train_start_pos_labels.shape)\n",
    "print(train_end_pos_labels.shape)\n",
    "print(train_ckip_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "model_path  = './chinese_wwm_pytorch/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertBinrayClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertBinrayClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "\n",
    "#             \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "0\n",
      "train acc: 0.9103111653447223\n",
      "332.3948754274651\n",
      "1\n",
      "train acc: 0.924954240390482\n",
      "259.30843767182523\n",
      "2\n",
      "train acc: 0.9212934716290421\n",
      "220.9661099026813\n",
      "3\n",
      "train acc: 0.9192597112060199\n",
      "196.8287749353128\n",
      "4\n",
      "train acc: 0.9204799674598332\n",
      "179.9746544690136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-19b59dcef698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_all_768\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbinary_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "trainset = TrainDataset(train_input_dict, \n",
    "                        train_binary_y)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_path  = './chinese_wwm_pytorch/'\n",
    "config = BertConfig.from_pretrained(model_path + 'config.json',output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(model_path,config=config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "binary_model = BertBinrayClassifier()\n",
    "binary_model = binary_model.to(device)\n",
    "binary_model = binary_model.double()\n",
    "binary_model.train()\n",
    "\n",
    "\n",
    "weight = torch.tensor([0.1,1]).double().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "optimizer = torch.optim.Adam(binary_model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0,EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , binary_y  = [t.to(device) for t in data]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "        \n",
    "  \n",
    "\n",
    "        bert_all_768 = bert_outputs[1]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total += bert_all_768.size()[0]\n",
    "        \n",
    "        \n",
    "        logits = binary_model(bert_all_768.double())\n",
    "        loss = loss_fn(logits , binary_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        correct += (logits == binary_y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(binary_model.state_dict(),'./TB_multispan/Bert_binary_alldataset_' + str(epoch) + '.pkl')\n",
    "    print(epoch)\n",
    "    print('train acc:' , correct/total)\n",
    "    print(running_loss)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
