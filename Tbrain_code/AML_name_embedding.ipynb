{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf3EflIBRILC"
   },
   "source": [
    "# Name Embedding\n",
    "# Data Preprocessing\n",
    "## basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1595406344518,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "f47gpSwLRILD",
    "outputId": "ddc5035c-bf71-4094-964f-824cd70eb731",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# PRETRAINED_MODEL_NAME = \"hfl/rbtl3\" # RBTL3\n",
    "PRETRAINED_MODEL_NAME = \"bert_wwm_pretrain_tbrain\" # pretrained_bert_wwm\n",
    "df_train = pd.read_csv('./dataset/tbrain_train_1.csv')\n",
    "# df_2 = pd.read_csv('./dataset/tbrain_test_0.csv')\n",
    "# df_train = pd.concat([df_train, df_2])\n",
    "# df_train = df_train.fillna('[\\'\\]')\n",
    "# df_train = df_train.reset_index(drop=True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOO0iT-SRILK"
   },
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCS6r7fQRILO"
   },
   "outputs": [],
   "source": [
    "def find_all_name(name, content):\n",
    "    # +1 for [CLS]\n",
    "    pos_list = [m.start()+1 for m in re.finditer(name, content)]\n",
    "    count = len(pos_list)\n",
    "    return pos_list , count\n",
    "\n",
    "def find_all_ckip(name, content):\n",
    "    pos_list = []\n",
    "    i = 0\n",
    "    while (i < len(content)):\n",
    "        i = content.find(name, i)\n",
    "        if (i == -1):\n",
    "            break\n",
    "        pos_list.append(i+1)\n",
    "        i += 1\n",
    "    count = len(pos_list)\n",
    "    return pos_list , count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzAZ4mBzRILS"
   },
   "outputs": [],
   "source": [
    "def orgi_2_array(names, contents, ckips):\n",
    "    x = []\n",
    "    binary_y = []\n",
    "    BIO_labels = []\n",
    "    nFound_count = 0\n",
    "    name_count = 0\n",
    "    name_embeds = []\n",
    "    \n",
    "    for i in range(len(contents)):\n",
    "        content = contents[i]\n",
    "        content = clean_string(content)\n",
    "\n",
    "        # record names\n",
    "        # name = names[i] # single\n",
    "        name_list = names[i]\n",
    "        names_label = ast.literal_eval(name_list) # string to list\n",
    "        ckip_list = ckips[i] #####\n",
    "        ckips_label = ast.literal_eval(ckip_list) #####\n",
    "        \n",
    "\n",
    "        # init pos label arr\n",
    "        BIO_label = np.full((512), 2) # initial to all 2 (outside)\n",
    "        name_embed = np.full((512), 0) #######\n",
    "        \n",
    "        start_pos = []\n",
    "        end_pos = []\n",
    "        for name in ckips_label:\n",
    "            temp, count = find_all_ckip(name, content)\n",
    "            for j in range(count):\n",
    "                start_pos.append(temp[j])\n",
    "                end_pos.append(temp[j] + len(name))\n",
    "\n",
    "#                  01234\n",
    "#                B 00100\n",
    "#                I 00011\n",
    "#                O 11000\n",
    "            for j in range(len(start_pos)):\n",
    "                if(start_pos[j] < 512 and end_pos[j] < 512):\n",
    "                    name_embed[start_pos[j] : end_pos[j]] = 1\n",
    "        name_embeds.append(name_embed)\n",
    "        \n",
    "        # no AML person\n",
    "        if(name_list == '[]'):\n",
    "            binary_y.append(0)\n",
    "            x.append(content)\n",
    "            BIO_label[0] = 0 # first position 0(begin)\n",
    "            BIO_labels.append(BIO_label)\n",
    "\n",
    "        else:\n",
    "            # initial position list\n",
    "            start_pos = []\n",
    "            end_pos = []\n",
    "\n",
    "            # if (True): # single\n",
    "            for name in names_label:\n",
    "              temp, count = find_all_name(name, content)\n",
    "              if(temp == []):\n",
    "  #                 print(name + ' find error in data', i)\n",
    "                  nFound_count += 1\n",
    "                  continue\n",
    "              for j in range(count):\n",
    "                start_pos.append(temp[j])\n",
    "                end_pos.append(temp[j] + len(name))\n",
    "\n",
    "#                  01234\n",
    "#                B 00100\n",
    "#                I 00011\n",
    "#                O 11000\n",
    "            for j in range(len(start_pos)):\n",
    "                if(start_pos[j] < 512 and end_pos[j] < 512):\n",
    "                    BIO_label[start_pos[j]] = 0\n",
    "                    BIO_label[start_pos[j]+1 : end_pos[j]] = 1\n",
    "                    \n",
    "            binary_y.append(1)\n",
    "            x.append(content)\n",
    "            BIO_labels.append(BIO_label)\n",
    "            \n",
    "\n",
    "    x = np.array(x)\n",
    "    binary_y = np.array(binary_y)\n",
    "    BIO_labels = np.array(BIO_labels)\n",
    "    name_embeds = np.array(name_embeds)\n",
    "    \n",
    "    print('nFound: ', nFound_count)\n",
    "    print('name_count:', name_count)\n",
    "    print(x.shape)\n",
    "    print(binary_y.shape)\n",
    "#     print(begin_pos_labels.shape)\n",
    "#     print(inside_pos_labels.shape)\n",
    "#     print(outside_pos_labels.shape)\n",
    "    print(BIO_labels.shape)\n",
    "    return x, binary_y, BIO_labels, name_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNbG1YCCocSR"
   },
   "source": [
    "## Get Data List (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3849,
     "status": "ok",
     "timestamp": 1595406353080,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "xgxsXLjgRILW",
    "outputId": "a2d6345c-b3d6-4477-fdfe-52695ac6ef00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(4426,)\n",
      "(4426,)\n",
      "(4426, 512)\n"
     ]
    }
   ],
   "source": [
    "names =  df_train['name']\n",
    "ckip_names =  df_train['ckip_names']\n",
    "contents = np.array(df_train['full_content'].tolist())\n",
    "train_x, train_binary_y, train_bio_labels, train_name_embeds = orgi_2_array(names, contents, ckip_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(train_binary_y)):\n",
    "    if (df_train.loc[i, 'name'] != '[]'):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(train_bio_labels[6])\n",
    "print(train_name_embeds[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1595406353081,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "f2KfiQ36RILa",
    "outputId": "5035baf2-58ff-4739-e118-889dcfe0e7de",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426 4426\n",
      "0.07568910980569363\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x),len(train_binary_y))\n",
    "print(sum(train_binary_y)/len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6267 4403 2255442\n",
      "359.89181426519866 512.2511923688394 1.0\n"
     ]
    }
   ],
   "source": [
    "c_0, c_1, c_2 = 0, 0, 0\n",
    "for i in range(len(train_bio_labels)):\n",
    "    for j in range(512):\n",
    "        if (train_bio_labels[i][j] == 0):\n",
    "            c_0 +=1\n",
    "        elif (train_bio_labels[i][j] == 1):\n",
    "            c_1 += 1\n",
    "        else:\n",
    "            c_2 += 1\n",
    "print(c_0, c_1, c_2)\n",
    "print(c_2 / c_0, c_2 / c_1, c_2 / c_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_prpyiUxRILf"
   },
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDGrE3spRILg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, name_embeds, y , bio_labels):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.name_embeds = name_embeds #####\n",
    "        self.y = y\n",
    "        self.bio_labels = bio_labels\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        name_embeds = self.name_embeds[idx] ####\n",
    "        bio_label = self.bio_labels[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, name_embeds, y , bio_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict, name_embeds):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.name_embeds = name_embeds ####\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        name_embeds = self.name_embeds[idx] ####\n",
    "        return inputid , tokentype , attentionmask, name_embeds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0TjewOuRILk"
   },
   "source": [
    "## Go Through Tokenizer (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR3fIw2KqesN"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# 把input轉換成bert格式\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntuaz0TXRILp"
   },
   "source": [
    "## Model Budling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BertModel needed\"\"\"\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False):\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    head_mask[i],\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    head_mask[i],\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"name embedding\"\"\"\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.name_embeddings = nn.Embedding(2, 768)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, name_embeds=None):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "            \n",
    "        if name_embeds is None: ############\n",
    "            name_embeds = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "            \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        name_embeddings = self.name_embeddings(name_embeds) ##############\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings + name_embeddings #########\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bert Model\"\"\"\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertLayer\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well\n",
    "    as a decoder, in which case a layer of cross-attention is added between\n",
    "    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n",
    "    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the\n",
    "    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n",
    "    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n",
    "\n",
    "    .. _`Attention is all you need`:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "#     @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        name_embeds=None############\n",
    "    ):\n",
    "        r\"\"\"\n",
    "    Return:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token)\n",
    "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "            layer weights are trained from the next sentence prediction (classification)\n",
    "            objective during pre-training.\n",
    "\n",
    "            This output is usually *not* a good summary\n",
    "            of the semantic content of the input, you're often better with averaging or pooling\n",
    "            the sequence of hidden-states for the whole input sequence.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "            \n",
    "        if name_embeds is None: ###############\n",
    "            name_embeds = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "            \n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings( #######\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, \\\n",
    "            name_embeds=name_embeds\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9gzlkNxRILq"
   },
   "outputs": [],
   "source": [
    "\"\"\" model budling \"\"\"\n",
    "# from transformers import BertModel\n",
    "class AMLPredictModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AMLPredictModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, config = config)\n",
    "        self.BIO_classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 3),\n",
    "        ) # BIO tagging\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        name_embeds=None##########\n",
    "#         position_ids=None,\n",
    "#         head_mask=None,\n",
    "#         inputs_embeds=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            name_embeds=name_embeds##########\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds\n",
    "        )\n",
    "#         have_AML = outputs[1] # pooled cls (cls token through 1 linear and tanh)\n",
    "#         have_AML = self.classifier(have_AML)\n",
    "        \n",
    "        BIO = self.BIO_classifier(outputs[0]) # 512*HIDDENSIZE word vectors\n",
    "        BIO = self.softmax(BIO)\n",
    "        \n",
    "        outputs = (BIO, ) + outputs[2:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mvE2_BOo2aP"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3882,
     "status": "ok",
     "timestamp": 1595406443519,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "MnV2_agYRILu",
    "outputId": "65a6c2b6-952b-42de-b254-65431a43dc7e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at bert_wwm_pretrain_tbrain and are newly initialized: ['bert.embeddings.name_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (name_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "BIO_classifier  Sequential(\n",
      "  (0): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n",
      "softmax         Softmax(dim=-1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model setting (training)\"\"\"\n",
    "from transformers import BertConfig, AdamW\n",
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "BATCH_SIZE = 4\n",
    "trainSet = TrainDataset(train_input_dict, train_name_embeds, train_binary_y, train_bio_labels) ######\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = AMLPredictModel(config)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) # AdamW = BertAdam\n",
    "binary_loss_fct = nn.CrossEntropyLoss()\n",
    "weight = torch.FloatTensor([359, 510, 1]).cuda()\n",
    "# 1000 900 1 ()被蓋掉\n",
    "# 359 510 1\n",
    "# 500 450 1\n",
    "# 250 150 1 (X)\n",
    "# 125 50 1 (X)\n",
    "BIO_loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "            if n == 'embeddings':\n",
    "                print(_)\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 626074,
     "status": "ok",
     "timestamp": 1595407176170,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "N-Uw9qeDRIL1",
    "outputId": "ca596dee-eec6-46ae-a9b3-7d84d7cebfd4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-06 16:29:23.103021+08:00\n",
      "2020-08-06 16:32:55.068482+08:00\t[epoch 1] loss: 715.152\n",
      "2020-08-06 16:36:29.046389+08:00\t[epoch 2] loss: 680.134\n",
      "2020-08-06 16:40:03.400942+08:00\t[epoch 3] loss: 668.137\n",
      "2020-08-06 16:43:38.219949+08:00\t[epoch 4] loss: 663.128\n",
      "2020-08-06 16:47:13.362265+08:00\t[epoch 5] loss: 655.922\n",
      "2020-08-06 16:50:48.785965+08:00\t[epoch 6] loss: 621.998\n",
      "2020-08-06 16:54:24.418191+08:00\t[epoch 7] loss: 616.025\n",
      "2020-08-06 16:58:00.237739+08:00\t[epoch 8] loss: 617.684\n",
      "2020-08-06 17:01:36.168567+08:00\t[epoch 9] loss: 619.799\n",
      "2020-08-06 17:05:11.984975+08:00\t[epoch 10] loss: 614.874\n"
     ]
    }
   ],
   "source": [
    "\"\"\" training \"\"\"\n",
    "from datetime import datetime,timezone,timedelta\n",
    "\n",
    "model = model.to(device)\n",
    "model.train() ##########################\n",
    "\n",
    "EPOCHS = 10\n",
    "dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "print(dt2)\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    binary_running_loss = 0.0\n",
    "    BIO_running_loss = 0.0\n",
    "    for data in trainLoader:\n",
    "    # data = testSet[21] # test model\n",
    "    # if(True):\n",
    "        \n",
    "      tokens_tensors, segments_tensors, masks_tensors, name_embeds, \\\n",
    "      labels, BIO_label = [t.to(device) for t in data] ########\n",
    "\n",
    "      # 將參數梯度歸零\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # forward pass\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                      token_type_ids=segments_tensors, \n",
    "                      attention_mask=masks_tensors,\n",
    "                     name_embeds=name_embeds)#########\n",
    "\n",
    "      BIO_pred = outputs[0]\n",
    "      BIO_pred = torch.transpose(BIO_pred, 1, 2)\n",
    "\n",
    "      # print(BIO_pred.shape)\n",
    "      # print(BIO_label.shape)\n",
    "      BIO_loss = BIO_loss_fct(BIO_pred, BIO_label)\n",
    "      # print(binary_loss, BIO_loss)\n",
    "      loss = BIO_loss\n",
    "      # print(loss)\n",
    "      # break\n",
    "      \n",
    "      # backward\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 紀錄當前 batch loss\n",
    "      running_loss += loss.item()\n",
    "      BIO_running_loss += BIO_loss.item()\n",
    "        \n",
    "    CHECKPOINT_NAME = './model_1/pre_bert_wwm_bio_only_ckipname_embedding_EPOCHES_' + str(epoch) + '(w359).pkl' \n",
    "    torch.save(model.state_dict(), CHECKPOINT_NAME)\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    # _, binary_acc, bio_acc = get_predictions(model, trainLoader, compute_acc=True)\n",
    "    dt1 = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    dt2 = dt1.astimezone(timezone(timedelta(hours=8))) # 轉換時區 -> 東八區\n",
    "    print('%s\\t[epoch %d] loss: %.3f' %\n",
    "          (dt2, epoch + 1, running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WSwVmiBRIL5"
   },
   "source": [
    "---\n",
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1595407694683,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "XnTi_GkuEvxu",
    "outputId": "2a4168be-6c18-48ae-b7ed-93e5df188ccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "# df_test = pd.read_csv('./dataset/multi_tbrain_test.csv')\n",
    "df_test = pd.read_csv('./dataset/tbrain_test_1.csv')\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(491):\n",
    "    if (df_test.loc[i, 'name'].find('洪正義') != -1):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['李訓成', '王均', '昱盛', '李', '王宇正', '楊國文', '蔡', '蔡開宇', '李應', '王為']\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[21, 'ckip_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['蔡開宇', '王宇正', '李訓成']\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[21, 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1595407695986,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "ATDm-9ZGqnOB",
    "outputId": "8b657889-faec-4efc-bd75-926de13ec499"
   },
   "outputs": [],
   "source": [
    "temp = df_test['name'].tolist()\n",
    "ans = []\n",
    "for i in range(len(temp)):\n",
    "  t = ast.literal_eval(temp[i])\n",
    "  if (len(t) == 0):\n",
    "    t.append('')\n",
    "  ans.append(t)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1595407696907,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "jGmYmQWSpChJ",
    "outputId": "6f823e5c-9c7f-43b6-d235-12b6c7ebaf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFound:  0\n",
      "name_count: 0\n",
      "(491,)\n",
      "(491,)\n",
      "(491, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = df_test['name']\n",
    "ckip_names =  df_test['ckip_names']\n",
    "contents = np.array(df_test['full_content'].tolist())\n",
    "test_x, test_binary_y, test_bio_labels, test_name_embeds = orgi_2_array(names, contents, ckip_names)\n",
    "test_binary_y.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(test_binary_y)):\n",
    "    if (df_test.loc[i, 'name'] == '[]'):\n",
    "        test_binary_y[i] = 0\n",
    "    else:\n",
    "        test_binary_y[i] = 1\n",
    "# test_binary_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lBWBRl9rw4D"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = \"hfl/rbtl3\" # RBTL3\n",
    "PRETRAINED_MODEL_NAME = \"bert_wwm_pretrain_tbrain\" # pretrained_bert_wwm\n",
    "# MODEL_PATH = './model_3/RBTL3_bio_only_EPOCHES_9.pkl'\n",
    "MODEL_PATH = './model/pre_bert_wwm_bio_only_name_embedding_EPOCHES_9(w359).pkl'\n",
    "# MODEL_PATH = './model/pre_bert_wwm_bio_only_EPOCHES_9.pkl'\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HTrUZ6Ct98e"
   },
   "outputs": [],
   "source": [
    "test_input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5zN9utmvc19"
   },
   "outputs": [],
   "source": [
    "def bio_2_string(tokens_tensors, have_AML, BIO_tagging, ckip_result, BIO_prob):\n",
    "  result = []\n",
    "  if (have_AML.item() == 0):\n",
    "    result.append('')\n",
    "  else:\n",
    "    for j in range(1, 512):\n",
    "      if (BIO_tagging[j] == 0):\n",
    "        start = j\n",
    "        end = j + 1\n",
    "        while (end < 512 and BIO_tagging[end] == 1):\n",
    "          end += 1\n",
    "        if (end > start + 1):\n",
    "          if (start <= 3):\n",
    "            s = tokenizer.decode(token_ids = tokens_tensors[start : end +2], skip_special_tokens = True)\n",
    "          else:\n",
    "            s = tokenizer.decode(token_ids = tokens_tensors[start-1 : end +2], skip_special_tokens = True)\n",
    "          s = s.replace(' ', '')\n",
    "          # print(s)\n",
    "          for k in range(len(ckip_result)):\n",
    "            if (len(ckip_result[k]) < 2):\n",
    "              continue\n",
    "            elif (re.findall(r\"[%s]+\" %non_stops, ckip_result[k]) != [] \\\n",
    "                     or re.findall(r\"[%s]+\" %stops, ckip_result[k]) != []): # 有標點\n",
    "              continue\n",
    "            found = s.find(ckip_result[k])\n",
    "            if (found != -1):\n",
    "              if (found == 0):\n",
    "                # print(s)\n",
    "                prob = BIO_prob[start][0] # begin\n",
    "                for i in range(1, len(ckip_result[k]) + 1):\n",
    "                  p = BIO_prob[start+i][1]  # inside\n",
    "                  # print(p)\n",
    "                  prob *= p\n",
    "                # print('! len: ', len(ckip_result[k]), '\\tprobability: ', prob.item())\n",
    "              else:\n",
    "                # print(s)\n",
    "                prob = BIO_prob[start][1] # inside\n",
    "                for i in range(1, len(ckip_result[k]) + 1):\n",
    "                  p = BIO_prob[start+i][1]  # inside\n",
    "                  # print(p)\n",
    "                  prob *= p\n",
    "                # print('_ len: ', len(ckip_result[k]), '\\tprobability: ', prob.item())\n",
    "#               if (prob.item() >= 0.95):\n",
    "              if (True):\n",
    "                result.append(ckip_result[k])\n",
    "    if (len(result) == 0):\n",
    "      result.append('')\n",
    "    # print('---')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaeoDS0np-0S"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, testLoader, BATCH_SIZE):\n",
    "  result = []\n",
    "  total_count = 0 # 第n筆data\n",
    "  with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "      # 將所有 tensors 移到 GPU 上\n",
    "      if next(model.parameters()).is_cuda:\n",
    "        data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "      \n",
    "      # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "      # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "      tokens_tensors, segments_tensors, masks_tensors, name_embeds = data[:4] #####\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors,\n",
    "                  name_embeds=name_embeds) #####\n",
    "      \n",
    "      count = min(outputs[0].shape[0], BATCH_SIZE)\n",
    "      for i in range(count):  # run batchsize times\n",
    "#         have_AML = outputs[0][i].argmax()\n",
    "        BIO_pred = outputs[0][i].argmax(1) # 3*512 into class label\n",
    "        text_token = tokens_tensors[i]\n",
    "        ckip_names = df_test.loc[total_count, 'ckip_names']\n",
    "        ckip_names_list = ast.literal_eval(ckip_names) # string to list\n",
    "        r = bio_2_string(text_token, test_binary_y[total_count], BIO_pred, ckip_names_list, outputs[0][i])\n",
    "#         print(BIO_pred)\n",
    "        result.append(r)\n",
    "        total_count += 1\n",
    "#       break\n",
    "    # print(result)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkAJDq2Qt72t"
   },
   "outputs": [],
   "source": [
    "\"\"\"testing\"\"\"\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)\n",
    "# model = AMLPredictModel(config)\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# model = torch.load(MODEL_PATH)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "testSet = TestDataset(test_input_dict, test_name_embeds)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "predictions = get_predictions(model, testLoader, BATCH_SIZE)\n",
    "\n",
    "# pred = predictions.cpu().data.numpy()\n",
    "# pred = np.argmax(pred, axis=1)\n",
    "# accuracy = (pred == test_binary_y).mean()\n",
    "# print('Your test accuracy is %.6f' % (accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p5uEYssCYfD"
   },
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l360S9dPlOLP"
   },
   "outputs": [],
   "source": [
    "df_ckip = pd.read_csv('../../nlp/NewsClassify/Mouth/ForDenny/dataset1.csv')\n",
    "# df_ckip = pd.read_csv('ckip.csv')\n",
    "ckip_name = df_ckip.loc[df_ckip['ans'] == 1, 'name'].tolist()\n",
    "# ckip_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N69stFCSkVp-"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "ckip_name = set(ckip_name)\n",
    "for i in range(len(predictions)):\n",
    "  temp = set(predictions[i])\n",
    "  r = list(ckip_name & temp)\n",
    "  if (len(r) == 0):\n",
    "    r.append('')\n",
    "  result.append(r)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9717922606924643"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_all(result, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['蔡開宇', '王宇正'],\n",
       " [''],\n",
       " ['張永泉', '郭明賓'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['李瑞廷'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['牟孝儀', '陳學敏'],\n",
       " ['許祈文'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['黃顯雄', '黃世陽'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['劉昌松', '呂建安', '呂東英', '林裕豐'],\n",
       " [''],\n",
       " [''],\n",
       " ['蕭博文', '章啟明'],\n",
       " ['吳昇儒', '張建生'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['陳慰慈', '莊錫根'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['朱國榮', '劉慶珠'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['周麗真', '張志偉'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['孫幼英', '張玉鳳', '蘇芸樂', '林偉強', '鍾素娥'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['賴俊吉'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['繆竹怡', '姚介修'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['陳偉', '陳偉和'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['吳昇儒', '鄭聖儒'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['林右正', '李立法'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['莊琇媛'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['歐彥志', '陳俞雄', '楊文值', '張東耀', '楊士弘', '楊家班'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['朱國榮', '林桂馨'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['李全教'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['李威儀'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['邱世忠'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['柯賜海'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['劉威甫'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['何培才'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['姜維池', '郭永鴻'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['林敏志'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['劉吉雄', '林輝宏', '裴振福', '呂宗南', '吳東明', '楊自立'],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " [''],\n",
       " ['劉慶侯'],\n",
       " [''],\n",
       " [''],\n",
       " ['']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../../nlp/NewsClassify/Tbrain_formal.ipynb')\n",
    "\n",
    "from Tbrain_formal import check_pred_name_is_real_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_name_is_real_ans(pred_name_list,news,dataset):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    content = clean_string(news)\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "        \n",
    "    for name in pred_name_list:\n",
    "        \n",
    "        content_max_length = 512-3-len(name)\n",
    "        \n",
    "        if len(content) >= content_max_length:\n",
    "            content = content[:content_max_length]\n",
    "            \n",
    "        input_ids = tokenizer.encode(name, content)\n",
    "        if(len(input_ids)>512):\n",
    "            continue\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        while len(input_ids) < 512:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        train_input_ids.append(input_ids)\n",
    "        train_token_types.append(segment_ids)\n",
    "        train_attention_mask.append(input_mask)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    \n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    \n",
    "    check_point = ''\n",
    "    if(dataset == 0):\n",
    "#         最一開始的dataset\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_13.pkl'\n",
    "\n",
    "    elif (dataset == 1):\n",
    "#         tbrain_train (1).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset1_epoch17.pkl'\n",
    "\n",
    "    elif (dataset == 2):\n",
    "#         tbrain_train (2).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset2_epoch13.pkl'\n",
    "\n",
    "    elif (dataset == 3):\n",
    "#         tbrain_train (3).csv\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_dataset3_epoch18.pkl'\n",
    "\n",
    "    elif (dataset == 4):\n",
    "#        traindata + testdata\n",
    "        check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_alldataset_epoch18.pkl'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     check_point = '../../nlp/NewsClassify/TB_multispan/Bert_wwm_ckip_name_is_ans_13.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(pred_name_list)\n",
    "            return list(pred_name_list[pred>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(ans)):\n",
    "    if (predictions[i][0] == ''):\n",
    "        result.append([''])\n",
    "        continue\n",
    "    result.append(check_pred_name_is_real_ans(predictions[i] , df_test.loc[i,'full_content'], 1))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17219,
     "status": "ok",
     "timestamp": 1595407726669,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "l9mHMcv91BSH",
    "outputId": "c4fc8334-5956-476f-8755-bfc05fff0426"
   },
   "outputs": [],
   "source": [
    "eval_all(result, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Only name testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6256756756756757\n"
     ]
    }
   ],
   "source": [
    "only_name_ans = []\n",
    "only_name_pred = []\n",
    "for i in range(len(ans)):\n",
    "    if (ans[i][0] != ''):\n",
    "        only_name_ans.append(ans[i])\n",
    "        only_name_pred.append(result[i])\n",
    "print(eval_all(only_name_pred, only_name_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_bert_wwm_result1 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴俊吉'], [''], [''], ['張銘坤', '陳揚宗'], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李士綸', '吳哲瑋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林志聰', '伍政山'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['張承平'], [''], [''], [''], [''], [''], [''], ['雷俊玲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳仕修', '黃國昌', '徐仲榮'], [''], ['林睿耆', '周漢祥', '林昱伯', '詹騏瑋', '林煒智'], [''], [''], ['秦儷舫', '童仲彥', '黃國昌'], [''], [''], [''], [''], ['蘇怡寧', '禾馨'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['詹舜淇', '方俐婷', '詹逸宏', '詹雅琳', '詹雯婷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃顯雄', '黃世陽'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['葉大慧', '吳國昌', '吳孝昌', '魏君婷', '張欽堯'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李宛霞', '黃梅雪'], ['陳有意', '陳運作', '陳坦承', '陳武騰', '黃振榮'], ['許長裕'], [''], [''], ['蔡開宇', '李訓成', '王宇正'], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], ['王宇承', '陳瑞芳', '振瑞', '陳澤信'], [''], [''], ['李育英', '林煜傑', '李文潔', '劉矢口'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮', '楊敬熙'], [''], [''], ['黃聲儀', '陳功源', '黃泳學', '羅栩亮', '黃馨儀', '高兆良'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['洪正義'], [''], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], ['王毓雅', '羅瑞榮'], [''], [''], [''], [''], [''], ['蔡文娟'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['柯賜海'], [''], [''], [''], [''], ['黃文鴻', '吳承霖', '陳玟叡', '蔡英俊'], [''], [''], [''], [''], [''], ['賴素如'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['謝宥宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['郭政權', '楊天生', '蔡茂寅', '郭說明'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳承霖', '陳玟叡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何壽川'], [''], [''], [''], [''], [''], [''], [''], [''], ['李錫璋', '陳清江'], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "pre_bert_wwm_result2 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['宋芷妍', '王安石'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['繆竹怡'], [''], [''], ['陳建飛'], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], ['劉威甫'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['邱嘉進'], [''], [''], [''], ['陳宣銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許家榮', '林詩芳', '王紀棠', '殷倜凡', '謝巧莉'], [''], [''], ['姚慶佳', '姚慶佳男'], ['吳運豐', '雲從龍'], ['李春生', '周正華', '黃建強', '高振殷', '鄭銘富', '呂家緯'], [''], [''], [''], [''], [''], [''], ['鄧超鴻', '道克明'], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], ['柯志龍', '俞小凡', '翁家明', '瓊瑤', '張興蕙', '張哲維', '夏婉君'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蒲念慈'], [''], ['詹舜淇', '詹逸宏', '詹雅琳'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['侯君鍵'], [''], [''], [''], [''], ['李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠', '錢利忠'], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], [''], [''], [''], [''], ['李榮華'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠', '鍾榮昌', '彭振源'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴嚮景', '陳俊宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李佰全', '葉玲', '林陀桂英'], [''], [''], [''], [''], ['林政賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊治渝', '李宗瑞'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳東明', '劉吉雄', '林輝宏', '張建華', '裴振福', '呂宗南'], [''], [''], ['葉冠廷', '康明璋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林金龍', '黃鈺蘋', '呂翠峰', '黃子愛', '顏雪藝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳建三', '羅秋英', '陳斯婷', '陳斯婷批戰袍'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃振龍', '蔡文旭', '張治忠', '張道銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "pre_bert_wwm_result3 = [[''], [''], [''], [''], [''], [''], ['張君豪', '李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔培明'], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林欣月'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃薪哲', '吳寶玉', '余信憲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['袁昶平', '洪勝明'], [''], [''], ['葉麗珍', '祥禾', '趙鈞震', '葉麗貞', '陳耀東'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林愈得', '陳清裕', '曾盛陽', '曾盛麟', '曾美菁'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '葉美麗', '茂宇', '賴麗團'], [''], [''], [''], ['蔡維峻', '林銘宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊嘉仁', '林崇傑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊昇穎', '林嘉東'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['卓國華'], [''], [''], [''], ['戴盛世', '宣昶孔'], [''], ['許祈文'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李維凱'], [''], [''], [''], [''], ['吳宗憲', '林清井', '湯蕙禎', '劉奕發', '歐炳辰'], ['陳發貴'], [''], [''], [''], [''], [''], [''], ['陳俊佑', '陳致銘', '王延順'], [''], [''], [''], ['孔朝'], [''], ['詹昭書'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['朱啟仁'], ['戴盛世'], [''], [''], [''], ['徐詩彥'], [''], [''], [''], [''], ['陳麗珍'], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['游美雲'], [''], [''], [''], [''], ['洪丞俊', '謝介裕', '黃丹怡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['范筱梵夫', '江智銓', '江智詮', '王柏森', '范筱梵'], [''], [''], [''], [''], [''], [''], ['吳銀嵐', '吳金虎', '張安樂', '徐宏杰', '許國楨'], [''], [''], [''], ['蒲念慈'], ['陳之漢', '紀雅玲', '林睿君'], [''], ['李榮勝', '黃錦燕'], [''], [''], ['']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBTL3_result1 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴俊吉'], [''], [''], ['張銘坤', '陳揚宗'], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李士綸', '吳哲瑋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林志聰', '伍政山'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['張承平'], [''], [''], [''], [''], [''], [''], ['雷俊玲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡英文', '陳仕修', '黃國昌', '徐仲榮'], [''], ['林睿耆', '周漢祥', '林昱伯', '詹騏瑋', '林煒智'], [''], [''], ['秦儷舫', '童仲彥', '黃國昌'], [''], [''], [''], [''], ['禾馨'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['詹舜淇', '方俐婷', '詹逸宏', '詹雅琳', '詹雯婷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃顯雄', '黃世陽'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['葉大慧', '吳國昌', '吳孝昌', '魏君婷', '張欽堯'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李宛霞', '黃梅雪'], ['陳運作', '陳坦承', '黃振榮', '陳武騰', '黃不滿'], ['許長裕'], [''], [''], ['蔡開宇', '李訓成', '王宇正'], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], ['王俊忠', '王宇承', '振瑞', '陳澤信'], [''], [''], ['李育英', '林煜傑', '李文潔', '劉矢口'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮', '楊敬熙'], [''], [''], ['黃聲儀', '陳功源', '黃泳學', '羅栩亮', '黃馨儀'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['洪正義'], [''], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], ['王毓雅', '羅瑞榮'], [''], [''], [''], [''], [''], ['蔡文娟'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['柯賜海'], [''], [''], [''], [''], ['黃文鴻', '吳承霖', '丁偉杰', '陳玟叡', '蔡英俊'], [''], [''], [''], [''], [''], ['賴素如'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['謝宥宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['郭政權', '楊天生', '蔡茂寅', '郭說明'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳承霖', '陳玟叡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何壽川'], [''], [''], [''], [''], [''], [''], [''], [''], ['李錫璋', '陳清江'], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "RBTL3_result2 = [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王安石'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['繆竹怡'], [''], [''], ['陳建飛'], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['邱嘉進', '游芳男'], [''], [''], [''], ['陳宣銘'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許家榮', '林詩芳', '林維俊', '王紀棠', '殷倜凡', '謝巧莉'], [''], [''], ['姚慶佳', '姚慶佳男'], ['吳運豐', '雲從龍'], ['李春生', '周正華', '黃建強', '高振殷', '鄭銘富', '呂家緯'], [''], [''], [''], [''], [''], [''], ['鄧超鴻', '道克明'], [''], [''], [''], [''], [''], [''], [''], [''], ['崔明禮'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], ['柯志龍', '俞小凡', '瓊瑤', '林俊峰', '張興蕙', '張哲維', '夏婉君'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蒲念慈'], [''], ['詹雯婷', '方俐婷', '詹逸宏', '詹雅琳'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['侯君鍵'], [''], [''], [''], [''], ['于曉艷'], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '王光遠'], [''], [''], [''], [''], [''], ['鍾增林', '曾國財'], [''], [''], [''], [''], [''], ['李榮華', '鄭徒刑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楚瑞芳', '彭振源'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['賴嚮景', '陳俊宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李佰全', '葉玲', '林陀桂英'], [''], [''], [''], [''], ['林政賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['楊治渝', '李宗瑞'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['吳東明', '劉吉雄', '林輝宏', '張建華', '裴振福'], [''], [''], ['陳菊', '葉冠廷', '康明璋'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['顏則獲', '林金龍', '黃鈺蘋', '黃子愛', '顏雪藝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳建三', '羅秋英'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王益洲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡文旭', '張道銘', '呂雅純', '張治忠', '蔡英俊', '黃振龍', '李欣潔'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
    "\n",
    "RBTL3_result3 = [[''], [''], [''], [''], [''], [''], ['李孟謙', '于曉艷'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['崔培明'], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡賜爵', '劉昌松', '畢鈞輝'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳韋霖'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林欣月'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['黃薪哲', '吳寶玉', '余信憲'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['袁昶平', '洪勝明', '洪介紹'], [''], [''], ['祥禾', '葉麗珍', '葉麗貞', '趙鈞震'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['陳清裕', '林愈得', '曾美菁', '曾盛麟'], [''], [''], [''], [''], [''], [''], [''], ['林勇任', '郭雅雯', '蘇震清', '葉美麗', '茂宇', '賴麗團'], [''], [''], [''], ['蔡維峻', '林銘宏'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['林崇傑'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['周宗賢'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許祈文'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['李發布', '李維凱'], [''], [''], [''], [''], ['劉奕發', '吳宗憲', '歐炳辰', '林清井'], ['陳發貴'], [''], [''], [''], [''], [''], [''], ['陳俊佑', '陳致銘', '王延順'], [''], [''], [''], ['孔朝'], [''], ['詹昭書', '洪美秀'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['朱啟仁', '楊善淵'], ['楊富巖', '戴盛世', '錢利忠'], [''], [''], [''], ['徐詩彥', '林繼蘇'], [''], [''], [''], [''], ['陳麗珍'], [''], [''], [''], [''], ['何培才'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['許玉秀', '吳淑珍', '王隆昌'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['蔡思庭', '楊正平'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['游美雲'], [''], [''], [''], [''], ['洪丞俊', '謝介裕', '黃丹怡'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['王柏森', '江智銓', '范筱梵', '江智詮'], [''], [''], [''], [''], [''], [''], ['徐宏杰', '許國楨', '吳金虎', '張安樂'], [''], [''], [''], [''], ['林睿君'], [''], ['李榮勝', '黃錦燕'], [''], [''], ['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_result = []\n",
    "intersect_result = []\n",
    "for i in range(len(ans)):\n",
    "    temp1 = set(pre_bert_wwm_result3[i])\n",
    "    temp2 = set(RBTL3_result3[i])\n",
    "    union = list(temp1 | temp2)\n",
    "    intersect = list(temp1 & temp2)\n",
    "    if (len(union) == 0):\n",
    "        union.append('')\n",
    "    if (len(intersect) == 0):\n",
    "        intersect.append('')\n",
    "\n",
    "    union_result.append(union)\n",
    "    intersect_result.append(intersect)\n",
    "print(eval_all(union_result,ans))\n",
    "print(eval_all(intersect_result,ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaV5cGVupMAG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3PpHOIJA5eJ"
   },
   "source": [
    "# Modle Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdZns74dlnZk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from zhon.hanzi import stops, non_stops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "\n",
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content\n",
    "\n",
    "def bio_2_string(tokens_tensors, have_AML, BIO_tagging, ckip_result, tokenizer):\n",
    "  result = []\n",
    "  if (have_AML == 0):\n",
    "    result.append('')\n",
    "  else:\n",
    "    for j in range(1, 512):\n",
    "      if (BIO_tagging[j] == 0):\n",
    "        start = j\n",
    "        end = j + 1\n",
    "        while (end < 512 and BIO_tagging[end] == 1):\n",
    "          end += 1\n",
    "        if (end > start + 1):\n",
    "          s = tokenizer.decode(token_ids = tokens_tensors[start : end], skip_special_tokens = True)\n",
    "          s = s.replace(' ', '')\n",
    "          for k in range(len(ckip_result)):\n",
    "            if (len(ckip_result[k]) < 2):\n",
    "              continue\n",
    "            elif (re.findall(r\"[%s]+\" %non_stops, ckip_result[k]) != [] \\\n",
    "                     or re.findall(r\"[%s]+\" %stops, ckip_result[k]) != []): # 有標點\n",
    "              continue\n",
    "            found = s.find(ckip_result[k])\n",
    "            if (found != 1):\n",
    "              result.append(ckip_result[k])\n",
    "    if (len(result) == 0):\n",
    "      result.append('')\n",
    "  return result\n",
    "\n",
    "def get_predictions(model, testLoader, binary_y, ckip_names, tokenizer):\n",
    "  result = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "      # 將所有 tensors 移到 GPU 上\n",
    "      if next(model.parameters()).is_cuda:\n",
    "        data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "        # tokens_tensors ,  segments_tensors , masks_tensors = [t.to(\"cuda:0\") for t in data[:3]]\n",
    "      \n",
    "      # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "      # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "      tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "      outputs = model(input_ids=tokens_tensors, \n",
    "                  token_type_ids=segments_tensors, \n",
    "                  attention_mask=masks_tensors)\n",
    "      \n",
    "      count = min(outputs[0].shape[0], 1)\n",
    "      for i in range(count):  # run batchsize times\n",
    "        have_AML = outputs[0][i].argmax()\n",
    "        BIO_pred = outputs[1][i].argmax(1) # 3*512 into class label\n",
    "        text_token = tokens_tensors[i]\n",
    "        ckip_names_list = ast.literal_eval(ckip_names) # string to list\n",
    "        r = bio_2_string(text_token, binary_y, BIO_pred, ckip_names_list, tokenizer)  #####\n",
    "        result.append(r)\n",
    "  return result\n",
    "\n",
    "\"\"\" model budling \"\"\"\n",
    "class AMLPredictModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AMLPredictModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"hfl/rbtl3\", config = config)\n",
    "        self.classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 2),\n",
    "        ) # binary classification\n",
    "        self.BIO_classifier = nn.Sequential(\n",
    "                        nn.Linear(config.hidden_size, 3),\n",
    "        ) # BIO tagging\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        have_AML = outputs[1] # pooled cls (cls token through 1 linear and tanh)\n",
    "        have_AML = self.classifier(have_AML)\n",
    "        BIO = self.BIO_classifier(outputs[0]) # 512*HIDDENSIZE word vectors\n",
    "        BIO = self.softmax(BIO)\n",
    "        \n",
    "        outputs = (have_AML, BIO) + outputs[2:]\n",
    "        return outputs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask, \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "def get_AML_person(content, binary_y, ckip_name):\n",
    "  MODEL_PATH = './model/RBT3_bio_only_EPOCHES_2.pkl'\n",
    "  content = clean_string(content)\n",
    "  test_x = [content]\n",
    "\n",
    "  tokenizer = BertTokenizer.from_pretrained(\"hfl/rbtl3\")\n",
    "  test_input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                          add_special_tokens=True,\n",
    "                          max_length=512,\n",
    "                          return_special_tokens_mask=True,\n",
    "                          pad_to_max_length=True,\n",
    "                          return_tensors='pt',\n",
    "                          truncation=True)\n",
    "\n",
    "  config = BertConfig.from_pretrained(\"hfl/rbtl3\", output_hidden_states=True)\n",
    "  model = AMLPredictModel(config)\n",
    "  model = torch.load(MODEL_PATH)\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = model.to(device)\n",
    "  model = model.eval()\n",
    "  testSet = TestDataset(test_input_dict)\n",
    "  testLoader = DataLoader(testSet, batch_size=1)\n",
    "  # print(\"testSet\", testSet[0])\n",
    "  result = get_predictions(model, testLoader, binary_y, ckip_name, tokenizer)\n",
    "  result = set(result[0])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7788,
     "status": "ok",
     "timestamp": 1595403320782,
     "user": {
      "displayName": "李韋宗",
      "photoUrl": "",
      "userId": "06431049746033623123"
     },
     "user_tz": -480
    },
    "id": "jC9t17JFA_M7",
    "outputId": "3281483c-a340-43fe-cb7d-bae864a1b7ae"
   },
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "t = '對包商偷工減料放水 關西2公務員改重判10年\\n\\\n",
    "高等法院更一審今依貪污治罪條例的「公務員經辦公用工程舞弊罪」，將蔡開宇、王宇正均判10年重罪、褫奪公權5年，業者李訓成判7年6月、褫奪公權3年。可上訴。（資料照）\\n\\\n",
    "2019-04-23 14:45:13\\n\\\n",
    "〔記者楊國文／台北報導〕新竹縣關西鎮公所9年前先後辦理「關西鎮轄內全鎮柏油等後續六階工程」、「南山里等15里義民節道路鋪面改善工程」時，當時負責驗收的建設課技士蔡開宇及技佐王宇正2人，涉利用職務之便對偷工減料的承包商放水，高等法院更一審今依貪污治罪條例的「公務員經辦公用工程舞弊罪」，將蔡、王均判10年重罪、褫奪公權5年，業者李訓成判7年6月、褫奪公權3年。可上訴。\\n\\\n",
    "判決指出，昱盛公司於2010年間分別以1140萬元、1075萬標得關西鎮「南山里等15里義民節道路鋪面改善工程」、「關西鎮轄內全鎮柏油等後續六階工程」2項工程，關西鎮公所均指派建設課技士蔡開宇及技佐王宇正擔任工程承辦人和驗收人員。\\n\\\n",
    "請繼續往下閱讀...\\n\\\n",
    "蔡開宇、王宇正於2011年1月間排定各路段長度、寬度測量及鑽心進行採樣前，先把樁號、里程位置等資料提供給昱盛經理李訓成，方便他於驗收前動手腳，以利勘驗通過。\\n\\\n",
    "王宇正檢驗鑽試體厚度時，明知李應採取直徑10公分的試體，並在試體上簽名及註記編號後才能送實驗室檢驗壓實度，但王為掩蓋該公司偷工減料，僅採取直徑5公分試體，假裝測量厚度後，並將鑽心試體放回原鑽孔洞內，任由李訓成自行以合格試體送驗。\\n\\\n",
    "由於兩案僅有一件領取到工程款，另一件則未領取工程款即遭查獲。新竹地院認定蔡開宇觸犯公務員假借職務上之權力故意犯詐欺取財未遂罪，判3年4月、褫奪公權3年，王宇正觸犯公務員假借職務上之權力故意犯詐欺取財，判5年徒刑、褫奪公權5年，李訓成依詐欺取財罪及詐欺取財未遂罪合併判刑6年10月。\\n\\\n",
    "高院審理後，將蔡開宇、王宇正均依一審相同罪名，分別改判蔡2年10月、褫奪公權3年，王4年6月、褫奪公權5年，李仍依詐欺取財罪及詐欺取財未遂罪合併判刑5年2月。\\n\\\n",
    "不用抽 不用搶 現在用APP看新聞 保證天天中獎'\n",
    "ckip_n = '[\\'李訓成\\', \\'王均\\', \\'昱盛\\', \\'李\\', \\'王宇正\\', \\'楊國文\\', \\'蔡\\', \\'蔡開宇\\', \\'李應\\', \\'王為\\']'\n",
    "start = datetime.datetime.now().timestamp()\n",
    "ans = get_AML_person(t, 1, ckip_n)\n",
    "print(ans)\n",
    "end = datetime.datetime.now().timestamp()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TII4L6CjEafI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AML_RBTL3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
