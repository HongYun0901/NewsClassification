{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import BertTokenizer , BertConfig , BertModel , XLNetTokenizer, XLNetConfig , XLNetModel\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    content = content.replace('\\n','').replace('\\t','').replace(' ','').replace('\\xa0','')\n",
    "    content = re.sub(\"[●▼►★]\", \"\",content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "def combine_sentence(sentences):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < 510:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太常\n",
    "            if string == '':\n",
    "                n = 510\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict,text):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.text = text\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        text = self.text[idx]\n",
    "\n",
    "        return inputid , tokentype , attentionmask , text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "# class posClassfication_new(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(posClassfication_new, self).__init__()\n",
    "#         self.start_task = nn.Sequential(\n",
    "#             nn.Linear(768, 1),\n",
    "#         )    \n",
    "#         self.end_task = nn.Sequential(\n",
    "#             nn.Linear(768, 1),\n",
    "#         ) \n",
    "#         self.binary_task = nn.Sequential(\n",
    "#             nn.Linear(768, 2),\n",
    "#         )\n",
    "        \n",
    "\n",
    "# #             \n",
    "#     def forward(self, start_x, end_x, pool_cls):\n",
    "#         start_x = start_x.double()\n",
    "#         end_x = end_x.double()\n",
    "#         pool_cls = pool_cls.double()\n",
    "    \n",
    "#         start_out = self.start_task(start_x)\n",
    "#         end_out = self.end_task(end_x)\n",
    "#         binary_out = self.binary_task(pool_cls)\n",
    "        \n",
    "#         return start_out , end_out , binary_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlnet_news_has_ans(news):\n",
    "    from transformers import XLNetForSequenceClassification,XLNetTokenizer\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_dict):\n",
    "            self.input_ids = input_dict['input_ids']\n",
    "            self.token_type_ids = input_dict['token_type_ids']\n",
    "            self.attention_mask = input_dict['attention_mask']\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    lm_path = './chinese_xlnet_mid_pytorch/'\n",
    "\n",
    "    content = clean_string(news)\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    input_dict = tokenizer.batch_encode_plus([content], \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "    \n",
    "        \n",
    "\n",
    "    BATCH_SIZE = 1\n",
    "    testset = Testset(input_dict)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "\n",
    "    NUM_LABELS = 2\n",
    "    model = XLNetForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    check_point = './TB_multispan/XLNet_fromseq_3.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()[0]\n",
    "            return pred\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = '因巴拉圭東方市的「東方工業區」開發案控告前外交部長林宗俊誹謗的台商羅常軍，13日下午到台北地檢署出庭，羅常軍強調，他10日已經領到巴拉圭良民證，外交部指他在巴拉圭涉有訴訟，不但是胡說八道，更是刻意抹黑。 羅常軍是因外交部在2016年初給立法院的中央政府總預算審查的書面報告中，就「有效管理台巴（拉圭）工業區以促進國際合作經貿發展」案中，指「陳正平（原園區聘請管理經理）及羅常軍背信及詐欺案」仍在巴國法院進行，列入立法院公報並上網。  為此羅常軍行文外交部要求將公報自網路下架未果，因此提告前外文部長林宗俊等誹謗及偽造文書，總共在北檢有7個案件。   羅常軍明示，巴拉圭東方工業區開發案，就是國家坑害僑商的案子，所有文件證據都是齊備的，甚至包括機密及極機密文件。至於外交部指他在巴拉圭有詐欺及背信訴訟，羅常軍說，他10日已經領取巴拉圭的良民證，如果真如外交部所言，巴拉圭不可能核發良民證給他，顯然外交部是胡說八道刻意抹黑。'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlnet_news_has_ans(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6316, 0.3684]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_have_answer(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article         檢調偵辦「三鑫集團」以投資俄羅斯賭場等名目吸金12億元案，發現三鑫集團負責人曾裕仁去年因債務...\n",
      "binary                                                          1\n",
      "ckip_name                             ['陳男', '謝發布', '王妤昆', '曾裕仁']\n",
      "predict_name                                       ['王妤昆', '曾裕仁']\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_wwm_pretrain_tbrain/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_wwm_pretrain_tbrain/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['王妤昆', '曾裕仁']\n"
     ]
    }
   ],
   "source": [
    "####### \n",
    "test_df = pd.read_csv('./tbrain/2020-07-29.csv')\n",
    "different_ans = []\n",
    "\n",
    "for index,row in test_df.iterrows():\n",
    "    print(row)\n",
    "    ckip_name = ast.literal_eval(row['ckip_name'])\n",
    "    news = row['article']\n",
    "    print(check_pred_name_is_real_ans_split_and_avg(ckip_name,news,4))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_name_is_real_ans_split_and_avg(pred_name_list,news,dataset):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask , names):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "            self.names = names\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            name = self.names[idx]\n",
    "            return inputid , tokentype , attentionmask , name\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    def combine_sentence(sentences , max_len):\n",
    "        li = []\n",
    "        string = ''\n",
    "        for k in range(len(sentences)):\n",
    "            sentence = sentences[k]\n",
    "            if len(string) + len(sentence) < max_len:\n",
    "                string = string + sentence\n",
    "            else:\n",
    "    #             原本是空的代表sentences太常\n",
    "                if string == '':\n",
    "                    n = max_len\n",
    "                    tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                    string = tmp_li.pop(-1)\n",
    "                    li = li + tmp_li\n",
    "                else:\n",
    "                    li.append(string)\n",
    "                    string = sentence\n",
    "        if(string != ''):\n",
    "            li.append(string)\n",
    "        return li\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    \n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    testing_name = []\n",
    "    \n",
    "    \n",
    "    content = clean_string(news)\n",
    "    \n",
    "    max_length = 500\n",
    "    \n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content , max_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for name in pred_name_list:\n",
    "#             content_max_length = 512-3-len(name)\n",
    "            if len(chunk) >= max_length:\n",
    "                print('error !!!! lenth > 500')\n",
    "                continue\n",
    "            if name not in chunk:\n",
    "                continue\n",
    "\n",
    "            input_ids = tokenizer.encode(name, chunk)\n",
    "            if(len(input_ids)>512):\n",
    "                continue\n",
    "            sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(input_ids) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < 512:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            train_input_ids.append(input_ids)\n",
    "            train_token_types.append(segment_ids)\n",
    "            train_attention_mask.append(input_mask)\n",
    "            testing_name.append(name)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    testing_name = np.array(testing_name)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask, testing_name)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint  = ''\n",
    "    if dataset == 0:\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "    elif dataset == 1:\n",
    "#         dataset (1)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset1_epoch19.pkl'\n",
    "    elif dataset == 2:\n",
    "#         dataset (2)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset2_epoch11.pkl'\n",
    "    elif dataset == 3:\n",
    "#         dataset (3)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset3_epoch9.pkl'\n",
    "    elif dataset == 4:\n",
    "#         (train+test)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "    else:\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "\n",
    "    \n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors  = [t.to(device) for t in data[:-1]]\n",
    "            name = data[-1]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            \n",
    "            pred_name_list = np.array(name)\n",
    "            \n",
    "            val_dict = {}\n",
    "            count_dict = {}\n",
    "            for k in range(len(pred_name_list)):\n",
    "                name = pred_name_list[k]\n",
    "                probi = pred[k]\n",
    "                if name in val_dict:\n",
    "                    val_dict[name] += probi\n",
    "                    count_dict[name] += 1\n",
    "                else:\n",
    "                    val_dict[name] = probi\n",
    "                    count_dict[name] = 1\n",
    "                    \n",
    "            for name,count in count_dict.items():\n",
    "                val_dict[name] /= count\n",
    "            \n",
    "            keys = list(val_dict.keys())\n",
    "            values = list(val_dict.values())\n",
    "#             print(keys,values)\n",
    "            ans = []\n",
    "            th = 0.8\n",
    "            for k in range(len(keys)):  \n",
    "                if  values[k][1] > th:\n",
    "                    ans.append(keys[k])\n",
    "                \n",
    "#             pred = torch.argmax(pred,dim=-1)\n",
    "#             pred = pred.cpu().detach().numpy()\n",
    "            \n",
    "\n",
    "#             print(pred, name)\n",
    "#             return list(pred_name_list[pred>0])\n",
    "            return ans\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9858258170885463\n",
      "0.9848721430187827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"danny\"\"\"\n",
    "pre_bert_wwm_pred = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['李訓成', '蔡開宇', '王宇正'], [], ['張永泉', '郭明賓'], [], [], [], [], [], [], [], [], [], [], [], [], ['李瑞廷', '謝昌年'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['牟孝儀', '陳學敏', '牟明哲'], ['許祈文'], [], [], [], [], [], [], [], [], [], ['黃顯雄', '黃世陽'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['黃淑頻', '呂東英', '呂建安', '曾國輝'], [], [], ['賴永吉', '章啟明', '章民強', '李恆隆', '章啟光'], ['張建生', '張宜豐', '林宏彬', '陳正達'], [], [], [], [], ['傅春生'], [], ['莊錫根'], [], [], [], [], ['朱國榮', '金寶山', '葉佳瑛', '劉慶珠'], [], [], [], [], [], [], [], [], [], [], ['周麗真', '張志偉', '陳逢璿'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['張玉鳳', '林偉強', '孫幼英', '江國貴', '蘇芸樂', '鍾素娥'], [], [], [], [], [], [], ['賴俊吉'], [], [], [], [], [], [], [], [], [], [], [], [], ['繆竹怡'], [], [], [], [], [], [], [], [], [], [], [], ['黃載文', '陳偉', '陳永昌', '陳偉和', '胡志明'], [], [], [], [], [], [], [], [], [], [], ['鄭聖儒'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['林右正'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['楊文值', '楊士弘', '歐彥志', '陳俞雄', '張東耀'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['朱國榮', '林桂馨'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['張智凱'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['李全教'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['李威儀', '藍秀琪', '王桂霜'], [], [], [], [], [], [], [], ['邱世忠'], [], [], [], [], [], ['吳坤錦'], [], [], [], [], [], [], [], [], [], [], ['柯賜海'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['張桂銘', '劉威甫'], [], [], [], [], ['孔朝'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['鍾增林', '曾國財'], ['何培才'], [], [], [], ['姜維池', '郭永鴻', '葉清偉'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['林敏志'], [], [], [], [], [], ['裴振福', '劉吉雄', '楊自立', '林輝宏', '吳東明', '張建華', '呂宗南'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['王懷恭'], [], [], []]\n",
    "union_result = []\n",
    "intersect_result = []\n",
    "for i in range(len(ans)):\n",
    "  union = []\n",
    "  temp1 = set(pre_bert_wwm_pred[i])\n",
    "  temp2 = set(mypred[i])\n",
    "  union = list(temp1 | temp2)\n",
    "  intersect = list(temp1 & temp2)\n",
    "#   if (len(union) == 0):\n",
    "#     union.append('')\n",
    "#   if (len(intersect) == 0):\n",
    "#     intersect.append('')\n",
    "    \n",
    "  union_result.append(union)\n",
    "  intersect_result.append(intersect)\n",
    "print(eval_all(union_result,ans))\n",
    "print(eval_all(intersect_result,ans))\n",
    "\"\"\"-----\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?記者楊政郡／台中報導?2014年間利鑫公司推出「F.A.S.Ttm基金」（未經許可及合法設立登記），由陳思哲引介「阮涵財」或「林玉婷」（真實姓名皆不詳），以非法多層次傳銷方式吸金，達615萬美金（約新台幣1億8450萬）及166萬港幣（約新台幣747萬），台灣負責人陳思哲依違反銀行法加重罪判8年6月徒刑。判決書指出，陳思哲明知利鑫外匯公司（瑞士商）未向我國申請許可及公司設立登記，非銀行機構，竟與自稱利鑫公司顧問之「阮涵財」或「林玉婷」等人共謀，自2014年元月起，由陳思哲對外招攬不特定人參與投資，在中市、高雄市、台北市、新竹市等地，租借飯店舉辦利鑫公司投資說明會，說明會中由陳思哲介紹，「阮涵財」或「林玉婷」向與會不特定民眾解說「F.A.S.Ttm基金」投資方案及獎金種類。誆稱所收取資金，將操作外匯投資和貨幣衍生品，前景可期，參與投資會員，投資額1萬至2萬9900美元範圍，每週可固定獲利2%（稱基本配套）；投資額為3萬至9萬9900美元範圍，每週可固定獲利3%（稱無限配套）；投資額為10萬至50萬美元範圍，每週可獲利3.1%至3.5%不等（稱鑫級配套）。會員招攬下線投資，成為會員，每週可領取第1層下線週分紅30%、第2層下線週分紅20%、第3層至第10層週分紅10%與第11層至第25層週分紅5%不等獎金，以此非法多層次傳銷方式，吸引不特人投入資金。陳思哲以上述方式陸續招約20名投資者，吸收資金共615萬餘美元（折新台幣1億8450萬）及港幣166萬（折新台幣747萬）餘元，同年10月利鑫公司未再支付各投資人紅利，始知受騙。\n",
      "['阮涵財', '陳思哲', '阮涵', '林玉婷']\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_wwm_pretrain_tbrain/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_wwm_pretrain_tbrain/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.1023340225e-01, 4.8976656795e-01],\n",
      "        [1.9034050638e-03, 9.9809664488e-01],\n",
      "        [9.9968671799e-01, 3.1325456803e-04],\n",
      "        [3.0855970457e-02, 9.6914398670e-01]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['陳思哲', '林玉婷']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_728 = pd.read_csv('./tbrain/2020-07-28.csv')\n",
    "idx = 172\n",
    "news = test_728.iloc[idx]['article']\n",
    "pred_name_list = ast.literal_eval(test_728.iloc[idx]['predict_name'])\n",
    "\n",
    "print(news)\n",
    "print((pred_name_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "check_pred_name_is_real_ans(pred_name_list , news , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_name_is_real_ans(pred_name_list,news,dataset):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask , names):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "            self.names = names\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            name = self.names[idx]\n",
    "            return inputid , tokentype , attentionmask , name\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    def combine_sentence(sentences , max_len):\n",
    "        li = []\n",
    "        string = ''\n",
    "        for k in range(len(sentences)):\n",
    "            sentence = sentences[k]\n",
    "            if len(string) + len(sentence) < max_len:\n",
    "                string = string + sentence\n",
    "            else:\n",
    "    #             原本是空的代表sentences太常\n",
    "                if string == '':\n",
    "                    n = max_len\n",
    "                    tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                    string = tmp_li.pop(-1)\n",
    "                    li = li + tmp_li\n",
    "                else:\n",
    "                    li.append(string)\n",
    "                    string = sentence\n",
    "        if(string != ''):\n",
    "            li.append(string)\n",
    "        return li\n",
    "    \n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    \n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    testing_name = []\n",
    "    \n",
    "    \n",
    "    content = clean_string(news)\n",
    "    \n",
    "    max_length = 500\n",
    "    \n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content , max_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for name in pred_name_list:\n",
    "#             content_max_length = 512-3-len(name)\n",
    "            if len(chunk) >= max_length:\n",
    "                print('error !!!! lenth > 500')\n",
    "                continue\n",
    "            if name not in chunk:\n",
    "                continue\n",
    "\n",
    "            input_ids = tokenizer.encode(name, chunk)\n",
    "            if(len(input_ids)>512):\n",
    "                continue\n",
    "            sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(input_ids) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < 512:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            train_input_ids.append(input_ids)\n",
    "            train_token_types.append(segment_ids)\n",
    "            train_attention_mask.append(input_mask)\n",
    "            testing_name.append(name)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    testing_name = np.array(testing_name)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask, testing_name)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint  = ''\n",
    "    if dataset == 0:\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "    elif dataset == 1:\n",
    "#         dataset (1)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset1_epoch19.pkl'\n",
    "    elif dataset == 2:\n",
    "#         dataset (2)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset2_epoch11.pkl'\n",
    "    elif dataset == 3:\n",
    "#         dataset (3)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_dataset3_epoch9.pkl'\n",
    "    elif dataset == 4:\n",
    "#         (train+test)\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "    else:\n",
    "        checkpoint = './TB_multispan/bert_wwm_split512_ckip_name_is_ans_alldataset_epoch9.pkl'\n",
    "\n",
    "    \n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors  = [t.to(device) for t in data[:-1]]\n",
    "            name = data[-1]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(name)\n",
    "#             print(pred, name)\n",
    "            return list(pred_name_list[pred>0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_word_is_name(name):\n",
    "    hundred_name = ['趙','錢','孫','李','周','吳','鄭','王','馮','陳','褚','衛','蔣','沈','韓','楊','朱','秦','尤','許','何','呂','施','張','孔','曹','嚴','華','金','魏','陶','姜','戚','謝','鄒','章','蘇','潘','葛','范','彭','魯','韋','馬','苗','花','方','俞','任','袁','柳','鮑','史','唐','費','廉','薛','雷','賀','倪','湯','殷','羅','郝','安','于','傅','齊','康','伍','余','顧','孟','黃','蕭','尹','姚','邵','汪','毛','狄','戴','宋','龐','熊','紀','屈','項','祝','董','梁','杜','阮','藍','季','賈','江','童','顏','郭','盛','林','鍾','徐','邱','駱','高','夏','蔡','田','樊','胡','凌','霍','萬','柯','管','盧','莫','繆','解','應','丁','鄧','洪','包','石','崔','龔','程','裴','陸','甄','封','糜','焦','侯','全','甘','武','劉','詹','龍','葉','黎','白','邰','賴','卓','池','譚','溫','莊','瞿','連','習','向','古','易','廖','耿','歐','冷','簡','曾','司','歐','夏','諸','公','慕','牟']\n",
    "    return name[0] in hundred_name\n",
    "def check_first_word_list(name_list):\n",
    "    return [x for x in name_list if(check_first_word_is_name(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "def two_words_is_name(name_list):\n",
    "    from transformers import BertForSequenceClassification\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, input_dict ):\n",
    "            self.input_ids = input_dict['input_ids']\n",
    "            self.token_type_ids = input_dict['token_type_ids']\n",
    "            self.attention_mask = input_dict['attention_mask']\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            return inputid , tokentype , attentionmask\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "    print(\"device:\", device)\n",
    "\n",
    "\n",
    "    lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "\n",
    "    input_dict = tokenizer.batch_encode_plus(name_list, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=4,\n",
    "                                       truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "    BATCH_SIZE = len(name_list)\n",
    "    testset = TestDataset(input_dict)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    NUM_LABELS = 2\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    check_point = './TB_multispan/Bert_wwm_name_model_2words_all_2.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, \\\n",
    "            masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            \n",
    "            pred = torch.argmax(outputs[0],dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            name_list = np.array(name_list)\n",
    "            return list(name_list[pred>0])\n",
    "#             pred = torch.argmax(outputs[0][0] , dim = 0)\n",
    "#             return pred.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict,text):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.text = text\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        text = self.text[idx]\n",
    "\n",
    "        return inputid , tokentype , attentionmask , text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "class roberta_pos_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(roberta_pos_model, self).__init__()\n",
    "        self.start_task = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "\n",
    "        )    \n",
    "        self.end_task = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        ) \n",
    "            \n",
    "    def forward(self, start_x, end_x):\n",
    "        start_x = start_x.double()\n",
    "        end_x = end_x.double()\n",
    "         \n",
    "        start_out = self.start_task(start_x)\n",
    "        end_out = self.end_task(end_x)\n",
    "        \n",
    "        return start_out , end_out \n",
    "    \n",
    "    \n",
    "# pos_model = torch.load('./TB_multispan/start_pos_model8.pkl')  \n",
    "# torch.save(pos_model.state_dict(),'./TB_multispan/roberta_single_span_state_dict.pkl')\n",
    "# print(pos_model)\n",
    "    \n",
    "\n",
    "def roberta_single_span(news,ckip_names):\n",
    "    lm_path = './pretrain_roberta_on_TBdata/'\n",
    "    pos_model_path = './TB_multispan/roberta_single_span_state_dict.pkl'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "#     print(\"device:\", device) \n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    config = BertConfig.from_pretrained(lm_path + 'config.json',output_hidden_states=True)\n",
    "    model = BertModel.from_pretrained(lm_path,config=config)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.output_hidden_states = True\n",
    "    model.eval()\n",
    "    \n",
    "    pos_model = roberta_pos_model()\n",
    "    pos_model.load_state_dict(torch.load(pos_model_path))\n",
    "    pos_model = pos_model.double()\n",
    "    pos_model = pos_model.to(device)\n",
    "    pos_model.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    content = clean_string(news)\n",
    "\n",
    "    my_pred_name_list = []\n",
    "    \n",
    "\n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content)\n",
    "\n",
    "    test_input_dict = tokenizer.batch_encode_plus(chunks, \n",
    "                                     add_special_tokens=True,\n",
    "                                     max_length=512,\n",
    "                                    truncation=True,\n",
    "                                     return_special_tokens_mask=True,\n",
    "                                     pad_to_max_length=512,\n",
    "                                     return_tensors='pt')\n",
    "\n",
    "    BATCH_SIZE = 1\n",
    "    testset = TestDataset(test_input_dict,chunks)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "        text = data[-1]\n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "        \n",
    "\n",
    "        bert_all_768 = bert_outputs[0]\n",
    "        mini_batch = bert_all_768.size()[0]\n",
    "        bert_all_768 = bert_all_768.double()\n",
    "\n",
    "        start_pred , end_pred  = pos_model(bert_all_768, bert_all_768)\n",
    "\n",
    "            \n",
    "        start_pred = start_pred.reshape((mini_batch,512))\n",
    "        end_pred = end_pred.reshape((mini_batch,512))\n",
    "\n",
    "\n",
    "\n",
    "        topk = 10\n",
    "        myrange = 20\n",
    "        start_topk_indices = torch.topk(start_pred, topk).indices\n",
    "        end_top_k_indices = torch.topk(end_pred,topk).indices\n",
    "        all_indices = torch.cat([start_topk_indices,end_top_k_indices] , dim=-1)\n",
    "        all_indices = torch.unique(all_indices)\n",
    "\n",
    "        for i in range(all_indices.size()[0]):\n",
    "            start_index = all_indices[i]\n",
    "            start_index -= 1\n",
    "            ans_string = ''\n",
    "\n",
    "            if (start_index + myrange < 512) and (start_index - myrange > 0):\n",
    "                ans_string = text[0][start_index - myrange:start_index + myrange]\n",
    "            elif start_index + myrange > 512:\n",
    "                ans_string = text[0][start_index-myrange:-1]\n",
    "            elif start_index - myrange <= 0 :\n",
    "                ans_string = text[0][0:start_index+myrange]\n",
    "            else:\n",
    "                print('out of range')\n",
    "\n",
    "            for ckip_name in ckip_names:\n",
    "                if(ckip_name in ans_string and len(ckip_name) >= 2):\n",
    "                    my_pred_name_list.append(ckip_name)\n",
    "    \n",
    "    if len(my_pred_name_list)>0:\n",
    "        my_pred_name_list = ckip_names_filter(my_pred_name_list)\n",
    "    return my_pred_name_list\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckip_names_filter(name_list):\n",
    "    \n",
    "    lm_path = './pretrain_roberta_on_TBdata/'\n",
    "    name_model_state_path = './TB_multispan/name_model_state.pkl'\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "\n",
    "#     print(\"device:\", device) \n",
    "    \n",
    "    \n",
    "    name_model = NameModel()\n",
    "    name_model.load_state_dict(torch.load(name_model_state_path))\n",
    "    name_model = name_model.double()\n",
    "    name_model = name_model.to(device)\n",
    "    name_model.eval()\n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    test_input_dict = tokenizer.batch_encode_plus(name_list, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=5,\n",
    "                                        truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "\n",
    "    model = BertModel.from_pretrained(lm_path)\n",
    "    model = model.to(device)\n",
    "    model.output_hidden_states = True\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    bert_outputs = model(input_ids = test_input_dict['input_ids'].to(device), \n",
    "                        token_type_ids = test_input_dict['token_type_ids'].to(device), \n",
    "                        attention_mask= test_input_dict['attention_mask'].to(device))  \n",
    "\n",
    "    pool_cls = bert_outputs[1]\n",
    "    pool_cls = pool_cls\n",
    "\n",
    "\n",
    "    logits = name_model(pool_cls)\n",
    "    logits = torch.argmax(logits,dim = -1)\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    name_list = np.array(name_list)\n",
    "    \n",
    "    return list(name_list[logits>0])\n",
    "    \n",
    "\n",
    "class NameModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NameModel, self).__init__()\n",
    "        self.name_task = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(768,2)\n",
    "        )    \n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = x.double()\n",
    "        out = self.name_task(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertBinrayClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertBinrayClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "\n",
    "#             \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "def news_have_answer_bert(news):\n",
    "    binary_model_path = './TB_multispan/Bert_binary_alldataset_3.pkl'\n",
    "    lm_path = './chinese_wwm_pytorch/'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "#     print(\"device:\", device) \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    config = BertConfig.from_pretrained(lm_path + 'config.json',output_hidden_states=True)\n",
    "    model = BertModel.from_pretrained(lm_path,config=config)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(binary_model_path)\n",
    "    binary_model = BertBinrayClassifier()\n",
    "    binary_model.load_state_dict(checkpoint)\n",
    "    binary_model = binary_model.to(device)\n",
    "    binary_model.eval()\n",
    "    \n",
    "    news = clean_string(news)\n",
    "    test_input_dict = tokenizer.batch_encode_plus([news], \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=512,\n",
    "                                         return_tensors='pt')\n",
    "    BATCH_SIZE = 1\n",
    "    testset = TestDataset(test_input_dict,news)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    for data in testloader:\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "        \n",
    "        text = data[-1]\n",
    "        \n",
    "        lm_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "\n",
    "        binary_pred = binary_model(lm_outputs[1].float())\n",
    "        binary_pred = torch.argmax(binary_pred,dim = -1)\n",
    "        return binary_pred[0].item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "class XLNetBinrayClassifier(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(XLNetBinrayClassifier, self).__init__()\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sequence_summary(x)\n",
    "        x = self.classifier(x)\n",
    "        return x \n",
    "\n",
    "def news_have_answer(news):\n",
    "    binary_model_path = './TB_multispan/XLNet_binary_alldataset_fromback_3.pkl'\n",
    "    lm_path = './chinese_xlnet_mid_pytorch/'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "#     print(\"device:\", device) \n",
    "    \n",
    "    tokenizer = XLNetTokenizer.from_pretrained(lm_path)\n",
    "    config = XLNetConfig.from_pretrained(lm_path + 'config.json')\n",
    "    model = XLNetModel.from_pretrained(lm_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(binary_model_path)\n",
    "    binary_model = XLNetBinrayClassifier(config)\n",
    "    binary_model.load_state_dict(checkpoint)\n",
    "    binary_model = binary_model.to(device)\n",
    "    binary_model.eval()\n",
    "    \n",
    "    news = clean_string(news)\n",
    "    test_input_dict = tokenizer.batch_encode_plus([news], \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=512,\n",
    "                                         return_tensors='pt')\n",
    "    BATCH_SIZE = 1\n",
    "    testset = TestDataset(test_input_dict,news)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    for data in testloader:\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "        \n",
    "        text = data[-1]\n",
    "        \n",
    "        lm_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "\n",
    "        binary_pred = binary_model(lm_outputs[0].float())\n",
    "        binary_pred = torch.softmax(binary_pred,dim=-1)\n",
    "        print(binary_pred)\n",
    "        binary_pred = torch.argmax(binary_pred,dim = -1)\n",
    "        return binary_pred[0].item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "    \n",
    "class posClassfication_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(posClassfication_new, self).__init__()\n",
    "        self.start_task = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "        )    \n",
    "        self.end_task = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "        )    \n",
    "    def forward(self, start_x, end_x):\n",
    "        start_x = start_x.double()\n",
    "        end_x = end_x.double()\n",
    "        \n",
    "        start_out = self.start_task(start_x)\n",
    "        end_out = self.end_task(end_x)\n",
    "        \n",
    "        return start_out , end_out    \n",
    "    \n",
    "    \n",
    "# pos_model = torch.load('./TB_multispan/XLNet_pos_model10.pkl')\n",
    "# torch.save(pos_model.state_dict(),'./TB_multispan/XLNet_single_span_state_dict.pkl')\n",
    "# print(pos_model)\n",
    "    \n",
    "    \n",
    "def xlnet_single_span(news,ckip_names):\n",
    "    \n",
    "    lm_path = './chinese_xlnet_mid_pytorch/'\n",
    "    pos_model_path = './TB_multispan/XLNet_single_span_state_dict.pkl'\n",
    "    pos_model_path = './TB_multispan/XLNet_only_labels_single_span_statedict_32k0.pkl'\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "#     print(\"device:\", device) \n",
    "    \n",
    "    tokenizer = XLNetTokenizer.from_pretrained(lm_path)\n",
    "    config = XLNetConfig.from_pretrained(lm_path + 'config.json')\n",
    "    model = XLNetModel.from_pretrained(lm_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    pos_model = posClassfication_new()\n",
    "    pos_model.load_state_dict(torch.load(pos_model_path))\n",
    "    pos_model = pos_model.to(device)\n",
    "    pos_model = pos_model.double()\n",
    "    pos_model.eval()\n",
    "    \n",
    "    news = clean_string(news)\n",
    "    split_news = cut_sent(news)\n",
    "    chunks = combine_sentence(split_news)\n",
    "\n",
    "\n",
    "    test_input_dict = tokenizer.batch_encode_plus(chunks, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=512,\n",
    "                                         return_tensors='pt')\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    testset = TestDataset(test_input_dict,chunks)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    my_pred_name_list = []\n",
    "    for data in testloader:\n",
    "\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "        text = data[-1]\n",
    "\n",
    "        lm_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "        \n",
    "        start_pred , end_pred  = pos_model(lm_outputs[0].double(), lm_outputs[0].double())\n",
    "\n",
    "        \n",
    "        mini_batch = lm_outputs[0].size()[0]\n",
    "        start_pred = start_pred.reshape((mini_batch,512))\n",
    "        end_pred = end_pred.reshape((mini_batch,512))\n",
    "\n",
    "        topk = 10\n",
    "        myrange = 20\n",
    "        start_topk_indices = torch.topk(start_pred, topk).indices\n",
    "        end_top_k_indices = torch.topk(end_pred,topk).indices\n",
    "\n",
    "        all_indices = torch.cat([start_topk_indices,end_top_k_indices] , dim=-1)\n",
    "        all_indices = torch.unique(all_indices)\n",
    "\n",
    "        \n",
    "        for i in range(all_indices.size()[0]):\n",
    "            start_index = all_indices[i]\n",
    "            start_index -= 1\n",
    "            ans_string = ''\n",
    "\n",
    "            if (start_index + myrange < 512) and (start_index - myrange > 0):\n",
    "                ans_string = text[0][start_index - myrange:start_index + myrange]\n",
    "            elif start_index + myrange > 512:\n",
    "                ans_string = text[0][start_index-myrange:-1]\n",
    "            elif start_index - myrange <= 0 :\n",
    "                ans_string = text[0][0:start_index+myrange]\n",
    "            else:\n",
    "                print('out of range')\n",
    "\n",
    "            for ckip_name in ckip_names:\n",
    "                if(ckip_name in ans_string and len(ckip_name) >= 2):\n",
    "                    my_pred_name_list.append(ckip_name)\n",
    "\n",
    "    if len(my_pred_name_list)>0:\n",
    "        my_pred_name_list = ckip_names_filter(my_pred_name_list)\n",
    "    return my_pred_name_list\n",
    "     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbt_pretrain_combine_model(news , ckip_names):\n",
    "    # model_path\n",
    "    # binary_model_path = './combine_model5.pkl'\n",
    "    # pos_model_path = ''\n",
    "    lm_path = './pretrain_roberta_on_TBdata/'\n",
    "\n",
    "    combine_model_path = './TB_multispan/combine_model5.pkl'\n",
    "\n",
    "\n",
    "    # load model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = 'cpu'\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # for BERT and RoBERTa\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertModel.from_pretrained(lm_path)\n",
    "    model = model.to(device)\n",
    "    model.output_hidden_states = True\n",
    "    model.eval()\n",
    "\n",
    "    # binary_model = torch.load(binary_model_path)\n",
    "    # binary_model = binary_model.to(device)\n",
    "    # binary_model.eval()\n",
    "\n",
    "    # pos_model = torch.load(pos_model_path)\n",
    "    # pos_model = pos_model.to(device)\n",
    "    # pos_model.eval()\n",
    "\n",
    "    combine_model = torch.load(combine_model_path)\n",
    "    combine_model = combine_model.to(device)\n",
    "    combine_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    # process input \n",
    "\n",
    "    news = clean_string(news)\n",
    "    split_news = cut_sent(news)\n",
    "    chunks = combine_sentence(split_news)\n",
    "\n",
    "\n",
    "    test_input_dict = tokenizer.batch_encode_plus(chunks, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         truncation=True,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=512,\n",
    "                                         return_tensors='pt')\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 1\n",
    "    testset = TestDataset(test_input_dict,chunks)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    my_pred_name_list = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "\n",
    "        text = data[-1]\n",
    "\n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "\n",
    "    #     pred binary\n",
    "\n",
    "\n",
    "        pool_cls = bert_outputs[1]\n",
    "        pool_cls = pool_cls.double()\n",
    "\n",
    "\n",
    "        bert_all_768 = bert_outputs[0]\n",
    "        mini_batch = bert_all_768.size()[0]\n",
    "        bert_all_768 = bert_all_768.double()\n",
    "\n",
    "        start_pred , end_pred , binary_pred  = combine_model(bert_all_768, bert_all_768 , pool_cls)\n",
    "\n",
    "        binary_pred = torch.argmax(binary_pred,dim = -1)\n",
    "        start_pred = start_pred.reshape((mini_batch,512))\n",
    "        end_pred = end_pred.reshape((mini_batch,512))\n",
    "\n",
    "        if(binary_pred[0]<1):\n",
    "            continue\n",
    "\n",
    "\n",
    "        topk = 5\n",
    "        myrange = 18\n",
    "        start_topk_indices = torch.topk(start_pred, topk).indices\n",
    "        for i in range(topk):\n",
    "            start_index = start_topk_indices[0][i]\n",
    "\n",
    "            start_index -= 1\n",
    "            ans_string = ''\n",
    "\n",
    "            if (start_index + myrange < 512) and (start_index - myrange > 0):\n",
    "                ans_string = text[0][start_index - myrange:start_index + myrange]\n",
    "            elif start_index + myrange > 512:\n",
    "                ans_string = text[0][start_index-myrange:-1]\n",
    "            elif start_index - myrange <= 0 :\n",
    "                ans_string = text[0][0:start_index+myrange]\n",
    "            else:\n",
    "                print('out of range')\n",
    "\n",
    "            for ckip_name in ckip_names:\n",
    "                if(ckip_name in ans_string and len(ckip_name) >= 2):\n",
    "                    my_pred_name_list.append(ckip_name)\n",
    "\n",
    "    return my_pred_name_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
