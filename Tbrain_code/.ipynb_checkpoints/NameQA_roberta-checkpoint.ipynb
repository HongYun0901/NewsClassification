{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops , stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "# from transformers import RobertaForTokenClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer , BertConfig , BertModel, XLNetTokenizer ,XLNetModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertForPreTraining\n",
    "from torch import nn\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = 1\n",
    "path = './dataset/dataset' + str(dataset) + '/'\n",
    "\n",
    "train_df = pd.read_csv(path + \"tbrain_train.csv\")\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "test_df = pd.read_csv(path + \"tbrain_test.csv\")\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "\n",
    "lm_path = './pretrain_roberta_on_TBdata/'\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_string(content):\n",
    "#     cc = OpenCC('t2s')\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "#     content = cc.convert(content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences , max_len):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < max_len:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太長\n",
    "            if string == '':\n",
    "                n = max_len\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_split(df):\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    max_seq_length = 512 #xlnet no limit try try\n",
    "    train_x = [] #all train_x\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    train_y = []\n",
    "    #index必須的\n",
    "    for index , row in df.iterrows():\n",
    "        content = row['full_content']\n",
    "        content = clean_string(content)\n",
    "        content_ckip_names = ast.literal_eval(row['ckip_names'])\n",
    "        name_ans = ast.literal_eval(row['name'])\n",
    "        #no ans\n",
    "        if len(name_ans) == 0:\n",
    "            continue\n",
    "\n",
    "        for ckip_name in content_ckip_names:\n",
    "            content_max_length = 512-3-len(ckip_name)\n",
    "            if len(content) >= content_max_length:\n",
    "                split_content = cut_sent(content)\n",
    "                chunks = combine_sentence(split_content , content_max_length)\n",
    "                for chunk in chunks:\n",
    "                    if ckip_name not in chunk:\n",
    "                        continue\n",
    "                    input_ids = tokenizer.encode(ckip_name, chunk)\n",
    "                    if(len(input_ids)>512):\n",
    "                        continue\n",
    "                    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "                    num_seg_a = sep_index + 1\n",
    "                    num_seg_b = len(input_ids) - num_seg_a\n",
    "                    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    while len(input_ids) < 512:\n",
    "                        input_ids.append(0)\n",
    "                        input_mask.append(0)\n",
    "                        segment_ids.append(0)\n",
    "\n",
    "                    if ckip_name in name_ans:\n",
    "                        train_y.append(1)\n",
    "                    else:\n",
    "                        train_y.append(0)\n",
    "\n",
    "                    train_input_ids.append(input_ids)\n",
    "                    train_token_types.append(segment_ids)\n",
    "                    train_attention_mask.append(input_mask)\n",
    "                    train_x.append((ckip_name,chunk)) \n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                input_ids = tokenizer.encode(ckip_name, content)\n",
    "                if(len(input_ids)>512):\n",
    "                    continue\n",
    "                sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "                num_seg_a = sep_index + 1\n",
    "                num_seg_b = len(input_ids) - num_seg_a\n",
    "                segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "                input_mask = [1] * len(input_ids)\n",
    "\n",
    "                while len(input_ids) < 512:\n",
    "                    input_ids.append(0)\n",
    "                    input_mask.append(0)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "\n",
    "                if ckip_name in name_ans:\n",
    "                    train_y.append(1)\n",
    "                else:\n",
    "                    train_y.append(0)\n",
    "\n",
    "                train_input_ids.append(input_ids)\n",
    "                train_token_types.append(segment_ids)\n",
    "                train_attention_mask.append(input_mask)\n",
    "                train_x.append((ckip_name,content)) \n",
    "\n",
    "\n",
    "\n",
    "    #全部跑完轉np\n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    train_y = np.array(train_y)\n",
    "\n",
    "    print(len(train_x))\n",
    "    print(train_input_ids.shape)\n",
    "    print(train_token_types.shape)\n",
    "    print(train_attention_mask.shape)\n",
    "    print(train_y.shape)\n",
    "    \n",
    "    return train_x , train_input_ids ,train_token_types  , train_attention_mask , train_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208\n",
      "(3208, 512)\n",
      "(3208, 512)\n",
      "(3208, 512)\n",
      "(3208,)\n",
      "295\n",
      "(295, 512)\n",
      "(295, 512)\n",
      "(295, 512)\n",
      "(295,)\n"
     ]
    }
   ],
   "source": [
    "train_x , train_input_ids ,train_token_types  , train_attention_mask , train_y  = get_model_input_split(train_df)\n",
    "test_x , test_input_ids ,test_token_types  , test_attention_mask , test_y  = get_model_input_split(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_ids , token_type_ids , attention_mask , y ):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.y = y\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return inputid , tokentype , attentionmask, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(model,dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in dataloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors, \n",
    "                                labels=labels)\n",
    "            pred = outputs[1]\n",
    "            total += pred.size()[0]\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            correct += (pred==labels).sum().item()\n",
    "\n",
    "    return correct/total \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 5.96 GiB already allocated; 11.56 MiB free; 6.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f8c9cfb8254e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 5.96 GiB already allocated; 11.56 MiB free; 6.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "trainset = TrainDataset(train_input_ids ,train_token_types , train_attention_mask ,train_y)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE , shuffle=True)\n",
    "\n",
    "testset = TrainDataset(test_input_ids ,test_token_types , test_attention_mask ,test_y)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE , shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "NUM_LABELS = 2\n",
    "tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "model  = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors, \n",
    "                            labels=labels)\n",
    "        loss = outputs[0]\n",
    "        pred = outputs[1]\n",
    "        total += pred.size()[0]\n",
    "        pred = torch.argmax(pred,dim=-1)\n",
    "        correct += (pred==labels).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    checkpoint_path = './QAModel/' + str(dataset) + '/' \n",
    "        \n",
    "    torch.save(model.state_dict(),checkpoint_path + 'roberta_QA_split' + str(epoch) + '.pkl')\n",
    "    print('[epoch %d] loss: %.3f' %(epoch + 1, running_loss))\n",
    "    print('train_acc:' ,correct/total)\n",
    "    print('test_acc:' , get_eval(model,testloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "NUM_LABELS =2\n",
    "checkpoint = './TB_multispan/xlnet_ckip_name_is_ans_alldataset_epoch12.pkl'\n",
    "model  = XLNetForSequenceClassification.from_pretrained('./chinese_xlnet_mid_pytorch/',num_labels=NUM_LABELS)\n",
    "model.load_state_dict(torch.load('./TB_multispan/xlnet_ckip_name_is_ans_alldataset_epoch12.pkl'))\n",
    "model.eval()\n",
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>binary</th>\n",
       "      <th>ckip_name</th>\n",
       "      <th>predict_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>檢調偵辦「三鑫集團」以投資俄羅斯賭場等名目吸金12億元案，發現三鑫集團負責人曾裕仁去年因債務...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳男', '謝發布', '王妤昆', '曾裕仁']</td>\n",
       "      <td>['王妤昆', '曾裕仁']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>〔記者王善嬿／嘉市報導〕嘉義地檢署查獲台商王子南、羅則嬌等7人組集團進行兩岸地下匯兌，1年多...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王善嬿', '羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '鐘辛禾', '田某...</td>\n",
       "      <td>['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>中華心蓮慈善助學協會理事長童行成(68歲)於4年前起，以協會名義推出「幼兒教育撲滿計畫」等集...</td>\n",
       "      <td>1</td>\n",
       "      <td>['童行成', '鄧玉瑩']</td>\n",
       "      <td>['童行成']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>漢唐光電科技董事長徐建志曾和和皮膚科名醫蔡佳雅發生不倫戀，他以不實財報佯稱漢唐公司將上市，販...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳宏陽', '蔡佳雅', '徐雲其', '徐建志', '徐雲', '蔡夫']</td>\n",
       "      <td>['蔡佳雅', '徐雲其', '徐建志']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>檢調接獲報案，指位於台北市的台灣搜房公司2014年5月至12月間推出英國不動產投資方案，以保...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黃穎溪', '呂威東', '游仁汶', '吳珮', '高崇信', '彭傑', '高男']</td>\n",
       "      <td>['高崇信', '呂威東', '黃穎溪']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>國民黨宜蘭縣長參選人、羅東鎮長廖泓花遭控動用公所第二預備金補助其他鄉鎮活動涉嫌違反《預算法》...</td>\n",
       "      <td>1</td>\n",
       "      <td>['羅東陳書豪', '涂文賢', '張鴻人', '廖泓花', '韓彥宏', '施仁旭']</td>\n",
       "      <td>['廖泓花', '韓彥宏']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>〔記者葉俞揚／新北報導〕怡婷科技公司正妹負責人曾怡婷以投資蘋果、三星等手機買賣，宣稱最高年報...</td>\n",
       "      <td>1</td>\n",
       "      <td>['李文行', '怡婷', '張雅苓', '楊靖雯', '陳陽孜', '陳弘俊', '葉俞揚...</td>\n",
       "      <td>['曾雅琪', '陳弘俊', '曾怡婷', '楊靖雯']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>刑事局中部打擊犯罪中心破獲以林姓男子為首的「假轉帳、真詐騙」犯罪集團，專在網路佯裝買家，向賣...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳男', '林昶杰', '王煌忠']</td>\n",
       "      <td>['林昶杰']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>〔記者錢利忠／台北報導〕綽號「貢丸」的竹聯幫地隆會會長陳威富，涉嫌勾結運將江家瑋、林雲勳、梁...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳威富', '江家瑋', '貢丸', '林雲勳', '錢利忠', '陳嫌', '梁姵寶']</td>\n",
       "      <td>['陳威富', '林雲勳', '梁姵寶', '江家瑋']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>屏東縣前車城鄉長林俊豪於在2010年任職鄉長後利用招標機會收取不法回扣，經去年屏東地院重判有...</td>\n",
       "      <td>1</td>\n",
       "      <td>['賴筱涵', '李建冰', '賴家賢', '吳宛財', '林俊', '黃進郁', '楊白萱...</td>\n",
       "      <td>['李建冰', '吳宛財', '黃進郁', '林俊豪', '蕭信宏', '蔡佳雅', '王品...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>〔財經頻道／綜合報導〕2013年爆發的台苯掏空弊案，至今尚在漫長的司法訴訟之中，台苯大股東何...</td>\n",
       "      <td>1</td>\n",
       "      <td>['蘇一為', '吳永，', '陳敏盛', '吳恆', '張鈞傑', '吳重賢', '劉結依...</td>\n",
       "      <td>['張鈞傑', '陳敏盛', '蘇一為']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>【王惠文╱高雄報導】曾任「花媽」陳夢羽機要秘書的高市府前專委趙竹珍，因承辦世運行銷招標案收受...</td>\n",
       "      <td>1</td>\n",
       "      <td>['趙竹珍', '王惠文', '陳夢羽']</td>\n",
       "      <td>['陳夢羽', '趙竹珍']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>〔記者陳政甫／新北報導〕股市炒手許志成結合佳總興業等公司人士，炒作佳總、佶優、上櫃公司萬潤科...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黎家慧', '許志成', '陳政甫']</td>\n",
       "      <td>['黎家慧', '許志成']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>曾是天道盟太陽會會長「鐵霸」小弟兼貼身司機的太陽會大哥林政汝，二年多前因染上毒癮被鐵霸掃地出...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林政汝', '鐵霸']</td>\n",
       "      <td>['林政汝']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>〔記者張誠瑜／台中報導〕男子林燕樺、陳容佳於2016年加入詐欺集團當車手，提款27次共18萬...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林燕樺', '陳容佳', '張誠瑜', '陳男', '林男']</td>\n",
       "      <td>['林燕樺', '陳容佳']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>因巴拉圭東方市的「東方工業區」開發案控告前外交部長林宗俊誹謗的台商羅常軍，13日下午到台北地...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳正平', '林宗俊', '羅常軍', ' 羅常軍', '巴拉圭良']</td>\n",
       "      <td>['陳正平', '林宗俊', '羅常軍']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>環球不動產估價師事務負責人陳致雲，明明只有代書資格未取得估價師證照，9年前卻與台北地院民事執...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳致雲', '林茂娥', '丁漢源', '圖利罪判林男', '蔡男', '陳男', '林男']</td>\n",
       "      <td>['陳致雲', '林茂娥']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>〔記者錢利忠、王冠仁／台北報導〕今年6月才獲頒模範公務員獎的移民署長楊敬以，被《鏡週刊》踢爆...</td>\n",
       "      <td>1</td>\n",
       "      <td>['楊敬', '楊敬以', '王冠仁', '鮑秀娟', '黃雅惠', '錢利忠']</td>\n",
       "      <td>['楊敬以']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>〔記者王捷／台南報導〕台灣汽電共生股份有限公司台南廠，傳出有2名老員工疑似設人頭公司向下游廠...</td>\n",
       "      <td>1</td>\n",
       "      <td>['蕭吉峰', '林男', '顏佑霖', '劉男', '王捷']</td>\n",
       "      <td>['顏佑霖', '蕭吉峰']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>女子何佳華被控在臉書及LINE成立「高雄嬰兒用品買賣」、「花花柑仔店」等網購群組販賣奶粉、尿...</td>\n",
       "      <td>1</td>\n",
       "      <td>['何佳華', '何女', '王怡如']</td>\n",
       "      <td>['何佳華']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>〔記者黃建緯／新北報導〕選前全國大掃黑，新北市海山、三峽警分局分別瓦解基隆饒志銘、桃園楊其典...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林子軒', '饒男', '東東', '楊其典', '沈秋正', '饒志銘', '黃建緯'...</td>\n",
       "      <td>['楊其典']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>〔記者蔡彰盛／新竹報導〕扯！新竹地區兄弟檔陳月修、陳陽孜與男子郭菁鴻共3名漁船船長，被控將漁...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳陽孜', '蔡彰盛', '郭菁鴻', '陳月修']</td>\n",
       "      <td>['陳陽孜', '郭菁鴻', '陳月修']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>「掃黑行動專案」全面啟動，台北市刑大破獲竹聯幫「月堂」堂主吳德康的不法暴力集團，警方日前接獲...</td>\n",
       "      <td>1</td>\n",
       "      <td>['曾舜泰', '吳德康', '太子宗', '月堂', '黃怡文']</td>\n",
       "      <td>['曾舜泰', '吳德康']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>〔記者王淑山／新北報導〕不動產土地開發公司「貽信開發建設」，涉嫌以投資國內法拍屋及越南不動產...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王淑山', '張女', '吳致遠', '吳男', '張元恬']</td>\n",
       "      <td>['吳致遠', '張元恬']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>〔記者楊國文／台北報導〕國民黨籍宜蘭縣議員王承光被控以每票1000元代價向選民買票，被檢方依...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王承光', '王停權', '楊國文']</td>\n",
       "      <td>['王承光', '王停權']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>〔社會新聞中心／綜合報導〕法務部調查局北部地區機動工作站掌握情資，有一艘中國籍漁船從泰國金三...</td>\n",
       "      <td>1</td>\n",
       "      <td>['張明儒', '翁家佑', '簡志枝']</td>\n",
       "      <td>['張明儒', '簡志枝']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>〔記者黃易揚／綜合報導〕嘉義地檢署日前起訴史上最大地下匯兌案的經營主嫌邱富志夫婦等多人，查出...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黃易揚', '邱男', '蔡宗穎', '邱富志', '梁男', '林韋志', '陳育菱'...</td>\n",
       "      <td>['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>嘉義縣警局朴子分局外事組警員吳典歌，與同仁逮捕在KTV坐檯陪酒的2名女逃逸移工時，媒介女移工...</td>\n",
       "      <td>1</td>\n",
       "      <td>['吳員', '阿蒂', '蘇菲雅', '吳員知法', '吳典歌', '容留女']</td>\n",
       "      <td>['吳典歌']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>吸金136億元、棄保潛逃的非法吸金集團主嫌邱家良今天持偽造假護照入境泰國時被查獲，雖然他否認...</td>\n",
       "      <td>1</td>\n",
       "      <td>['邱男', '林俊安', '邱家良']</td>\n",
       "      <td>['邱家良']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>空軍第二戰術戰鬥機隊憲兵二中隊中隊長洪國源，明知部隊核發個人獎金不得擅自挪用，竟任意勾選人頭...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林女', '洪男', '洪國源', '倪男']</td>\n",
       "      <td>['洪國源']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>〔記者彭方賢／台北報導〕嘉義縣水上國中前校長陳正平，5年前任內辦理學校風雨操場鐵皮屋拆除業務...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳正平', '彭方賢']</td>\n",
       "      <td>['陳正平']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>〔記者陳政甫／新竹報導〕國民黨新竹縣橫山鄉長吳怡易涉嫌在縣議員任內，以鄰居作為人頭，詐領助理...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳政甫', '邱男', '錢英杰', '黃男', '蕭男', '清潔隊', '吳怡易',...</td>\n",
       "      <td>['吳怡易']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>電子支付近年日益普及，國內有不法傳銷業者看準這股風潮，推出名為「Q點支付」的傳銷方案吸金，聲...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Jeremy Millar', '駱芷瑋', '吉米', '邱富志', '連庭亨', '...</td>\n",
       "      <td>['連庭亨', '毛維倫']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article  binary  \\\n",
       "0    檢調偵辦「三鑫集團」以投資俄羅斯賭場等名目吸金12億元案，發現三鑫集團負責人曾裕仁去年因債務...       1   \n",
       "22   〔記者王善嬿／嘉市報導〕嘉義地檢署查獲台商王子南、羅則嬌等7人組集團進行兩岸地下匯兌，1年多...       1   \n",
       "42   中華心蓮慈善助學協會理事長童行成(68歲)於4年前起，以協會名義推出「幼兒教育撲滿計畫」等集...       1   \n",
       "48   漢唐光電科技董事長徐建志曾和和皮膚科名醫蔡佳雅發生不倫戀，他以不實財報佯稱漢唐公司將上市，販...       1   \n",
       "89   檢調接獲報案，指位於台北市的台灣搜房公司2014年5月至12月間推出英國不動產投資方案，以保...       1   \n",
       "125  國民黨宜蘭縣長參選人、羅東鎮長廖泓花遭控動用公所第二預備金補助其他鄉鎮活動涉嫌違反《預算法》...       1   \n",
       "131  〔記者葉俞揚／新北報導〕怡婷科技公司正妹負責人曾怡婷以投資蘋果、三星等手機買賣，宣稱最高年報...       1   \n",
       "135  刑事局中部打擊犯罪中心破獲以林姓男子為首的「假轉帳、真詐騙」犯罪集團，專在網路佯裝買家，向賣...       1   \n",
       "139  〔記者錢利忠／台北報導〕綽號「貢丸」的竹聯幫地隆會會長陳威富，涉嫌勾結運將江家瑋、林雲勳、梁...       1   \n",
       "146  屏東縣前車城鄉長林俊豪於在2010年任職鄉長後利用招標機會收取不法回扣，經去年屏東地院重判有...       1   \n",
       "151  〔財經頻道／綜合報導〕2013年爆發的台苯掏空弊案，至今尚在漫長的司法訴訟之中，台苯大股東何...       1   \n",
       "159  【王惠文╱高雄報導】曾任「花媽」陳夢羽機要秘書的高市府前專委趙竹珍，因承辦世運行銷招標案收受...       1   \n",
       "170  〔記者陳政甫／新北報導〕股市炒手許志成結合佳總興業等公司人士，炒作佳總、佶優、上櫃公司萬潤科...       1   \n",
       "194  曾是天道盟太陽會會長「鐵霸」小弟兼貼身司機的太陽會大哥林政汝，二年多前因染上毒癮被鐵霸掃地出...       1   \n",
       "201  〔記者張誠瑜／台中報導〕男子林燕樺、陳容佳於2016年加入詐欺集團當車手，提款27次共18萬...       1   \n",
       "218  因巴拉圭東方市的「東方工業區」開發案控告前外交部長林宗俊誹謗的台商羅常軍，13日下午到台北地...       1   \n",
       "225  環球不動產估價師事務負責人陳致雲，明明只有代書資格未取得估價師證照，9年前卻與台北地院民事執...       1   \n",
       "247  〔記者錢利忠、王冠仁／台北報導〕今年6月才獲頒模範公務員獎的移民署長楊敬以，被《鏡週刊》踢爆...       1   \n",
       "260  〔記者王捷／台南報導〕台灣汽電共生股份有限公司台南廠，傳出有2名老員工疑似設人頭公司向下游廠...       1   \n",
       "275  女子何佳華被控在臉書及LINE成立「高雄嬰兒用品買賣」、「花花柑仔店」等網購群組販賣奶粉、尿...       1   \n",
       "282  〔記者黃建緯／新北報導〕選前全國大掃黑，新北市海山、三峽警分局分別瓦解基隆饒志銘、桃園楊其典...       1   \n",
       "297  〔記者蔡彰盛／新竹報導〕扯！新竹地區兄弟檔陳月修、陳陽孜與男子郭菁鴻共3名漁船船長，被控將漁...       1   \n",
       "311  「掃黑行動專案」全面啟動，台北市刑大破獲竹聯幫「月堂」堂主吳德康的不法暴力集團，警方日前接獲...       1   \n",
       "312  〔記者王淑山／新北報導〕不動產土地開發公司「貽信開發建設」，涉嫌以投資國內法拍屋及越南不動產...       1   \n",
       "320  〔記者楊國文／台北報導〕國民黨籍宜蘭縣議員王承光被控以每票1000元代價向選民買票，被檢方依...       1   \n",
       "326  〔社會新聞中心／綜合報導〕法務部調查局北部地區機動工作站掌握情資，有一艘中國籍漁船從泰國金三...       1   \n",
       "327  〔記者黃易揚／綜合報導〕嘉義地檢署日前起訴史上最大地下匯兌案的經營主嫌邱富志夫婦等多人，查出...       1   \n",
       "329  嘉義縣警局朴子分局外事組警員吳典歌，與同仁逮捕在KTV坐檯陪酒的2名女逃逸移工時，媒介女移工...       1   \n",
       "331  吸金136億元、棄保潛逃的非法吸金集團主嫌邱家良今天持偽造假護照入境泰國時被查獲，雖然他否認...       1   \n",
       "342  空軍第二戰術戰鬥機隊憲兵二中隊中隊長洪國源，明知部隊核發個人獎金不得擅自挪用，竟任意勾選人頭...       1   \n",
       "343  〔記者彭方賢／台北報導〕嘉義縣水上國中前校長陳正平，5年前任內辦理學校風雨操場鐵皮屋拆除業務...       1   \n",
       "357  〔記者陳政甫／新竹報導〕國民黨新竹縣橫山鄉長吳怡易涉嫌在縣議員任內，以鄰居作為人頭，詐領助理...       1   \n",
       "369  電子支付近年日益普及，國內有不法傳銷業者看準這股風潮，推出名為「Q點支付」的傳銷方案吸金，聲...       1   \n",
       "\n",
       "                                             ckip_name  \\\n",
       "0                          ['陳男', '謝發布', '王妤昆', '曾裕仁']   \n",
       "22   ['王善嬿', '羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '鐘辛禾', '田某...   \n",
       "42                                      ['童行成', '鄧玉瑩']   \n",
       "48            ['陳宏陽', '蔡佳雅', '徐雲其', '徐建志', '徐雲', '蔡夫']   \n",
       "89      ['黃穎溪', '呂威東', '游仁汶', '吳珮', '高崇信', '彭傑', '高男']   \n",
       "125       ['羅東陳書豪', '涂文賢', '張鴻人', '廖泓花', '韓彥宏', '施仁旭']   \n",
       "131  ['李文行', '怡婷', '張雅苓', '楊靖雯', '陳陽孜', '陳弘俊', '葉俞揚...   \n",
       "135                               ['陳男', '林昶杰', '王煌忠']   \n",
       "139    ['陳威富', '江家瑋', '貢丸', '林雲勳', '錢利忠', '陳嫌', '梁姵寶']   \n",
       "146  ['賴筱涵', '李建冰', '賴家賢', '吳宛財', '林俊', '黃進郁', '楊白萱...   \n",
       "151  ['蘇一為', '吳永，', '陳敏盛', '吳恆', '張鈞傑', '吳重賢', '劉結依...   \n",
       "159                              ['趙竹珍', '王惠文', '陳夢羽']   \n",
       "170                              ['黎家慧', '許志成', '陳政甫']   \n",
       "194                                      ['林政汝', '鐵霸']   \n",
       "201                  ['林燕樺', '陳容佳', '張誠瑜', '陳男', '林男']   \n",
       "218              ['陳正平', '林宗俊', '羅常軍', ' 羅常軍', '巴拉圭良']   \n",
       "225  ['陳致雲', '林茂娥', '丁漢源', '圖利罪判林男', '蔡男', '陳男', '林男']   \n",
       "247          ['楊敬', '楊敬以', '王冠仁', '鮑秀娟', '黃雅惠', '錢利忠']   \n",
       "260                   ['蕭吉峰', '林男', '顏佑霖', '劉男', '王捷']   \n",
       "275                               ['何佳華', '何女', '王怡如']   \n",
       "282  ['林子軒', '饒男', '東東', '楊其典', '沈秋正', '饒志銘', '黃建緯'...   \n",
       "297                       ['陳陽孜', '蔡彰盛', '郭菁鴻', '陳月修']   \n",
       "311                 ['曾舜泰', '吳德康', '太子宗', '月堂', '黃怡文']   \n",
       "312                  ['王淑山', '張女', '吳致遠', '吳男', '張元恬']   \n",
       "320                              ['王承光', '王停權', '楊國文']   \n",
       "326                              ['張明儒', '翁家佑', '簡志枝']   \n",
       "327  ['黃易揚', '邱男', '蔡宗穎', '邱富志', '梁男', '林韋志', '陳育菱'...   \n",
       "329          ['吳員', '阿蒂', '蘇菲雅', '吳員知法', '吳典歌', '容留女']   \n",
       "331                               ['邱男', '林俊安', '邱家良']   \n",
       "342                          ['林女', '洪男', '洪國源', '倪男']   \n",
       "343                                     ['陳正平', '彭方賢']   \n",
       "357  ['陳政甫', '邱男', '錢英杰', '黃男', '蕭男', '清潔隊', '吳怡易',...   \n",
       "369  ['Jeremy Millar', '駱芷瑋', '吉米', '邱富志', '連庭亨', '...   \n",
       "\n",
       "                                          predict_name  \n",
       "0                                       ['王妤昆', '曾裕仁']  \n",
       "22   ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南']  \n",
       "42                                             ['童行成']  \n",
       "48                               ['蔡佳雅', '徐雲其', '徐建志']  \n",
       "89                               ['高崇信', '呂威東', '黃穎溪']  \n",
       "125                                     ['廖泓花', '韓彥宏']  \n",
       "131                       ['曾雅琪', '陳弘俊', '曾怡婷', '楊靖雯']  \n",
       "135                                            ['林昶杰']  \n",
       "139                       ['陳威富', '林雲勳', '梁姵寶', '江家瑋']  \n",
       "146  ['李建冰', '吳宛財', '黃進郁', '林俊豪', '蕭信宏', '蔡佳雅', '王品...  \n",
       "151                              ['張鈞傑', '陳敏盛', '蘇一為']  \n",
       "159                                     ['陳夢羽', '趙竹珍']  \n",
       "170                                     ['黎家慧', '許志成']  \n",
       "194                                            ['林政汝']  \n",
       "201                                     ['林燕樺', '陳容佳']  \n",
       "218                              ['陳正平', '林宗俊', '羅常軍']  \n",
       "225                                     ['陳致雲', '林茂娥']  \n",
       "247                                            ['楊敬以']  \n",
       "260                                     ['顏佑霖', '蕭吉峰']  \n",
       "275                                            ['何佳華']  \n",
       "282                                            ['楊其典']  \n",
       "297                              ['陳陽孜', '郭菁鴻', '陳月修']  \n",
       "311                                     ['曾舜泰', '吳德康']  \n",
       "312                                     ['吳致遠', '張元恬']  \n",
       "320                                     ['王承光', '王停權']  \n",
       "326                                     ['張明儒', '簡志枝']  \n",
       "327                       ['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']  \n",
       "329                                            ['吳典歌']  \n",
       "331                                            ['邱家良']  \n",
       "342                                            ['洪國源']  \n",
       "343                                            ['陳正平']  \n",
       "357                                            ['吳怡易']  \n",
       "369                                     ['連庭亨', '毛維倫']  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tbrain/2020-07-29.csv')\n",
    "\n",
    "correct =data[data['binary']==1]\n",
    "\n",
    "correct\n",
    "# correct = np.array(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['王妤昆', '曾裕仁']\n",
      "ans :  ['王妤昆', '曾裕仁']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南']\n",
      "ans :  ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南', '鐘辛禾']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['童行成']\n",
      "ans :  ['童行成']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳宏陽', '蔡佳雅', '徐雲其', '徐建志']\n",
      "ans :  ['徐建志']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['黃穎溪', '高崇信']\n",
      "ans :  ['高崇信', '呂威東', '黃穎溪', '彭傑']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['廖泓花']\n",
      "ans :  ['廖泓花']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['張雅苓', '楊靖雯', '曾怡婷', '曾雅琪']\n",
      "ans :  ['曾雅琪', '曾怡婷']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林昶杰', '王煌忠']\n",
      "ans :  ['林昶杰']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳威富', '江家瑋', '林雲勳', '梁姵寶']\n",
      "ans :  ['陳威富', '林雲勳', '梁姵寶', '江家瑋']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['賴筱涵', '賴家賢', '吳宛財', '黃進郁', '林俊豪', '蔡佳雅', '陳翠銘', '黃志茂']\n",
      "ans :  ['吳宛財', '林俊豪', '蔡佳雅']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蘇一為', '陳敏盛', '張鈞傑', '吳重賢', '劉結依', '何山']\n",
      "ans :  ['張鈞傑', '陳敏盛', '蘇一為', '劉結依', '何山']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['趙竹珍']\n",
      "ans :  ['趙竹珍']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['黎家慧', '許志成']\n",
      "ans :  ['黎家慧', '許志成']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林政汝']\n",
      "ans :  ['林政汝']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林燕樺', '陳容佳']\n",
      "ans :  ['林燕樺', '陳容佳']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林宗俊', '羅常軍', ' 羅常軍']\n",
      "ans :  ['陳正平', '林宗俊', '羅常軍']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳致雲', '林茂娥']\n",
      "ans :  ['陳致雲', '林茂娥']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['楊敬以', '黃雅惠']\n",
      "ans :  ['楊敬以']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蕭吉峰', '顏佑霖']\n",
      "ans :  ['顏佑霖', '蕭吉峰']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['何佳華', '王怡如']\n",
      "ans :  ['何佳華']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林子軒', '楊其典', '沈秋正', '饒志銘']\n",
      "ans :  ['楊其典', '饒志銘', '林子軒']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳陽孜', '郭菁鴻', '陳月修']\n",
      "ans :  ['陳陽孜', '郭菁鴻', '陳月修']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['曾舜泰', '吳德康']\n",
      "ans :  ['曾舜泰', '吳德康']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['吳致遠', '張元恬']\n",
      "ans :  ['吳致遠', '張元恬']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['王承光']\n",
      "ans :  ['王承光']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['張明儒', '翁家佑', '簡志枝']\n",
      "ans :  ['張明儒', '簡志枝', '翁家佑']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蔡宗穎', '邱富志', '林韋志', '陳育菱', '梁駿潔']\n",
      "ans :  ['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['阿蒂', '蘇菲雅', '吳典歌']\n",
      "ans :  ['吳典歌']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['邱家良']\n",
      "ans :  ['邱家良']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['洪國源']\n",
      "ans :  ['洪國源']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳正平']\n",
      "ans :  ['陳正平']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['吳怡易']\n",
      "ans :  ['吳怡易']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['邱富志', '連庭亨']\n",
      "ans :  ['連庭亨']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8697144606235516"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tbrain/2020-07-29_test.csv')\n",
    "\n",
    "test_df =data[data['binary']==1]\n",
    "# test_df = pd.read_csv(\"./dataset/tbrain_test (2).csv\")\n",
    "# test_df = test_df.fillna('None')\n",
    "\n",
    "names =  test_df['predict_name'].tolist()\n",
    "contents = np.array(test_df['article'].tolist())\n",
    "ckip_names = test_df['ckip_name'].tolist()\n",
    "\n",
    "valid_count = 0\n",
    "score = 0.0\n",
    "\n",
    "pred_list = []\n",
    "ans_list = []\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "    name_list = ast.literal_eval(names[i])\n",
    "    \n",
    "    \n",
    "    if len(name_list) == 0:\n",
    "        continue\n",
    "        \n",
    "    pred =  check_pred_name_is_real_ans(content_ckip_names,content)\n",
    "    \n",
    "    print('---------------')\n",
    "    print('pred : ' , pred)\n",
    "    print('ans : ', name_list)\n",
    "        \n",
    "        \n",
    "    pred_list.append(pred)\n",
    "    ans_list.append(name_list)\n",
    "    \n",
    "eval_all(pred_list,ans_list)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./tbrain/2020-07-29.csv does not exist: './tbrain/2020-07-29.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c79f9c94e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_728\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tbrain/2020-07-29.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./tbrain/2020-07-29.csv does not exist: './tbrain/2020-07-29.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_728 = pd.read_csv('./tbrain/2020-07-29.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>ckip_name</th>\n",
       "      <th>predict_name</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.12.31 02:55【頭家開講】小螺絲入口鑽出金鴻君集團董事長李碧青文｜張毓迪攝...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0台股台股盤勢【華冠投顧】台股繼續彈漲+33點!明天結算怎麼看?!華冠投顧※來源：華冠投顧2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.11.29 08:57【Ciao!義大利靈魂飲5】果渣白蘭地一點都不渣濃郁果香是釀...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>??【台北訊】單身經濟在台灣越加紅火。便利店常常能見到一人份的水果，餐館也會有單人份的火鍋、...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0台股台股盤勢【新光證券】盤前分析新光證券※來源：新光證券2019/07/31 08:34f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>A股港股大陸政經中國1年期及5年期以上LPR均降5基點超出市場預期鉅亨網編輯林思婷2019/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1.鴻海(2317)：三大法人持續買超，Q3獲利飆增8成、董座喊毛利率要拚10%，早盤股價開...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>台積電公告7月營收847.58億元，月減1.3％、年增14％，雖較6月微幅衰退，仍是今年單月...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>71歲英國退休工程師李佳城 因為開車超速收到100英鎊（約台幣3,850元）的罰單，感到不服...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>〔記者黃捷／台北報導〕香港2014年初出現一家「MYCOIN」比特幣投資公司，短短1年就吸金...</td>\n",
       "      <td>['王均昇', '黃捷', '陳良博', '王子南', '陳亞辛']</td>\n",
       "      <td>['王均昇', '陳亞辛', '王子南', '陳良博']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article  \\\n",
       "0    2019.12.31 02:55【頭家開講】小螺絲入口鑽出金鴻君集團董事長李碧青文｜張毓迪攝...   \n",
       "1    0台股台股盤勢【華冠投顧】台股繼續彈漲+33點!明天結算怎麼看?!華冠投顧※來源：華冠投顧2...   \n",
       "2    2019.11.29 08:57【Ciao!義大利靈魂飲5】果渣白蘭地一點都不渣濃郁果香是釀...   \n",
       "3    ??【台北訊】單身經濟在台灣越加紅火。便利店常常能見到一人份的水果，餐館也會有單人份的火鍋、...   \n",
       "4    0台股台股盤勢【新光證券】盤前分析新光證券※來源：新光證券2019/07/31 08:34f...   \n",
       "..                                                 ...   \n",
       "370  A股港股大陸政經中國1年期及5年期以上LPR均降5基點超出市場預期鉅亨網編輯林思婷2019/...   \n",
       "371  1.鴻海(2317)：三大法人持續買超，Q3獲利飆增8成、董座喊毛利率要拚10%，早盤股價開...   \n",
       "372  台積電公告7月營收847.58億元，月減1.3％、年增14％，雖較6月微幅衰退，仍是今年單月...   \n",
       "373  71歲英國退休工程師李佳城 因為開車超速收到100英鎊（約台幣3,850元）的罰單，感到不服...   \n",
       "374  〔記者黃捷／台北報導〕香港2014年初出現一家「MYCOIN」比特幣投資公司，短短1年就吸金...   \n",
       "\n",
       "                              ckip_name                  predict_name  binary  \n",
       "0                                    []                            []       0  \n",
       "1                                    []                            []       0  \n",
       "2                                    []                            []       0  \n",
       "3                                    []                            []       0  \n",
       "4                                    []                            []       0  \n",
       "..                                  ...                           ...     ...  \n",
       "370                                  []                            []       0  \n",
       "371                                  []                            []       0  \n",
       "372                                  []                            []       0  \n",
       "373                                  []                            []       0  \n",
       "374  ['王均昇', '黃捷', '陳良博', '王子南', '陳亞辛']  ['王均昇', '陳亞辛', '王子南', '陳良博']       1  \n",
       "\n",
       "[375 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?記者楊政郡／台中報導?2014年間利鑫公司推出「F.A.S.Ttm基金」（未經許可及合法設立登記），由陳思哲引介「阮涵財」或「林玉婷」（真實姓名皆不詳），以非法多層次傳銷方式吸金，達615萬美金（約新台幣1億8450萬）及166萬港幣（約新台幣747萬），台灣負責人陳思哲依違反銀行法加重罪判8年6月徒刑。判決書指出，陳思哲明知利鑫外匯公司（瑞士商）未向我國申請許可及公司設立登記，非銀行機構，竟與自稱利鑫公司顧問之「阮涵財」或「林玉婷」等人共謀，自2014年元月起，由陳思哲對外招攬不特定人參與投資，在中市、高雄市、台北市、新竹市等地，租借飯店舉辦利鑫公司投資說明會，說明會中由陳思哲介紹，「阮涵財」或「林玉婷」向與會不特定民眾解說「F.A.S.Ttm基金」投資方案及獎金種類。誆稱所收取資金，將操作外匯投資和貨幣衍生品，前景可期，參與投資會員，投資額1萬至2萬9900美元範圍，每週可固定獲利2%（稱基本配套）；投資額為3萬至9萬9900美元範圍，每週可固定獲利3%（稱無限配套）；投資額為10萬至50萬美元範圍，每週可獲利3.1%至3.5%不等（稱鑫級配套）。會員招攬下線投資，成為會員，每週可領取第1層下線週分紅30%、第2層下線週分紅20%、第3層至第10層週分紅10%與第11層至第25層週分紅5%不等獎金，以此非法多層次傳銷方式，吸引不特人投入資金。陳思哲以上述方式陸續招約20名投資者，吸收資金共615萬餘美元（折新台幣1億8450萬）及港幣166萬（折新台幣747萬）餘元，同年10月利鑫公司未再支付各投資人紅利，始知受騙。\n",
      "['阮涵財', '陳思哲', '阮涵', '林玉婷', '陳男']\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_wwm_pretrain_tbrain/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_wwm_pretrain_tbrain/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.2630005181e-02, 9.3736994267e-01],\n",
      "        [1.7002505483e-03, 9.9829977751e-01],\n",
      "        [9.9968278408e-01, 3.1728003523e-04],\n",
      "        [3.5155680776e-01, 6.4844316244e-01],\n",
      "        [9.9966692924e-01, 3.3309168066e-04]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['阮涵財', '陳思哲', '林玉婷']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_728 = pd.read_csv('./tbrain/2020-07-28.csv')\n",
    "idx = 172\n",
    "news = test_728.iloc[idx]['article']\n",
    "pred_name_list = ast.literal_eval(test_728.iloc[idx]['predict_name'])\n",
    "\n",
    "print(news)\n",
    "pred_name_list.append('陳男')\n",
    "print((pred_name_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "check_pred_name_is_real_ans(pred_name_list , news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_name_is_real_ans(pred_name_list,news):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    tokenizer = XLNetTokenizer.from_pretrained('./chinese_xlnet_mid_pytorch/')\n",
    "\n",
    "    content = clean_string(news)\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "        \n",
    "    for name in pred_name_list:\n",
    "        \n",
    "        content_max_length = 512-3-len(name)\n",
    "        \n",
    "        if len(content) >= content_max_length:\n",
    "            content = content[:content_max_length]\n",
    "            \n",
    "        input_ids = tokenizer.encode(name, content)\n",
    "        if(len(input_ids)>512):\n",
    "            continue\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        while len(input_ids) < 512:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        train_input_ids.append(input_ids)\n",
    "        train_token_types.append(segment_ids)\n",
    "        train_attention_mask.append(input_mask)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    \n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import XLNetForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(\"device:\", device)\n",
    "\n",
    "    lm_path = './TB_multispan/xlnet_ckip_name_is_ans_alldataset_epoch12.pkl'\n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('./chinese_xlnet_mid_pytorch/')\n",
    "    model = XLNetForSequenceClassification.from_pretrained('./chinese_xlnet_mid_pytorch/',num_labels=NUM_LABELS)\n",
    "    check_point = './TB_multispan/xlnet_ckip_name_is_ans_alldataset_epoch12.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "#             print(pred)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(pred_name_list)\n",
    "            return list(pred_name_list[pred>0])\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
