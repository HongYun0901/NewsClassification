{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "# from transformers import RobertaForTokenClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import BertTokenizer , BertConfig , BertModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertForPreTraining\n",
    "from torch import nn\n",
    "import json\n",
    "import requests\n",
    "pretrain_model_name = './chinese_roberta_wwm/'\n",
    "my_pretrain = './pretrain_roberta_on_TBdata'\n",
    "\n",
    "api_url = 'http://35.221.209.220/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"./tbrain/tbrain_train_split.csv\")\n",
    "train_df = pd.read_csv(\"./tbrain/tbrain_train.csv\")\n",
    "\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "# test_df = pd.read_csv(\"./tbrain/tbrain_test_split.csv\")\n",
    "test_df = pd.read_csv(\"./tbrain/tbrain_test.csv\")\n",
    "\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    content = content.replace('\\n','').replace('\\t','').replace(' ','').replace('\\xa0','')\n",
    "    content = re.sub(\"[●▼►★]\", \"\",content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < 510:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太常\n",
    "            if string == '':\n",
    "                n = 510\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11530,)\n",
      "(11530, 512)\n",
      "(11530, 512)\n",
      "(11530, 512)\n"
     ]
    }
   ],
   "source": [
    "contents = train_df['full_content'].tolist()\n",
    "names =  train_df['name'].tolist()\n",
    "ckip_names = train_df['ckip_names'].tolist()\n",
    "\n",
    "train_B = []\n",
    "train_I = []\n",
    "train_O = []\n",
    "train_x = []\n",
    "\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    name_li  = ast.literal_eval(names[i])\n",
    "    ckip_name_li = ast.literal_eval(ckip_names[i])\n",
    "    \n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "#         BIO\n",
    "        B = np.zeros(512)\n",
    "        I = np.zeros(512)\n",
    "        O = np.ones(512)\n",
    "        \n",
    "        if(len(name_li) == 0):\n",
    "            train_B.append(B)\n",
    "            train_I.append(I)\n",
    "            train_O.append(O)\n",
    "            train_x.append(content)\n",
    "            \n",
    "        for name in name_li:\n",
    "            start_pos = chunk.find(name)\n",
    "            if(start_pos == -1 or len(name) == 1):\n",
    "                continue\n",
    "                \n",
    "            start_pos += 1\n",
    "            end_pos = start_pos + len(name)\n",
    "            B[start_pos] = 1\n",
    "            I[start_pos+1:end_pos] = 1\n",
    "            O[start_pos:end_pos] = 0\n",
    "            \n",
    "            train_B.append(B)\n",
    "            train_I.append(I)\n",
    "            train_O.append(O)\n",
    "            train_x.append(content)\n",
    "\n",
    "            \n",
    "            \n",
    "train_x = np.array(train_x)\n",
    "train_B = np.array(train_B)\n",
    "train_I = np.array(train_I)\n",
    "train_O = np.array(train_O)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_B.shape)\n",
    "print(train_I.shape)\n",
    "print(train_O.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrain_roberta_on_TBdata/')\n",
    "config = BertConfig.from_pretrained('./pretrain_roberta_on_TBdata/'  + 'config.json',output_hidden_states=True)\n",
    "\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "with open('./BIOtrain_input_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_input_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./pretrain_roberta_on_TBdata/')\n",
    "config = BertConfig.from_pretrained('./pretrain_roberta_on_TBdata/'  + 'config.json',output_hidden_states=True)\n",
    "with open('./BIOtrain_input_dict.pickle', 'rb') as handle:\n",
    "    train_input_dict = pickle.load(handle)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, B , I , O):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.B = B\n",
    "        self.I = I\n",
    "        self.O = O\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        B = self.B[idx]\n",
    "        I = self.I[idx]\n",
    "        O = self.O[idx]\n",
    "        return inputid , tokentype , attentionmask, B , I , O\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BIOModel, self).__init__()\n",
    "        self.B_task = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 1)\n",
    "        )    \n",
    "        self.I_task = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 1)\n",
    "        )    \n",
    "        self.O_task = nn.Sequential(\n",
    "            nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(768, 1),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 1)\n",
    "        )    \n",
    "#             \n",
    "    def forward(self, x):\n",
    "        x = x.double()\n",
    "        \n",
    "        B_out = self.B_task(x)\n",
    "        I_out = self.I_task(x)\n",
    "        O_out = self.O_task(x)\n",
    "        \n",
    "        return B_out , I_out , O_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1499, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "BATCH_SIZE = 6\n",
    "trainset = TrainDataset(train_input_dict, \n",
    "                        train_B,\n",
    "                        train_I,\n",
    "                        train_O)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = BertModel.from_pretrained('./pretrain_roberta_on_TBdata/',config=config)\n",
    "model = model.to(device)\n",
    "model.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "BIO_model = BIOModel()\n",
    "BIO_model = BIO_model.to(device)\n",
    "BIO_model = BIO_model.double()\n",
    "BIO_model.train()\n",
    "\n",
    "\n",
    "weights = torch.tensor([1,1,0.01]).to(device).double()\n",
    "loss_fn = nn.CrossEntropyLoss(weight = weights)\n",
    "# loss_fn = nn.KLDivLoss()\n",
    "\n",
    "EPOCHS = 5\n",
    "# optimizer = torch.optim.Adam(BIO_model.parameters(), lr=1e-5)\n",
    "optimizer = BertAdam(BIO_model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , b_labels, i_labels , o_labels = [t.to(device) for t in data]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "        \n",
    "  \n",
    "        bert_all_768 = bert_outputs[0]\n",
    "    \n",
    "        mini_batch = bert_all_768.size()[0]\n",
    "        \n",
    "        bert_all_768 = bert_all_768.double()\n",
    "        optimizer.zero_grad()      \n",
    "        \n",
    "        b_out, i_out , o_out = BIO_model(bert_all_768)\n",
    "        \n",
    "#         b_out = b_out.reshape((mini_batch,512))\n",
    "#         i_out = i_out.reshape((mini_batch,512))\n",
    "#         o_out = o_out.reshape((mini_batch,512))\n",
    "        \n",
    "        b_out = b_out.permute(0, 2, 1) \n",
    "        i_out = i_out.permute(0, 2, 1) \n",
    "        o_out = o_out.permute(0, 2, 1) \n",
    "        \n",
    "        bio_out = torch.cat([b_out,i_out,o_out], dim = 1)\n",
    "        bio_out = bio_out.permute(0,2,1)\n",
    "#         bio_out = torch.softmax(bio_out, dim = 1)\n",
    "#         print(b_out.shape)\n",
    "#         print(bio_out.shape)\n",
    "#         print(bio_out[0])\n",
    "#         print(b_out.shape)\n",
    "        b_labels = b_labels.reshape((mini_batch,1,512))\n",
    "        i_labels = i_labels.reshape((mini_batch,1,512))\n",
    "        o_labels = o_labels.reshape((mini_batch,1,512))\n",
    "        bio_labels = torch.cat([b_labels,i_labels,o_labels], dim = 1)\n",
    "        bio_labels = torch.argmax(bio_labels , dim = 1 )\n",
    "#         bio_labels = torch.softmax()\n",
    "#         bio_labels = bio_labels.permute(0,2,1)\n",
    "\n",
    "        \n",
    "        for k in range((mini_batch)):\n",
    "            tmp_bio_out = bio_out[k]\n",
    "            tmp_bio_labels = bio_labels[k]\n",
    "            if k == 0 :\n",
    "                loss = loss_fn(tmp_bio_out,tmp_bio_labels)\n",
    "            else:\n",
    "                loss += loss_fn(tmp_bio_out,tmp_bio_labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    torch.save(BIO_model,'./TB_multispan/BIOModel' + str(epoch) + '.pkl')\n",
    "    print('[epoch %d] loss: %.3f' %(epoch + 1, running_loss))\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict,text):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.text = text\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        text = self.text[idx]\n",
    "\n",
    "        return inputid , tokentype , attentionmask , text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n",
    "BIO_model.eval()\n",
    "model.eval()\n",
    "\n",
    "names =  test_df['name'].tolist()\n",
    "contents = np.array(test_df['full_content'].tolist())\n",
    "ckip_names = test_df['ckip_names'].tolist()\n",
    "\n",
    "score = 0.0\n",
    "valid_count = 0\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "\n",
    "    if(content=='nan'):\n",
    "        continue\n",
    "#     name = names[i]\n",
    "    name_li  = ast.literal_eval(names[i])\n",
    "    my_pred_name_list = []\n",
    "    \n",
    "    if len(name_li) == 0:\n",
    "        score += eval(my_pred_name_list,name_li)\n",
    "        valid_count += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    split_content = cut_sent(content)\n",
    "\n",
    "    chunks = combine_sentence(split_content)\n",
    "\n",
    "    test_input_dict = tokenizer.batch_encode_plus(chunks, \n",
    "                                     add_special_tokens=True,\n",
    "                                     max_length=512,\n",
    "                                    truncation=True,\n",
    "                                     return_special_tokens_mask=True,\n",
    "                                     pad_to_max_length=512,\n",
    "                                     return_tensors='pt')\n",
    "\n",
    "    testset = TestDataset(train_input_dict, chunks)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for data in testloader:\n",
    "\n",
    "        tokens_tensors ,  segments_tensors , masks_tensors = [t.to(device) for t in data[:-1]]\n",
    "\n",
    "        text = data[-1]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "        bert_all_768 = bert_outputs[0]\n",
    "        mini_batch = bert_all_768.size()[0]\n",
    "        bert_all_768 = bert_all_768.double()\n",
    "        \n",
    "        b_out, i_out , o_out = BIO_model(bert_all_768)\n",
    "#         b_out = b_out.reshape((mini_batch,512))\n",
    "#         i_out = i_out.reshape((mini_batch,512))\n",
    "#         o_out = o_out.reshape((mini_batch,512))\n",
    "        b_out = b_out.permute(0, 2, 1) \n",
    "        i_out = i_out.permute(0, 2, 1) \n",
    "        o_out = o_out.permute(0, 2, 1) \n",
    "        \n",
    "        bio_out = torch.cat([b_out,i_out,o_out], dim = 1)\n",
    "        print(bio_out[0])\n",
    "        bio_out = torch.softmax(bio_out,dim=1)\n",
    "        print(bio_out[0][:,10])\n",
    "        print(bio_out.shape)\n",
    "        \n",
    "#         bio_decoding\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "        \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
