{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops, stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import BertTokenizer , BertConfig , BertModel , XLNetTokenizer, XLNetConfig , XLNetModel\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"./tbrain/tbrain_train_split.csv\")\n",
    "train_df = pd.read_csv(\"./tbrain/tbrain_train.csv\")\n",
    "\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "# test_df = pd.read_csv(\"./tbrain/tbrain_test_split.csv\")\n",
    "test_df = pd.read_csv(\"./tbrain/tbrain_test.csv\")\n",
    "\n",
    "train_df = pd.concat([train_df,test_df])\n",
    "\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "#     cc = OpenCC('t2s')\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "#     content = cc.convert(content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < 510:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太常\n",
    "            if string == '':\n",
    "                n = 510\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917,)\n",
      "(4917,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "names =  train_df['name'].tolist()\n",
    "contents = np.array(train_df['full_content'].tolist())\n",
    "ckip_names = train_df['ckip_names'].tolist()\n",
    "x = []\n",
    "binary_y = []\n",
    "names_y  = []\n",
    "content_all_names = []\n",
    "start_pos_labels = []\n",
    "end_pos_labels = []\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "\n",
    "    if(content=='nan'):\n",
    "        continue\n",
    "\n",
    "    \n",
    "    name_li  = ast.literal_eval(names[i])\n",
    "    if(len(name_li) == 0 ):\n",
    "#         content切句 \n",
    "#         split_content = cut_sent(content)\n",
    "#         chunks = combine_sentence(split_content)\n",
    "        \n",
    "#         for chunk in chunks:\n",
    "#             start_pos_label = np.zeros(512)\n",
    "#             end_pos_label = np.zeros(512)\n",
    "#             binary_y.append(0)\n",
    "#             start_pos_label[0] = 1\n",
    "#             end_pos_label[0] = 1\n",
    "#             x.append(chunk)\n",
    "#             start_pos_labels.append(start_pos_label)\n",
    "#             end_pos_labels.append(end_pos_label)\n",
    "#             content_all_names.append(content_ckip_names)\n",
    "        binary_y.append(0)\n",
    "        x.append(content)\n",
    "#         if len(content)  <= 512:\n",
    "#             x.append(content)\n",
    "#         else:\n",
    "#             x.append(content[-512:])\n",
    "        \n",
    "    else:\n",
    "        binary_y.append(1)\n",
    "        x.append(content)\n",
    "\n",
    "#         if len(content)  <= 512:\n",
    "#             x.append(content)\n",
    "#         else:\n",
    "#             x.append(content[-512:])\n",
    "        \n",
    "#         for name in name_li:\n",
    "        \n",
    "#             name = name.replace('\\n','').replace('\\t','').replace(' ','')\n",
    "#             #  content切句 \n",
    "            \n",
    "#             _pos = 0\n",
    "#             while True:\n",
    "#                 start_pos = content.find(name,_pos)\n",
    "#                 if(start_pos == -1):\n",
    "#                     break\n",
    "#                 start_pos += 1\n",
    "#                 _pos = start_pos\n",
    "#                 end_pos = start_pos + len(name)\n",
    "\n",
    "#                 if(start_pos < 512 and end_pos < 512 ):\n",
    "#                     binary_y.append(1)\n",
    "#                     x.append(content)\n",
    "\n",
    "#                     start_pos_label = np.zeros(512)\n",
    "#                     end_pos_label = np.zeros(512)\n",
    "#                     start_pos_label[start_pos] = 1\n",
    "#                     end_pos_label[end_pos] = 1\n",
    "\n",
    "#                     start_pos_labels.append(start_pos_label)\n",
    "#                     end_pos_labels.append(end_pos_label)\n",
    "#                     content_all_names.append(content_ckip_names)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "train_x = np.array(x)\n",
    "train_binary_y = np.array(binary_y)\n",
    "train_start_pos_labels = np.array(start_pos_labels)\n",
    "train_end_pos_labels = np.array(end_pos_labels)\n",
    "train_ckip_names = np.array(content_all_names)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_binary_y.shape)\n",
    "print(train_start_pos_labels.shape)\n",
    "print(train_end_pos_labels.shape)\n",
    "print(train_ckip_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "model_path  = './chinese_xlnet_mid_pytorch/'\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train acc: 0.9778320113890584\n",
      "105.24525765707222\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "BATCH_SIZE = 3\n",
    "trainset = TrainDataset(train_input_dict, \n",
    "                        train_binary_y)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "NUM_LABELS = 2\n",
    "model_path  = './chinese_xlnet_mid_pytorch/'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_path,num_labels=NUM_LABELS)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "for epoch in range(0,EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , binary_y  = [t.to(device) for t in data]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors,\n",
    "                                labels =binary_y )\n",
    "        \n",
    "  \n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += len(binary_y)\n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        correct += (logits == binary_y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(model.state_dict(),'./TB_multispan/XLNet_fromseq_' + str(epoch) + '.pkl')\n",
    "    print(epoch)\n",
    "    print('train acc:' , correct/total)\n",
    "    print(running_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlnet_news_has_ans(news):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_dict):\n",
    "            self.input_ids = input_dict['input_ids']\n",
    "            self.token_type_ids = input_dict['token_type_ids']\n",
    "            self.attention_mask = input_dict['attention_mask']\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    lm_path = './chinese_xlnet_mid_pytorch/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    content = clean_string(news)\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    train_input_dict = tokenizer.batch_encode_plus([content], \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    testset = Testset( )\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import XLNetForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    NUM_LABELS = 2\n",
    "    model = XLNetForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    check_point = './TB_multispan/rbtl3_ckip_name_is_ans_dataset3_epoch8.pkl'\n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            return pred\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ca72f1de1413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbert_all_768\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbert_all_768\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(10,EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , binary_y  = [t.to(device) for t in data]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "        \n",
    "  \n",
    "\n",
    "        bert_all_768 = bert_outputs[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total += bert_all_768.size()[0]\n",
    "        \n",
    "        \n",
    "        logits = binary_model(bert_all_768.double())\n",
    "        loss = loss_fn(logits , binary_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        correct += (logits == binary_y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(binary_model.state_dict(),'./TB_multispan/XLNet_binary_alldataset_4k_' + str(epoch) + '.pkl')\n",
    "    print(epoch)\n",
    "    print('train acc:' , correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454,)\n",
      "(454,)\n",
      "(454, 512)\n",
      "(454, 512)\n",
      "(454,)\n"
     ]
    }
   ],
   "source": [
    "names =  test_df['name'].tolist()\n",
    "contents = np.array(test_df['full_content'].tolist())\n",
    "ckip_names = test_df['ckip_names'].tolist()\n",
    "\n",
    "x = []\n",
    "binary_y = []\n",
    "names_y  = []\n",
    "content_all_names = []\n",
    "\n",
    "start_pos_labels = []\n",
    "end_pos_labels = []\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "\n",
    "\n",
    "    if(content=='nan'):\n",
    "        continue\n",
    "#     name = names[i]\n",
    "    name_li  = ast.literal_eval(names[i])\n",
    "\n",
    "\n",
    "    if(len(name_li) == 0):\n",
    "        \n",
    "        start_pos_label = np.zeros(512)\n",
    "        end_pos_label = np.zeros(512)\n",
    "        binary_y.append(0)\n",
    "        start_pos_label[0] = 1\n",
    "        end_pos_label[0] = 1\n",
    "        x.append(content)\n",
    "        start_pos_labels.append(start_pos_label)\n",
    "        end_pos_labels.append(end_pos_label)\n",
    "        content_all_names.append(content_ckip_names)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for name in name_li:\n",
    "        \n",
    "            name = name.replace('\\n','').replace('\\t','').replace(' ','')\n",
    "\n",
    "            start_pos = chunk.find(name)\n",
    "            if(start_pos == -1):\n",
    "                continue\n",
    "            start_pos += 1\n",
    "            _pos = start_pos\n",
    "            end_pos = start_pos + len(name)\n",
    "\n",
    "            if(start_pos < 512 and end_pos < 512 ):\n",
    "                binary_y.append(1)\n",
    "                x.append(chunk)\n",
    "\n",
    "                start_pos_label = np.zeros(512)\n",
    "                end_pos_label = np.zeros(512)\n",
    "                start_pos_label[start_pos] = 1\n",
    "                end_pos_label[end_pos] = 1\n",
    "\n",
    "                start_pos_labels.append(start_pos_label)\n",
    "                end_pos_labels.append(end_pos_label)\n",
    "                content_all_names.append(content_ckip_names)\n",
    "        \n",
    "\n",
    "    \n",
    "test_x = np.array(x)\n",
    "test_binary_y = np.array(binary_y)\n",
    "test_start_pos_labels = np.array(start_pos_labels)\n",
    "test_end_pos_labels = np.array(end_pos_labels)\n",
    "test_ckip_names = np.array(content_all_names)\n",
    "\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_binary_y.shape)\n",
    "print(test_start_pos_labels.shape)\n",
    "print(test_end_pos_labels.shape)\n",
    "print(test_ckip_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "model_path  = './chinese_xlnet_mid_pytorch/'\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "test_input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "train acc: 0.9845814977973568\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "testset = TrainDataset(test_input_dict, \n",
    "                        test_binary_y,\n",
    "                        test_start_pos_labels,\n",
    "                        test_end_pos_labels)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_path  = './chinese_xlnet_mid_pytorch/'\n",
    "config = XLNetConfig.from_pretrained(model_path + 'config.json',output_hidden_states=True)\n",
    "model = XLNetModel.from_pretrained(model_path,config=config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "binary_model_path = './TB_multispan/XLNet_binary16.pkl'\n",
    "checkpoint = torch.load(binary_model_path)\n",
    "binary_model = XLNetBinrayClassifier(config)\n",
    "binary_model.load_state_dict(checkpoint)\n",
    "binary_model = binary_model.to(device)\n",
    "binary_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in testloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , binary_y ,start_pos_labels , end_pos_labels = [t.to(device) for t in data]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "        \n",
    "  \n",
    "\n",
    "        bert_all_768 = bert_outputs[0]\n",
    "    \n",
    "        total += bert_all_768.size()[0]\n",
    "        logits = binary_model(bert_all_768)\n",
    "\n",
    "        \n",
    "        \n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        correct += (logits == binary_y).sum().item()\n",
    "        \n",
    "    print('train acc:' , correct/total)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
