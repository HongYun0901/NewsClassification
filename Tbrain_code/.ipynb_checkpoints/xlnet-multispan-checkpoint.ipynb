{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "cc = OpenCC('t2s')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "api = 'http://35.221.209.220/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./TB/tbrain_nlp_train.csv\")\n",
    "test_df = pd.read_csv(\"./TB/tbrain_nlp_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import XLNetTokenizer\n",
    "# from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.4.0\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "tokenizer = XLNetTokenizer.from_pretrained('./chinese_xlnet_mid_pytorch/')\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4937,)\n",
      "(4937,)\n",
      "(4937, 512)\n",
      "(4937, 512)\n",
      "(4937,)\n",
      "(4937,)\n"
     ]
    }
   ],
   "source": [
    "train=train_df['name']\n",
    "train_x=train_df['full_content']\n",
    "start_pos = []\n",
    "train_start_pos = []\n",
    "train_end_pos = []\n",
    "end_pos = []\n",
    "train_y =[]\n",
    "for i in range(4937):\n",
    "    put = train[i]\n",
    "    start = np.zeros(512)\n",
    "    end = np.zeros(512)\n",
    "    if(type(put)==float):  ##nan\n",
    "        train_y.append(0)\n",
    "        start[0]=1\n",
    "        end[0]=1\n",
    "        start_pos.append(start)\n",
    "        end_pos.append(end)\n",
    "        continue\n",
    "    else:\n",
    "        s_pos=train_x[i].find(put)\n",
    "        if(s_pos+len(put)<512):\n",
    "            start[s_pos]=1\n",
    "            end[s_pos+len(put)]=1\n",
    "        else:\n",
    "            start[0]=1\n",
    "            end[0]=1\n",
    "        start_pos.append(start)\n",
    "        end_pos.append(end)\n",
    "        train_y.append(1)\n",
    "\n",
    "train_x=np.array(train_x.tolist())\n",
    "train_y=np.array(train_y)\n",
    "start_pos = np.array(start_pos)\n",
    "end_pos = np.array(end_pos)\n",
    "for i in range(4937):\n",
    "    train_start_pos.append(start_pos[i].nonzero()[0][0])\n",
    "    train_end_pos.append(end_pos[i].nonzero()[0][0])\n",
    "train_start_pos = np.array(train_start_pos)\n",
    "train_end_pos = np.array(train_end_pos)\n",
    "print(train_start_pos.shape)\n",
    "print(train_end_pos.shape)\n",
    "print(start_pos.shape)\n",
    "print(end_pos.shape)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 202   0   0   0   0\n",
      "   0   0   0   0   0   0  25   0   0   0   0   0   2   0   0   0   0  17\n",
      "   0   0   0   0   0   0   0   0   0   0   0 189   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  73   0\n",
      " 173 119   0   0   0  16   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 104   1   0   0   0   0 230   0   0   0 376   0   0  70   0   0\n",
      "   0   0   0   0   0   0 405   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   9   0   0   0   0   0  46   0   0 345   0\n",
      "   0 131   0   0   0   0 179   0   0   0 295   0   0   0   0 197   0 129\n",
      "   0   0 472 473   0   0   0   0   0   0   0   0  67   0   8   0   0   0\n",
      "   0 294   0   0   0   0   0   0  18   0  17   0   0   0  14  28   0  13\n",
      "   0   0   0   0   0   0   0   0   0   0 231   0   0   0 218   0   0   0\n",
      "  94   0   0   0   0   0  95   0   0   0   0   0   0   0  56   0   0   0\n",
      "   0   0   0 424   0   0   0   0   0   0  20   0   0  59   0   0   0 180\n",
      "   0   0   0 236   0   0 118 110   0   0   0   0   0  20   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 210   0   0   0   0   0   0  50   0   0   0   0   0   0   0   0\n",
      "   0   0   0  89   0   0   0   0   0   0   0 171   0 368   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 231   0   0   0   0   0   0   0 225   0\n",
      "   0   0  96   0   0   0   0   0   0   0   0   0   0   0   0   0   0 367\n",
      "   0   0   0   0   0   9   0   0  37   0   0   0   0 227   0   0 206   0\n",
      "   0 322   0   0   0   0   0 165   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 134   0   0   0   0   0  22   0   0   0  11   0  64   0   0\n",
      "   0   0  39   0   0   0   0   0   0   0 213   0 213   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 232   0   0 118   0   0   0\n",
      "   0   0   0   0 236   0   0   0   0  23   0   6   0   0   0   0   0   0\n",
      "   0   0  22   0   0 281   0   0 397   0   0   0   0   0   0  82   0   0\n",
      "   0  57  10   0   0   0   0   0   0   0   0   0   0   0   0   0  97   0\n",
      " 115   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 205   0   0   0   0\n",
      "   0   0   0   0   0   0  28   0   0   0   0   0   5   0   0   0   0  20\n",
      "   0   0   0   0   0   0   0   0   0   0   0 192   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  76   0\n",
      " 176 122   0   0   0  19   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 107   4   0   0   0   0 233   0   0   0 379   0   0  73   0   0\n",
      "   0   0   0   0   0   0 408   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  13   0   0   0   0   0  49   0   0 348   0\n",
      "   0 134   0   0   0   0 182   0   0   0 298   0   0   0   0 201   0 132\n",
      "   0   0 475 476   0   0   0   0   0   0   0   0  70   0  11   0   0   0\n",
      "   0 297   0   0   0   0   0   0  21   0  20   0   0   0  17  31   0  16\n",
      "   0   0   0   0   0   0   0   0   0   0 234   0   0   0 221   0   0   0\n",
      "  97   0   0   0   0   0  98   0   0   0   0   0   0   0  59   0   0   0\n",
      "   0   0   0 427   0   0   0   0   0   0  23   0   0  62   0   0   0 183\n",
      "   0   0   0 239   0   0 121 113   0   0   0   0   0  23   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 213   0   0   0   0   0   0  53   0   0   0   0   0   0   0   0\n",
      "   0   0   0  92   0   0   0   0   0   0   0 174   0 371   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 234   0   0   0   0   0   0   0 228   0\n",
      "   0   0  99   0   0   0   0   0   0   0   0   0   0   0   0   0   0 370\n",
      "   0   0   0   0   0  12   0   0  40   0   0   0   0 230   0   0 209   0\n",
      "   0 325   0   0   0   0   0 168   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 137   0   0   0   0   0  25   0   0   0  14   0  67   0   0\n",
      "   0   0  42   0   0   0   0   0   0   0 216   0 216   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 235   0   0 121   0   0   0\n",
      "   0   0   0   0 239   0   0   0   0  26   0   9   0   0   0   0   0   0\n",
      "   0   0  25   0   0 284   0   0 400   0   0   0   0   0   0  85   0   0\n",
      "   0  60  13   0   0   0   0   0   0   0   0   0   0   0   0   0 100   0\n",
      " 118   0   0   0   0  23   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "(543, 512)\n",
      "(543, 512)\n",
      "(543,)\n",
      "(543,)\n"
     ]
    }
   ],
   "source": [
    "test=test_df['name']\n",
    "test_x = test_df['full_content']\n",
    "test_y =[]\n",
    "t_start_pos = []\n",
    "t_end_pos = []\n",
    "test_start_pos = []\n",
    "test_end_pos = []\n",
    "for i in range(543):\n",
    "    put = test[i]\n",
    "    start = np.zeros(512)\n",
    "    end = np.zeros(512)\n",
    "    if(type(put)==float):  ##nan\n",
    "        test_y.append(0)\n",
    "        start[0]=1\n",
    "        end[0]=1\n",
    "        t_start_pos.append(start)\n",
    "        t_end_pos.append(end)\n",
    "        continue\n",
    "    else:\n",
    "        test_y.append(1)\n",
    "        s_pos=test_x[i].find(put)\n",
    "        if(s_pos+len(put)<512):\n",
    "            start[s_pos]=1\n",
    "            end[s_pos+len(put)]=1\n",
    "        else:\n",
    "            start[0]=1\n",
    "            end[0]=1\n",
    "        t_start_pos.append(start)\n",
    "        t_end_pos.append(end)\n",
    "test_x=np.array(test_x.tolist())\n",
    "test_y=np.array(test_y)\n",
    "t_start_pos =np.array(t_start_pos)\n",
    "t_end_pos = np.array(t_end_pos)\n",
    "for i in range(543):\n",
    "    test_start_pos.append(t_start_pos[i].nonzero()[0][0])\n",
    "    test_end_pos.append(t_end_pos[i].nonzero()[0][0])\n",
    "test_start_pos = np.array(test_start_pos)\n",
    "test_end_pos = np.array(test_end_pos)\n",
    "print(test_start_pos)\n",
    "print(test_end_pos)\n",
    "print(t_start_pos.shape)\n",
    "print(t_end_pos.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_dict = tokenizer.batch_encode_plus(train_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StartDataset(Dataset):\n",
    "#     def __init__(self, input_dict,y):\n",
    "#         self.input_ids = input_dict['input_ids']\n",
    "#         self.token_type_ids = input_dict['token_type_ids']\n",
    "#         self.attention_mask = input_dict['attention_mask']\n",
    "#         self.y =y\n",
    "        \n",
    "#     def __getitem__(self,idx):\n",
    "#         inputid = self.input_ids[idx]\n",
    "#         tokentype = self.token_type_ids[idx]\n",
    "#         attentionmask = self.attention_mask[idx]\n",
    "#         y = self.y[idx]\n",
    "#         return inputid , tokentype , attentionmask , y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDataset(Dataset):\n",
    "    def __init__(self, input_dict,start_pos,end_pos):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        start = self.start_pos[idx]\n",
    "        end = self.end_pos[idx]\n",
    "        return inputid , tokentype , attentionmask , start, end\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "       \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "startset = PosDataset(input_dict,train_start_pos,train_end_pos)\n",
    "trainloader = DataLoader(startset, batch_size=BATCH_SIZE)\n",
    "for batch,data in enumerate(trainloader):\n",
    "    print(batch)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./chinese_xlnet_mid_pytorch/ were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ./chinese_xlnet_mid_pytorch/ and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 512\n",
    "\n",
    "model  = XLNetForSequenceClassification.from_pretrained('./chinese_xlnet_mid_pytorch/',num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    predictions_withoutmax = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            after_softmax = F.softmax(logits.data, dim=1)\n",
    "            _, pred = torch.max(after_softmax, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                \n",
    "            if predictions_withoutmax is None:\n",
    "                predictions_withoutmax = after_softmax\n",
    "            else:\n",
    "                predictions_withoutmax = torch.cat((predictions_withoutmax,after_softmax))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions_withoutmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('xlnet_single_epoch20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 4.03 GiB already allocated; 7.56 MiB free; 4.16 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-28535b117e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegments_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                         labels=labels)\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# 將參數梯度歸零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mnon_tgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmlen\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mnon_tgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_tgt_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 4.03 GiB already allocated; 7.56 MiB free; 4.16 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 10 #看看如何\n",
    "for epoch in range(EPOCHS):\n",
    "    step = 0\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "#         print(labels)\n",
    "#         if(labels.shape[0]==1):\n",
    "#             put = labels[0].nonzero()\n",
    "#             label = put\n",
    "#         else:\n",
    "#             put = labels[0].nonzero()\n",
    "#             put2 = labels[1].nonzero()\n",
    "#             label = torch.cat((put,put2),1)\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors,\n",
    "                        labels=labels)\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "    torch.save(model, 'xlnet_single_epoch25.pkl')\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names = ['type','title','text']\n",
    "#dftest = pd.read_csv('./data_after_sep/test.tsv',sep='\\t',names=column_names)\n",
    "#texts = dftest['text'].tolist()\n",
    "input_dict = tokenizer.batch_encode_plus(test_x, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=510,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "testset = TestDataset(input_dict)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, testloader)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 512)\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 115   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6\n",
      "   0   0   0   0   0   0   0   0   0   0   0 115   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 115   0\n",
      " 205  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  13   6   0   0   0   0  51   0   0   0 115   0   0   0   0   0\n",
      "   0   0   0   0   0   0  56   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 173   0   0   0   0\n",
      "   0  45   0   0   0   0 115   0   0   0  86   0   0   0   0   6   0  24\n",
      "   0   0   6  51   0   0   0   0   0   0   0   0   0   0  11   0   0   0\n",
      "   0 115   0   0   0   0   0   0  13   0   6   0   0   0   6 115   0  51\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  45   0   0   0   0   0  13   0   0   0   0   0   0   0 205   0   0   0\n",
      "   0   0   0 115   0   0   0   0   0   0  86   0   0   0   0   0   0   0\n",
      "   0   0   0  51   0   0 122  11   0   0   0   0   0   6   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0 115   0   0   0   0   0   0   0   0\n",
      "   0   0   0  13   0   0   0   0   0   0   0 115   0 115   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 115   0   0   0   0   0   0   0  51   0\n",
      "   0   0  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "   0   0   0   0   0   0   0   0 115   0   0   0   0 352   0   0   0   0\n",
      "   0  45   0   0   0   0   0 115   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  45   0   0   0   0   0 115   0   0   0  13   0  56   0   0\n",
      "   0   0 115   0   0   0   0   0   0   0  51   0   6   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  24   0   0   0   0   0   0\n",
      "   0   0   0   0  13   0   0   0   0 115   0   6   0   0   0   0   0   0\n",
      "   0   0 109   0   0   0   0   0  56   0   0   0   0   0   0   0   0   0\n",
      "   0  13   0   0   0   0   0   0   0   0   0   0   0   0   0   0 115   0\n",
      " 243   0   0   0   0  13   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 202   0   0   0   0\n",
      "   0   0   0   0   0   0  25   0   0   0   0   0   2   0   0   0   0  17\n",
      "   0   0   0   0   0   0   0   0   0   0   0 189   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  73   0\n",
      " 173 119   0   0   0  16   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 104   1   0   0   0   0 230   0   0   0 376   0   0  70   0   0\n",
      "   0   0   0   0   0   0 405   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   9   0   0   0   0   0  46   0   0 345   0\n",
      "   0 131   0   0   0   0 179   0   0   0 295   0   0   0   0 197   0 129\n",
      "   0   0 472 473   0   0   0   0   0   0   0   0  67   0   8   0   0   0\n",
      "   0 294   0   0   0   0   0   0  18   0  17   0   0   0  14  28   0  13\n",
      "   0   0   0   0   0   0   0   0   0   0 231   0   0   0 218   0   0   0\n",
      "  94   0   0   0   0   0  95   0   0   0   0   0   0   0  56   0   0   0\n",
      "   0   0   0 424   0   0   0   0   0   0  20   0   0  59   0   0   0 180\n",
      "   0   0   0 236   0   0 118 110   0   0   0   0   0  20   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 210   0   0   0   0   0   0  50   0   0   0   0   0   0   0   0\n",
      "   0   0   0  89   0   0   0   0   0   0   0 171   0 368   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 231   0   0   0   0   0   0   0 225   0\n",
      "   0   0  96   0   0   0   0   0   0   0   0   0   0   0   0   0   0 367\n",
      "   0   0   0   0   0   9   0   0  37   0   0   0   0 227   0   0 206   0\n",
      "   0 322   0   0   0   0   0 165   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 134   0   0   0   0   0  22   0   0   0  11   0  64   0   0\n",
      "   0   0  39   0   0   0   0   0   0   0 213   0 213   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 232   0   0 118   0   0   0\n",
      "   0   0   0   0 236   0   0   0   0  23   0   6   0   0   0   0   0   0\n",
      "   0   0  22   0   0 281   0   0 397   0   0   0   0   0   0  82   0   0\n",
      "   0  57  10   0   0   0   0   0   0   0   0   0   0   0   0   0  97   0\n",
      " 115   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "Your test accuracy is 85.082873\n"
     ]
    }
   ],
   "source": [
    "pred = predictions.cpu().data.numpy()\n",
    "# pred = torch.from_numpy(pred)\n",
    "#pred = torch.softmax(pred,dim=1)\n",
    "# print(pred[13][115])\n",
    "# print(pred[13][202])\n",
    "print(pred.shape)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "print(pred)\n",
    "print(test_start_pos)\n",
    "accuracy = (pred == test_start_pos).mean()\n",
    "print('Your test accuracy is %.6f' % (accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
