{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'XLNetForPreTraining'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe9d7110d5f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mXLNetForPreTraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'XLNetForPreTraining'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "# from transformers import RobertaForTokenClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer , XLNetTokenizer , XLNetForPreTraining\n",
    "from transformers import BertTokenizer , BertConfig , BertModel\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "cc = OpenCC('t2s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"./tbrain/tbrain_train_split.csv\")\n",
    "train_df = pd.read_csv(\"./tbrain/tbrain_train.csv\")\n",
    "\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "# test_df = pd.read_csv(\"./tbrain/tbrain_test_split.csv\")\n",
    "test_df = pd.read_csv(\"./tbrain/tbrain_test.csv\")\n",
    "\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(content):\n",
    "    content = content.replace('\\n','').replace('\\t','').replace(' ','').replace('\\xa0','')\n",
    "    content = re.sub(\"[●▼►★]\", \"\",content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < 510:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太常\n",
    "            if string == '':\n",
    "                n = 510\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4917\n"
     ]
    }
   ],
   "source": [
    "train_content = train_df['full_content'].apply(clean_string).tolist()\n",
    "test_content = test_df['full_content'].apply(clean_string).tolist()\n",
    "all_content = train_content + test_content\n",
    "print(len(all_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12140\n"
     ]
    }
   ],
   "source": [
    "pretrain_content = []\n",
    "\n",
    "for content in all_content:\n",
    "    split_contents = cut_sent(content)\n",
    "    tmp = combine_sentence(split_contents)\n",
    "    pretrain_content = pretrain_content + tmp\n",
    "\n",
    "print(len(pretrain_content))   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XLNetTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a620b698784e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetForPreTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XLNetTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(pretrain_model_name)\n",
    "model = XLNetForPreTraining.from_pretrained(pretrain_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tokenizer.batch_encode_plus(pretrain_content, \n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=512,\n",
    "                                         return_special_tokens_mask=True,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "pretrainset = PretrainDataset(input_dict)\n",
    "pretrainloader = DataLoader(pretrainset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in pretrainloader:   \n",
    "        tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, )\n",
    "    model.save_pretrained('./pretrain_roberta_on_TBdata')\n",
    "    print('epoch:', epoch+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
