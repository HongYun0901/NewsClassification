{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from zhon.hanzi import non_stops , stops\n",
    "import os\n",
    "import pickle\n",
    "from opencc import OpenCC\n",
    "# from transformers import RobertaForTokenClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer , BertConfig , BertModel, XLNetTokenizer ,XLNetModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification,BertForPreTraining\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertForPreTraining\n",
    "from torch import nn\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n",
      "(491, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = 1\n",
    "path = './dataset/dataset' + str(dataset) + '/'\n",
    "\n",
    "train_df = pd.read_csv(path + \"tbrain_train.csv\")\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "test_df = pd.read_csv(path + \"tbrain_test.csv\")\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "\n",
    "lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_string(content):\n",
    "#     cc = OpenCC('t2s')\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？')# erease white space cause English name error\n",
    "    content = re.sub(\"[+\\.\\/_,$%●▼►^*(+\\\"\\']+|[+——~@#￥%……&*（）★]\", \"\",content)\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "#     content = cc.convert(content)\n",
    "    return content\n",
    "\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para) \n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  \n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "\n",
    "def combine_sentence(sentences , max_len):\n",
    "    li = []\n",
    "    string = ''\n",
    "    for k in range(len(sentences)):\n",
    "        sentence = sentences[k]\n",
    "        if len(string) + len(sentence) < max_len:\n",
    "            string = string + sentence\n",
    "        else:\n",
    "#             原本是空的代表sentences太長\n",
    "            if string == '':\n",
    "                n = max_len\n",
    "                tmp_li = [sentence[i:i+n] for i in range(0, len(sentence), n)]\n",
    "                string = tmp_li.pop(-1)\n",
    "                li = li + tmp_li\n",
    "            else:\n",
    "                li.append(string)\n",
    "                string = sentence\n",
    "    if(string != ''):\n",
    "        li.append(string)\n",
    "    return li\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_split(df):\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    max_seq_length = 512 #xlnet no limit try try\n",
    "    train_x = [] #all train_x\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    train_y = []\n",
    "    #index必須的\n",
    "    for index , row in df.iterrows():\n",
    "        content = row['full_content']\n",
    "        content = clean_string(content)\n",
    "        content_ckip_names = ast.literal_eval(row['ckip_names'])\n",
    "        name_ans = ast.literal_eval(row['name'])\n",
    "        #no ans\n",
    "        if len(name_ans) == 0:\n",
    "            continue\n",
    "\n",
    "        for ckip_name in content_ckip_names:\n",
    "            content_max_length = 512-3-len(ckip_name)\n",
    "            if len(content) >= content_max_length:\n",
    "                split_content = cut_sent(content)\n",
    "                chunks = combine_sentence(split_content , content_max_length)\n",
    "                for chunk in chunks:\n",
    "                    if ckip_name not in chunk:\n",
    "                        continue\n",
    "                    input_ids = tokenizer.encode(ckip_name, chunk)\n",
    "                    if(len(input_ids)>512):\n",
    "                        continue\n",
    "                    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "                    num_seg_a = sep_index + 1\n",
    "                    num_seg_b = len(input_ids) - num_seg_a\n",
    "                    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "                    input_mask = [1] * len(input_ids)\n",
    "\n",
    "                    while len(input_ids) < 512:\n",
    "                        input_ids.append(0)\n",
    "                        input_mask.append(0)\n",
    "                        segment_ids.append(0)\n",
    "\n",
    "                    if ckip_name in name_ans:\n",
    "                        train_y.append(1)\n",
    "                    else:\n",
    "                        train_y.append(0)\n",
    "\n",
    "                    train_input_ids.append(input_ids)\n",
    "                    train_token_types.append(segment_ids)\n",
    "                    train_attention_mask.append(input_mask)\n",
    "                    train_x.append((ckip_name,chunk)) \n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                input_ids = tokenizer.encode(ckip_name, content)\n",
    "                if(len(input_ids)>512):\n",
    "                    continue\n",
    "                sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "                num_seg_a = sep_index + 1\n",
    "                num_seg_b = len(input_ids) - num_seg_a\n",
    "                segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "                input_mask = [1] * len(input_ids)\n",
    "\n",
    "                while len(input_ids) < 512:\n",
    "                    input_ids.append(0)\n",
    "                    input_mask.append(0)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "\n",
    "                if ckip_name in name_ans:\n",
    "                    train_y.append(1)\n",
    "                else:\n",
    "                    train_y.append(0)\n",
    "\n",
    "                train_input_ids.append(input_ids)\n",
    "                train_token_types.append(segment_ids)\n",
    "                train_attention_mask.append(input_mask)\n",
    "                train_x.append((ckip_name,content)) \n",
    "\n",
    "\n",
    "\n",
    "    #全部跑完轉np\n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    train_y = np.array(train_y)\n",
    "\n",
    "    print(len(train_x))\n",
    "    print(train_input_ids.shape)\n",
    "    print(train_token_types.shape)\n",
    "    print(train_attention_mask.shape)\n",
    "    print(train_y.shape)\n",
    "    \n",
    "    return train_x , train_input_ids ,train_token_types  , train_attention_mask , train_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208\n",
      "(3208, 512)\n",
      "(3208, 512)\n",
      "(3208, 512)\n",
      "(3208,)\n",
      "295\n",
      "(295, 512)\n",
      "(295, 512)\n",
      "(295, 512)\n",
      "(295,)\n"
     ]
    }
   ],
   "source": [
    "train_x , train_input_ids ,train_token_types  , train_attention_mask , train_y  = get_model_input_split(train_df)\n",
    "test_x , test_input_ids ,test_token_types  , test_attention_mask , test_y  = get_model_input_split(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, ans):\n",
    "    if bool(pred) is not bool(ans):\n",
    "        return 0\n",
    "    elif not pred and not ans:\n",
    "        return 1\n",
    "    else:\n",
    "        pred = set(pred)\n",
    "        ans = set(ans)\n",
    "        interaction_len = len(pred & ans)\n",
    "        if interaction_len == 0:\n",
    "            return 0\n",
    "\n",
    "        pred_len = len(pred)\n",
    "        ans_len = len(ans)\n",
    "        return 2 / (pred_len / interaction_len + ans_len / interaction_len)\n",
    "\n",
    "\n",
    "def eval_all(pred_list, ans_list):\n",
    "    assert len(pred_list) == len(ans_list)\n",
    "    return sum(eval(p, a) for p, a in zip(pred_list, ans_list)) / len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_ids , token_type_ids , attention_mask , y ):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.y = y\n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return inputid , tokentype , attentionmask, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(model,dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in dataloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors, \n",
    "                                labels=labels)\n",
    "            pred = outputs[1]\n",
    "            total += pred.size()[0]\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            correct += (pred==labels).sum().item()\n",
    "\n",
    "    return correct/total \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./bert_wwm_pretrain_tbrain/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_wwm_pretrain_tbrain/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 259.808\n",
      "train_acc: 0.8668952618453866\n",
      "test_acc: 0.9491525423728814\n",
      "[epoch 2] loss: 110.264\n",
      "train_acc: 0.955423940149626\n",
      "test_acc: 0.8779661016949153\n",
      "[epoch 3] loss: 58.655\n",
      "train_acc: 0.9812967581047382\n",
      "test_acc: 0.9491525423728814\n",
      "[epoch 4] loss: 31.484\n",
      "train_acc: 0.9918952618453866\n",
      "test_acc: 0.9491525423728814\n",
      "[epoch 5] loss: 25.537\n",
      "train_acc: 0.9928304239401496\n",
      "test_acc: 0.9457627118644067\n",
      "[epoch 6] loss: 18.202\n",
      "train_acc: 0.9962593516209476\n",
      "test_acc: 0.9559322033898305\n",
      "[epoch 7] loss: 14.457\n",
      "train_acc: 0.996571072319202\n",
      "test_acc: 0.9491525423728814\n",
      "[epoch 8] loss: 22.214\n",
      "train_acc: 0.9934538653366584\n",
      "test_acc: 0.9389830508474576\n",
      "[epoch 9] loss: 15.946\n",
      "train_acc: 0.9943890274314214\n",
      "test_acc: 0.9355932203389831\n",
      "[epoch 10] loss: 11.066\n",
      "train_acc: 0.9959476309226932\n",
      "test_acc: 0.9559322033898305\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "trainset = TrainDataset(train_input_ids ,train_token_types , train_attention_mask ,train_y)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE , shuffle=True)\n",
    "\n",
    "testset = TrainDataset(test_input_ids ,test_token_types , test_attention_mask ,test_y)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE , shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "NUM_LABELS = 2\n",
    "tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "model  = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-6)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors, \n",
    "                            labels=labels)\n",
    "        loss = outputs[0]\n",
    "        pred = outputs[1]\n",
    "        total += pred.size()[0]\n",
    "        pred = torch.argmax(pred,dim=-1)\n",
    "        correct += (pred==labels).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    checkpoint_path = './QAModel/' + str(dataset) + '/' \n",
    "        \n",
    "    torch.save(model.state_dict(),checkpoint_path + 'bert_wwm_QA_split_lr3e-6_' + str(epoch) + '.pkl')\n",
    "    print('[epoch %d] loss: %.3f' %(epoch + 1, running_loss))\n",
    "    print('train_acc:' ,correct/total)\n",
    "    print('test_acc:' , get_eval(model,testloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_qa_binary_split(pred_name_list , news , checkpoint):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask , names):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "            self.names = names\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            name = self.names[idx]\n",
    "            return inputid , tokentype , attentionmask , name\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "#     lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "    \n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "    testing_name = []\n",
    "    \n",
    "    \n",
    "    content = clean_string(news)\n",
    "    \n",
    "    max_length = 500\n",
    "    \n",
    "    split_content = cut_sent(content)\n",
    "    chunks = combine_sentence(split_content , max_length)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for name in pred_name_list:\n",
    "            if len(chunk) >= max_length:\n",
    "                print('error !!!! lenth > 500')\n",
    "                continue\n",
    "            if name not in chunk:\n",
    "                continue\n",
    "\n",
    "            input_ids = tokenizer.encode(name, chunk)\n",
    "            if(len(input_ids)>512):\n",
    "                continue\n",
    "            sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(input_ids) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < 512:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            train_input_ids.append(input_ids)\n",
    "            train_token_types.append(segment_ids)\n",
    "            train_attention_mask.append(input_mask)\n",
    "            testing_name.append(name)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    testing_name = np.array(testing_name)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask, testing_name)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    \n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors  = [t.to(device) for t in data[:-1]]\n",
    "            name = data[-1]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(name)\n",
    "            return list(pred_name_list[pred>0])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4426, 4)\n",
      "(491, 4)\n",
      "device: cuda:0\n",
      "dataset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './QAModel/2/roberta_QA_split2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c8cb8d99a1a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# split model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msplit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msplit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0msplit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msplit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './QAModel/2/roberta_QA_split2.pkl'"
     ]
    }
   ],
   "source": [
    "dataset = 2\n",
    "path = './dataset/dataset' + str(dataset) + '/'\n",
    "\n",
    "train_df = pd.read_csv(path + \"tbrain_train.csv\")\n",
    "train_df = train_df.fillna('None')\n",
    "\n",
    "test_df = pd.read_csv(path + \"tbrain_test.csv\")\n",
    "test_df = test_df.fillna('None')\n",
    "\n",
    "# train_df = pd.concat([train_df,test_df])\n",
    "# lm_path = './bert_wwm_pretrain_tbrain/'\n",
    "# lm_path = './pretrain_roberta_on_TBdata'\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print('dataset',dataset)\n",
    "\n",
    "base_path = './QAModel/' + str(dataset) + '/'\n",
    "\n",
    "split_checkpoint = base_path + 'roberta_QA_split2.pkl'\n",
    "\n",
    "NUM_LABELS = 2\n",
    "tokenizer = BertTokenizer.from_pretrained(lm_path)\n",
    "\n",
    "\n",
    "# split model\n",
    "split_model = BertForSequenceClassification.from_pretrained(lm_path,num_labels=NUM_LABELS)\n",
    "split_model.load_state_dict(torch.load(split_checkpoint))\n",
    "split_model = split_model.to(device)\n",
    "split_model.eval()\n",
    "\n",
    "split_pred = []\n",
    "\n",
    "ans = []\n",
    "\n",
    "for index,row in test_df.iterrows():\n",
    "    news = row['full_content']\n",
    "    ckip_names = ast.literal_eval(row['ckip_names'])\n",
    "    names = ast.literal_eval(row['name'])\n",
    "    \n",
    "    if len(names) == 0 :\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    split_result = name_qa_binary_split(ckip_names , news ,split_checkpoint)\n",
    "    split_result = list(set(split_result))\n",
    "    \n",
    "    ans.append(names)\n",
    "    split_pred.append(split_result)\n",
    "print('score:',eval_all(split_pred,ans))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>binary</th>\n",
       "      <th>ckip_name</th>\n",
       "      <th>predict_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>檢調偵辦「三鑫集團」以投資俄羅斯賭場等名目吸金12億元案，發現三鑫集團負責人曾裕仁去年因債務...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳男', '謝發布', '王妤昆', '曾裕仁']</td>\n",
       "      <td>['王妤昆', '曾裕仁']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>〔記者王善嬿／嘉市報導〕嘉義地檢署查獲台商王子南、羅則嬌等7人組集團進行兩岸地下匯兌，1年多...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王善嬿', '羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '鐘辛禾', '田某...</td>\n",
       "      <td>['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>中華心蓮慈善助學協會理事長童行成(68歲)於4年前起，以協會名義推出「幼兒教育撲滿計畫」等集...</td>\n",
       "      <td>1</td>\n",
       "      <td>['童行成', '鄧玉瑩']</td>\n",
       "      <td>['童行成']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>漢唐光電科技董事長徐建志曾和和皮膚科名醫蔡佳雅發生不倫戀，他以不實財報佯稱漢唐公司將上市，販...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳宏陽', '蔡佳雅', '徐雲其', '徐建志', '徐雲', '蔡夫']</td>\n",
       "      <td>['蔡佳雅', '徐雲其', '徐建志']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>檢調接獲報案，指位於台北市的台灣搜房公司2014年5月至12月間推出英國不動產投資方案，以保...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黃穎溪', '呂威東', '游仁汶', '吳珮', '高崇信', '彭傑', '高男']</td>\n",
       "      <td>['高崇信', '呂威東', '黃穎溪']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>國民黨宜蘭縣長參選人、羅東鎮長廖泓花遭控動用公所第二預備金補助其他鄉鎮活動涉嫌違反《預算法》...</td>\n",
       "      <td>1</td>\n",
       "      <td>['羅東陳書豪', '涂文賢', '張鴻人', '廖泓花', '韓彥宏', '施仁旭']</td>\n",
       "      <td>['廖泓花', '韓彥宏']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>〔記者葉俞揚／新北報導〕怡婷科技公司正妹負責人曾怡婷以投資蘋果、三星等手機買賣，宣稱最高年報...</td>\n",
       "      <td>1</td>\n",
       "      <td>['李文行', '怡婷', '張雅苓', '楊靖雯', '陳陽孜', '陳弘俊', '葉俞揚...</td>\n",
       "      <td>['曾雅琪', '陳弘俊', '曾怡婷', '楊靖雯']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>刑事局中部打擊犯罪中心破獲以林姓男子為首的「假轉帳、真詐騙」犯罪集團，專在網路佯裝買家，向賣...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳男', '林昶杰', '王煌忠']</td>\n",
       "      <td>['林昶杰']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>〔記者錢利忠／台北報導〕綽號「貢丸」的竹聯幫地隆會會長陳威富，涉嫌勾結運將江家瑋、林雲勳、梁...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳威富', '江家瑋', '貢丸', '林雲勳', '錢利忠', '陳嫌', '梁姵寶']</td>\n",
       "      <td>['陳威富', '林雲勳', '梁姵寶', '江家瑋']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>屏東縣前車城鄉長林俊豪於在2010年任職鄉長後利用招標機會收取不法回扣，經去年屏東地院重判有...</td>\n",
       "      <td>1</td>\n",
       "      <td>['賴筱涵', '李建冰', '賴家賢', '吳宛財', '林俊', '黃進郁', '楊白萱...</td>\n",
       "      <td>['李建冰', '吳宛財', '黃進郁', '林俊豪', '蕭信宏', '蔡佳雅', '王品...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>〔財經頻道／綜合報導〕2013年爆發的台苯掏空弊案，至今尚在漫長的司法訴訟之中，台苯大股東何...</td>\n",
       "      <td>1</td>\n",
       "      <td>['蘇一為', '吳永，', '陳敏盛', '吳恆', '張鈞傑', '吳重賢', '劉結依...</td>\n",
       "      <td>['張鈞傑', '陳敏盛', '蘇一為']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>【王惠文╱高雄報導】曾任「花媽」陳夢羽機要秘書的高市府前專委趙竹珍，因承辦世運行銷招標案收受...</td>\n",
       "      <td>1</td>\n",
       "      <td>['趙竹珍', '王惠文', '陳夢羽']</td>\n",
       "      <td>['陳夢羽', '趙竹珍']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>〔記者陳政甫／新北報導〕股市炒手許志成結合佳總興業等公司人士，炒作佳總、佶優、上櫃公司萬潤科...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黎家慧', '許志成', '陳政甫']</td>\n",
       "      <td>['黎家慧', '許志成']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>曾是天道盟太陽會會長「鐵霸」小弟兼貼身司機的太陽會大哥林政汝，二年多前因染上毒癮被鐵霸掃地出...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林政汝', '鐵霸']</td>\n",
       "      <td>['林政汝']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>〔記者張誠瑜／台中報導〕男子林燕樺、陳容佳於2016年加入詐欺集團當車手，提款27次共18萬...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林燕樺', '陳容佳', '張誠瑜', '陳男', '林男']</td>\n",
       "      <td>['林燕樺', '陳容佳']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>因巴拉圭東方市的「東方工業區」開發案控告前外交部長林宗俊誹謗的台商羅常軍，13日下午到台北地...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳正平', '林宗俊', '羅常軍', ' 羅常軍', '巴拉圭良']</td>\n",
       "      <td>['陳正平', '林宗俊', '羅常軍']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>環球不動產估價師事務負責人陳致雲，明明只有代書資格未取得估價師證照，9年前卻與台北地院民事執...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳致雲', '林茂娥', '丁漢源', '圖利罪判林男', '蔡男', '陳男', '林男']</td>\n",
       "      <td>['陳致雲', '林茂娥']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>〔記者錢利忠、王冠仁／台北報導〕今年6月才獲頒模範公務員獎的移民署長楊敬以，被《鏡週刊》踢爆...</td>\n",
       "      <td>1</td>\n",
       "      <td>['楊敬', '楊敬以', '王冠仁', '鮑秀娟', '黃雅惠', '錢利忠']</td>\n",
       "      <td>['楊敬以']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>〔記者王捷／台南報導〕台灣汽電共生股份有限公司台南廠，傳出有2名老員工疑似設人頭公司向下游廠...</td>\n",
       "      <td>1</td>\n",
       "      <td>['蕭吉峰', '林男', '顏佑霖', '劉男', '王捷']</td>\n",
       "      <td>['顏佑霖', '蕭吉峰']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>女子何佳華被控在臉書及LINE成立「高雄嬰兒用品買賣」、「花花柑仔店」等網購群組販賣奶粉、尿...</td>\n",
       "      <td>1</td>\n",
       "      <td>['何佳華', '何女', '王怡如']</td>\n",
       "      <td>['何佳華']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>〔記者黃建緯／新北報導〕選前全國大掃黑，新北市海山、三峽警分局分別瓦解基隆饒志銘、桃園楊其典...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林子軒', '饒男', '東東', '楊其典', '沈秋正', '饒志銘', '黃建緯'...</td>\n",
       "      <td>['楊其典']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>〔記者蔡彰盛／新竹報導〕扯！新竹地區兄弟檔陳月修、陳陽孜與男子郭菁鴻共3名漁船船長，被控將漁...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳陽孜', '蔡彰盛', '郭菁鴻', '陳月修']</td>\n",
       "      <td>['陳陽孜', '郭菁鴻', '陳月修']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>「掃黑行動專案」全面啟動，台北市刑大破獲竹聯幫「月堂」堂主吳德康的不法暴力集團，警方日前接獲...</td>\n",
       "      <td>1</td>\n",
       "      <td>['曾舜泰', '吳德康', '太子宗', '月堂', '黃怡文']</td>\n",
       "      <td>['曾舜泰', '吳德康']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>〔記者王淑山／新北報導〕不動產土地開發公司「貽信開發建設」，涉嫌以投資國內法拍屋及越南不動產...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王淑山', '張女', '吳致遠', '吳男', '張元恬']</td>\n",
       "      <td>['吳致遠', '張元恬']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>〔記者楊國文／台北報導〕國民黨籍宜蘭縣議員王承光被控以每票1000元代價向選民買票，被檢方依...</td>\n",
       "      <td>1</td>\n",
       "      <td>['王承光', '王停權', '楊國文']</td>\n",
       "      <td>['王承光', '王停權']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>〔社會新聞中心／綜合報導〕法務部調查局北部地區機動工作站掌握情資，有一艘中國籍漁船從泰國金三...</td>\n",
       "      <td>1</td>\n",
       "      <td>['張明儒', '翁家佑', '簡志枝']</td>\n",
       "      <td>['張明儒', '簡志枝']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>〔記者黃易揚／綜合報導〕嘉義地檢署日前起訴史上最大地下匯兌案的經營主嫌邱富志夫婦等多人，查出...</td>\n",
       "      <td>1</td>\n",
       "      <td>['黃易揚', '邱男', '蔡宗穎', '邱富志', '梁男', '林韋志', '陳育菱'...</td>\n",
       "      <td>['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>嘉義縣警局朴子分局外事組警員吳典歌，與同仁逮捕在KTV坐檯陪酒的2名女逃逸移工時，媒介女移工...</td>\n",
       "      <td>1</td>\n",
       "      <td>['吳員', '阿蒂', '蘇菲雅', '吳員知法', '吳典歌', '容留女']</td>\n",
       "      <td>['吳典歌']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>吸金136億元、棄保潛逃的非法吸金集團主嫌邱家良今天持偽造假護照入境泰國時被查獲，雖然他否認...</td>\n",
       "      <td>1</td>\n",
       "      <td>['邱男', '林俊安', '邱家良']</td>\n",
       "      <td>['邱家良']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>空軍第二戰術戰鬥機隊憲兵二中隊中隊長洪國源，明知部隊核發個人獎金不得擅自挪用，竟任意勾選人頭...</td>\n",
       "      <td>1</td>\n",
       "      <td>['林女', '洪男', '洪國源', '倪男']</td>\n",
       "      <td>['洪國源']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>〔記者彭方賢／台北報導〕嘉義縣水上國中前校長陳正平，5年前任內辦理學校風雨操場鐵皮屋拆除業務...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳正平', '彭方賢']</td>\n",
       "      <td>['陳正平']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>〔記者陳政甫／新竹報導〕國民黨新竹縣橫山鄉長吳怡易涉嫌在縣議員任內，以鄰居作為人頭，詐領助理...</td>\n",
       "      <td>1</td>\n",
       "      <td>['陳政甫', '邱男', '錢英杰', '黃男', '蕭男', '清潔隊', '吳怡易',...</td>\n",
       "      <td>['吳怡易']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>電子支付近年日益普及，國內有不法傳銷業者看準這股風潮，推出名為「Q點支付」的傳銷方案吸金，聲...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Jeremy Millar', '駱芷瑋', '吉米', '邱富志', '連庭亨', '...</td>\n",
       "      <td>['連庭亨', '毛維倫']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article  binary  \\\n",
       "0    檢調偵辦「三鑫集團」以投資俄羅斯賭場等名目吸金12億元案，發現三鑫集團負責人曾裕仁去年因債務...       1   \n",
       "22   〔記者王善嬿／嘉市報導〕嘉義地檢署查獲台商王子南、羅則嬌等7人組集團進行兩岸地下匯兌，1年多...       1   \n",
       "42   中華心蓮慈善助學協會理事長童行成(68歲)於4年前起，以協會名義推出「幼兒教育撲滿計畫」等集...       1   \n",
       "48   漢唐光電科技董事長徐建志曾和和皮膚科名醫蔡佳雅發生不倫戀，他以不實財報佯稱漢唐公司將上市，販...       1   \n",
       "89   檢調接獲報案，指位於台北市的台灣搜房公司2014年5月至12月間推出英國不動產投資方案，以保...       1   \n",
       "125  國民黨宜蘭縣長參選人、羅東鎮長廖泓花遭控動用公所第二預備金補助其他鄉鎮活動涉嫌違反《預算法》...       1   \n",
       "131  〔記者葉俞揚／新北報導〕怡婷科技公司正妹負責人曾怡婷以投資蘋果、三星等手機買賣，宣稱最高年報...       1   \n",
       "135  刑事局中部打擊犯罪中心破獲以林姓男子為首的「假轉帳、真詐騙」犯罪集團，專在網路佯裝買家，向賣...       1   \n",
       "139  〔記者錢利忠／台北報導〕綽號「貢丸」的竹聯幫地隆會會長陳威富，涉嫌勾結運將江家瑋、林雲勳、梁...       1   \n",
       "146  屏東縣前車城鄉長林俊豪於在2010年任職鄉長後利用招標機會收取不法回扣，經去年屏東地院重判有...       1   \n",
       "151  〔財經頻道／綜合報導〕2013年爆發的台苯掏空弊案，至今尚在漫長的司法訴訟之中，台苯大股東何...       1   \n",
       "159  【王惠文╱高雄報導】曾任「花媽」陳夢羽機要秘書的高市府前專委趙竹珍，因承辦世運行銷招標案收受...       1   \n",
       "170  〔記者陳政甫／新北報導〕股市炒手許志成結合佳總興業等公司人士，炒作佳總、佶優、上櫃公司萬潤科...       1   \n",
       "194  曾是天道盟太陽會會長「鐵霸」小弟兼貼身司機的太陽會大哥林政汝，二年多前因染上毒癮被鐵霸掃地出...       1   \n",
       "201  〔記者張誠瑜／台中報導〕男子林燕樺、陳容佳於2016年加入詐欺集團當車手，提款27次共18萬...       1   \n",
       "218  因巴拉圭東方市的「東方工業區」開發案控告前外交部長林宗俊誹謗的台商羅常軍，13日下午到台北地...       1   \n",
       "225  環球不動產估價師事務負責人陳致雲，明明只有代書資格未取得估價師證照，9年前卻與台北地院民事執...       1   \n",
       "247  〔記者錢利忠、王冠仁／台北報導〕今年6月才獲頒模範公務員獎的移民署長楊敬以，被《鏡週刊》踢爆...       1   \n",
       "260  〔記者王捷／台南報導〕台灣汽電共生股份有限公司台南廠，傳出有2名老員工疑似設人頭公司向下游廠...       1   \n",
       "275  女子何佳華被控在臉書及LINE成立「高雄嬰兒用品買賣」、「花花柑仔店」等網購群組販賣奶粉、尿...       1   \n",
       "282  〔記者黃建緯／新北報導〕選前全國大掃黑，新北市海山、三峽警分局分別瓦解基隆饒志銘、桃園楊其典...       1   \n",
       "297  〔記者蔡彰盛／新竹報導〕扯！新竹地區兄弟檔陳月修、陳陽孜與男子郭菁鴻共3名漁船船長，被控將漁...       1   \n",
       "311  「掃黑行動專案」全面啟動，台北市刑大破獲竹聯幫「月堂」堂主吳德康的不法暴力集團，警方日前接獲...       1   \n",
       "312  〔記者王淑山／新北報導〕不動產土地開發公司「貽信開發建設」，涉嫌以投資國內法拍屋及越南不動產...       1   \n",
       "320  〔記者楊國文／台北報導〕國民黨籍宜蘭縣議員王承光被控以每票1000元代價向選民買票，被檢方依...       1   \n",
       "326  〔社會新聞中心／綜合報導〕法務部調查局北部地區機動工作站掌握情資，有一艘中國籍漁船從泰國金三...       1   \n",
       "327  〔記者黃易揚／綜合報導〕嘉義地檢署日前起訴史上最大地下匯兌案的經營主嫌邱富志夫婦等多人，查出...       1   \n",
       "329  嘉義縣警局朴子分局外事組警員吳典歌，與同仁逮捕在KTV坐檯陪酒的2名女逃逸移工時，媒介女移工...       1   \n",
       "331  吸金136億元、棄保潛逃的非法吸金集團主嫌邱家良今天持偽造假護照入境泰國時被查獲，雖然他否認...       1   \n",
       "342  空軍第二戰術戰鬥機隊憲兵二中隊中隊長洪國源，明知部隊核發個人獎金不得擅自挪用，竟任意勾選人頭...       1   \n",
       "343  〔記者彭方賢／台北報導〕嘉義縣水上國中前校長陳正平，5年前任內辦理學校風雨操場鐵皮屋拆除業務...       1   \n",
       "357  〔記者陳政甫／新竹報導〕國民黨新竹縣橫山鄉長吳怡易涉嫌在縣議員任內，以鄰居作為人頭，詐領助理...       1   \n",
       "369  電子支付近年日益普及，國內有不法傳銷業者看準這股風潮，推出名為「Q點支付」的傳銷方案吸金，聲...       1   \n",
       "\n",
       "                                             ckip_name  \\\n",
       "0                          ['陳男', '謝發布', '王妤昆', '曾裕仁']   \n",
       "22   ['王善嬿', '羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '鐘辛禾', '田某...   \n",
       "42                                      ['童行成', '鄧玉瑩']   \n",
       "48            ['陳宏陽', '蔡佳雅', '徐雲其', '徐建志', '徐雲', '蔡夫']   \n",
       "89      ['黃穎溪', '呂威東', '游仁汶', '吳珮', '高崇信', '彭傑', '高男']   \n",
       "125       ['羅東陳書豪', '涂文賢', '張鴻人', '廖泓花', '韓彥宏', '施仁旭']   \n",
       "131  ['李文行', '怡婷', '張雅苓', '楊靖雯', '陳陽孜', '陳弘俊', '葉俞揚...   \n",
       "135                               ['陳男', '林昶杰', '王煌忠']   \n",
       "139    ['陳威富', '江家瑋', '貢丸', '林雲勳', '錢利忠', '陳嫌', '梁姵寶']   \n",
       "146  ['賴筱涵', '李建冰', '賴家賢', '吳宛財', '林俊', '黃進郁', '楊白萱...   \n",
       "151  ['蘇一為', '吳永，', '陳敏盛', '吳恆', '張鈞傑', '吳重賢', '劉結依...   \n",
       "159                              ['趙竹珍', '王惠文', '陳夢羽']   \n",
       "170                              ['黎家慧', '許志成', '陳政甫']   \n",
       "194                                      ['林政汝', '鐵霸']   \n",
       "201                  ['林燕樺', '陳容佳', '張誠瑜', '陳男', '林男']   \n",
       "218              ['陳正平', '林宗俊', '羅常軍', ' 羅常軍', '巴拉圭良']   \n",
       "225  ['陳致雲', '林茂娥', '丁漢源', '圖利罪判林男', '蔡男', '陳男', '林男']   \n",
       "247          ['楊敬', '楊敬以', '王冠仁', '鮑秀娟', '黃雅惠', '錢利忠']   \n",
       "260                   ['蕭吉峰', '林男', '顏佑霖', '劉男', '王捷']   \n",
       "275                               ['何佳華', '何女', '王怡如']   \n",
       "282  ['林子軒', '饒男', '東東', '楊其典', '沈秋正', '饒志銘', '黃建緯'...   \n",
       "297                       ['陳陽孜', '蔡彰盛', '郭菁鴻', '陳月修']   \n",
       "311                 ['曾舜泰', '吳德康', '太子宗', '月堂', '黃怡文']   \n",
       "312                  ['王淑山', '張女', '吳致遠', '吳男', '張元恬']   \n",
       "320                              ['王承光', '王停權', '楊國文']   \n",
       "326                              ['張明儒', '翁家佑', '簡志枝']   \n",
       "327  ['黃易揚', '邱男', '蔡宗穎', '邱富志', '梁男', '林韋志', '陳育菱'...   \n",
       "329          ['吳員', '阿蒂', '蘇菲雅', '吳員知法', '吳典歌', '容留女']   \n",
       "331                               ['邱男', '林俊安', '邱家良']   \n",
       "342                          ['林女', '洪男', '洪國源', '倪男']   \n",
       "343                                     ['陳正平', '彭方賢']   \n",
       "357  ['陳政甫', '邱男', '錢英杰', '黃男', '蕭男', '清潔隊', '吳怡易',...   \n",
       "369  ['Jeremy Millar', '駱芷瑋', '吉米', '邱富志', '連庭亨', '...   \n",
       "\n",
       "                                          predict_name  \n",
       "0                                       ['王妤昆', '曾裕仁']  \n",
       "22   ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南']  \n",
       "42                                             ['童行成']  \n",
       "48                               ['蔡佳雅', '徐雲其', '徐建志']  \n",
       "89                               ['高崇信', '呂威東', '黃穎溪']  \n",
       "125                                     ['廖泓花', '韓彥宏']  \n",
       "131                       ['曾雅琪', '陳弘俊', '曾怡婷', '楊靖雯']  \n",
       "135                                            ['林昶杰']  \n",
       "139                       ['陳威富', '林雲勳', '梁姵寶', '江家瑋']  \n",
       "146  ['李建冰', '吳宛財', '黃進郁', '林俊豪', '蕭信宏', '蔡佳雅', '王品...  \n",
       "151                              ['張鈞傑', '陳敏盛', '蘇一為']  \n",
       "159                                     ['陳夢羽', '趙竹珍']  \n",
       "170                                     ['黎家慧', '許志成']  \n",
       "194                                            ['林政汝']  \n",
       "201                                     ['林燕樺', '陳容佳']  \n",
       "218                              ['陳正平', '林宗俊', '羅常軍']  \n",
       "225                                     ['陳致雲', '林茂娥']  \n",
       "247                                            ['楊敬以']  \n",
       "260                                     ['顏佑霖', '蕭吉峰']  \n",
       "275                                            ['何佳華']  \n",
       "282                                            ['楊其典']  \n",
       "297                              ['陳陽孜', '郭菁鴻', '陳月修']  \n",
       "311                                     ['曾舜泰', '吳德康']  \n",
       "312                                     ['吳致遠', '張元恬']  \n",
       "320                                     ['王承光', '王停權']  \n",
       "326                                     ['張明儒', '簡志枝']  \n",
       "327                       ['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']  \n",
       "329                                            ['吳典歌']  \n",
       "331                                            ['邱家良']  \n",
       "342                                            ['洪國源']  \n",
       "343                                            ['陳正平']  \n",
       "357                                            ['吳怡易']  \n",
       "369                                     ['連庭亨', '毛維倫']  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tbrain/2020-07-29.csv')\n",
    "\n",
    "correct =data[data['binary']==1]\n",
    "\n",
    "correct\n",
    "# correct = np.array(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['王妤昆', '曾裕仁']\n",
      "ans :  ['王妤昆', '曾裕仁']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '鐘辛禾', '周志羽', '陳潔彥', '王子南']\n",
      "ans :  ['羅則嬌', '李瑋玲', '楊培芬', '吳仰孜', '周志羽', '陳潔彥', '王子南', '鐘辛禾']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['童行成']\n",
      "ans :  ['童行成']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蔡佳雅', '徐雲其', '徐建志']\n",
      "ans :  ['徐建志']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['黃穎溪', '呂威東', '高崇信', '彭傑']\n",
      "ans :  ['高崇信', '呂威東', '黃穎溪', '彭傑']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['廖泓花']\n",
      "ans :  ['廖泓花']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳弘俊', '曾怡婷', '曾雅琪']\n",
      "ans :  ['曾雅琪', '曾怡婷']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林昶杰']\n",
      "ans :  ['林昶杰']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳威富', '江家瑋', '林雲勳', '梁姵寶']\n",
      "ans :  ['陳威富', '林雲勳', '梁姵寶', '江家瑋']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['賴筱涵', '吳宛財', '黃進郁', '林俊豪', '蔡佳雅', '王品妮']\n",
      "ans :  ['吳宛財', '林俊豪', '蔡佳雅']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蘇一為', '陳敏盛', '張鈞傑', '劉結依', '何山']\n",
      "ans :  ['張鈞傑', '陳敏盛', '蘇一為', '劉結依', '何山']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['趙竹珍']\n",
      "ans :  ['趙竹珍']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['黎家慧', '許志成']\n",
      "ans :  ['黎家慧', '許志成']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林政汝']\n",
      "ans :  ['林政汝']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林燕樺', '陳容佳']\n",
      "ans :  ['林燕樺', '陳容佳']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['林宗俊', '羅常軍', ' 羅常軍']\n",
      "ans :  ['陳正平', '林宗俊', '羅常軍']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳致雲', '林茂娥']\n",
      "ans :  ['陳致雲', '林茂娥']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['楊敬以']\n",
      "ans :  ['楊敬以']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蕭吉峰', '顏佑霖']\n",
      "ans :  ['顏佑霖', '蕭吉峰']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['何佳華']\n",
      "ans :  ['何佳華']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['楊其典', '饒志銘']\n",
      "ans :  ['楊其典', '饒志銘', '林子軒']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳陽孜', '郭菁鴻', '陳月修']\n",
      "ans :  ['陳陽孜', '郭菁鴻', '陳月修']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['曾舜泰', '吳德康']\n",
      "ans :  ['曾舜泰', '吳德康']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['吳致遠', '張元恬']\n",
      "ans :  ['吳致遠', '張元恬']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['王承光']\n",
      "ans :  ['王承光']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['張明儒', '翁家佑', '簡志枝']\n",
      "ans :  ['張明儒', '簡志枝', '翁家佑']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蔡宗穎', '邱富志', '陳育菱', '梁駿潔']\n",
      "ans :  ['梁駿潔', '邱富志', '蔡宗穎', '陳育菱']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['蘇菲雅', '吳典歌']\n",
      "ans :  ['吳典歌']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['邱家良']\n",
      "ans :  ['邱家良']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['洪國源']\n",
      "ans :  ['洪國源']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['陳正平']\n",
      "ans :  ['陳正平']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['吳怡易']\n",
      "ans :  ['吳怡易']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at ./pretrain_roberta_on_TBdata/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_roberta_on_TBdata/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "pred :  ['連庭亨', '毛維倫']\n",
      "ans :  ['連庭亨']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9323232323232324"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./tbrain/2020-07-29_test.csv')\n",
    "\n",
    "test_df =data[data['binary']==1]\n",
    "# test_df = pd.read_csv(\"./dataset/tbrain_test (2).csv\")\n",
    "# test_df = test_df.fillna('None')\n",
    "\n",
    "names =  test_df['predict_name'].tolist()\n",
    "contents = np.array(test_df['article'].tolist())\n",
    "ckip_names = test_df['ckip_name'].tolist()\n",
    "\n",
    "valid_count = 0\n",
    "score = 0.0\n",
    "\n",
    "pred_list = []\n",
    "ans_list = []\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    content = contents[i]\n",
    "    content = clean_string(content)\n",
    "    content_ckip_names = ast.literal_eval(ckip_names[i])\n",
    "    name_list = ast.literal_eval(names[i])\n",
    "    \n",
    "    \n",
    "    if len(name_list) == 0:\n",
    "        continue\n",
    "        \n",
    "    pred =  check_pred_name_is_real_ans(content_ckip_names,content)\n",
    "    \n",
    "    print('---------------')\n",
    "    print('pred : ' , pred)\n",
    "    print('ans : ', name_list)\n",
    "        \n",
    "        \n",
    "    pred_list.append(pred)\n",
    "    ans_list.append(name_list)\n",
    "    \n",
    "eval_all(pred_list,ans_list)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_name_is_real_ans(pred_name_list,news):\n",
    "    class Testset(Dataset):\n",
    "        def __init__(self, input_ids , token_type_ids , attention_mask):\n",
    "            self.input_ids = input_ids\n",
    "            self.token_type_ids = token_type_ids\n",
    "            self.attention_mask = attention_mask\n",
    "        def __getitem__(self,idx):\n",
    "            inputid = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "\n",
    "            return inputid , tokentype , attentionmask\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('./pretrain_roberta_on_TBdata/')\n",
    "\n",
    "    content = clean_string(news)\n",
    "    train_input_ids = []\n",
    "    train_token_types = []\n",
    "    train_attention_mask = []\n",
    "        \n",
    "    for name in pred_name_list:\n",
    "        \n",
    "        content_max_length = 512-3-len(name)\n",
    "        \n",
    "        if len(content) >= content_max_length:\n",
    "            content = content[:content_max_length]\n",
    "            \n",
    "        input_ids = tokenizer.encode(name, content)\n",
    "        if(len(input_ids)>512):\n",
    "            continue\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        while len(input_ids) < 512:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        train_input_ids.append(input_ids)\n",
    "        train_token_types.append(segment_ids)\n",
    "        train_attention_mask.append(input_mask)\n",
    "        \n",
    "    train_input_ids = np.array(train_input_ids)\n",
    "    train_token_types  = np.array(train_token_types)\n",
    "    train_attention_mask = np.array(train_attention_mask)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = train_input_ids.shape[0]\n",
    "    \n",
    "    testset = Testset(train_input_ids ,train_token_types , train_attention_mask)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    from transformers import BertForSequenceClassification\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(\"device:\", device)\n",
    "#     checkpoint_path = './QAModel/1/roberta_QA_split_lr3e-6_4.pkl' \n",
    "        \n",
    "#     torch.save(model.state_dict(),checkpoint_path + 'roberta_QA_split_lr3e-6_' + str(epoch) + '.pkl')\n",
    "#     lm_path = './QAModel/1/roberta_QA_split_lr3e-6_4.pkl' \n",
    "    NUM_LABELS = 2\n",
    "    tokenizer = BertTokenizer.from_pretrained('./pretrain_roberta_on_TBdata/')\n",
    "    model = BertForSequenceClassification.from_pretrained('./pretrain_roberta_on_TBdata/',num_labels=NUM_LABELS)\n",
    "    check_point = './QAModel/1/bert_wwm_QA_split_lr3e-6_9.pkl' \n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "            pred = torch.softmax(outputs[0] , dim = -1)\n",
    "            torch.set_printoptions(precision=10)\n",
    "#             print(pred)\n",
    "            pred = torch.argmax(pred,dim=-1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            pred_name_list = np.array(pred_name_list)\n",
    "            return list(pred_name_list[pred>0])\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
