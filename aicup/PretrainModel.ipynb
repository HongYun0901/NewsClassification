{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import collections\n",
    "import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some value\n",
    "max_seq_length = 512\n",
    "masked_lm_prob = 0.15\n",
    "max_predictions_per_seq = 20\n",
    "rng = random.Random()\n",
    "\n",
    "pretrain_model_path = '../chinese_wwm_pytorch/'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['article_id', 'start_position', 'end_position', 'entity_text', 'entity_type']\n",
    "df = pd.read_csv('./data/train_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from zhon.hanzi import stops\n",
    "def cut_sent(para):\n",
    "    para = re.sub(\"([。！？\\?])([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\.{6})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\…{2})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"([。！？\\?][”’])([^，。！？\\?])\", r\"\\1\\n\\2\", para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？').replace('.','。')\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content\n",
    "\n",
    "class TrainingInstance(object):\n",
    "    # \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,is_random_next,attention_mask,original_tokens):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.attention_mask = attention_mask\n",
    "        self.original_tokens = original_tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "# \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()   \n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(\n",
    "    tokens,\n",
    "    masked_lm_prob,\n",
    "    max_predictions_per_seq,\n",
    "    vocab_words,\n",
    "    rng,\n",
    "    ):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]' or token == '[PAD]':\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "\n",
    "    output_tokens = list(tokens)\n",
    "\n",
    "    num_to_predict = min(max_predictions_per_seq, max(1,\n",
    "                         int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if index in covered_indexes:\n",
    "            continue\n",
    "        covered_indexes.add(index)\n",
    "\n",
    "        masked_token = None\n",
    "\n",
    "    # 80% of the time, replace with [MASK]\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = '[MASK]'\n",
    "        else:\n",
    "\n",
    "      # 10% of the time, keep original\n",
    "\n",
    "            if rng.random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "            else:\n",
    "\n",
    "      # 10% of the time, replace with random word\n",
    "\n",
    "                masked_token = vocab_words[rng.randint(0,\n",
    "                        len(vocab_words) - 1)]\n",
    "\n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        masked_lms.append(MaskedLmInstance(index=index,\n",
    "                          label=tokens[index]))\n",
    "\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2184\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "all_texts = df['article'].tolist()\n",
    "print(len(all_texts))\n",
    "all_texts = list(dict.fromkeys(all_texts))\n",
    "print(len(all_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "all_documents = [[]]\n",
    "for text in all_texts:\n",
    "    text = clean_string(text)\n",
    "    sentences = cut_sent(text)\n",
    "    all_documents.append(sentences)\n",
    "\n",
    "# Remove empty documents\n",
    "all_documents = [x for x in all_documents if x]\n",
    "rng.shuffle(all_documents)\n",
    "print(len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_instance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "for document_index in range(len(all_documents)):\n",
    "    document = all_documents[document_index]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "    target_seq_length = max_num_tokens\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                tokens_b = []\n",
    "\n",
    "        # Random next\n",
    "\n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = rng.randint(0,\n",
    "                                len(all_documents) - 1)\n",
    "                        if random_document_index != document_index:\n",
    "                            break\n",
    "\n",
    "                    random_document = \\\n",
    "                        all_documents[random_document_index]\n",
    "                    random_start = rng.randint(0, len(random_document)\n",
    "                            - 1)\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                else:\n",
    "\n",
    "        # Actual next\n",
    "\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens,\n",
    "                                  rng)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                tokens.append('[CLS]')\n",
    "                segment_ids.append(0)\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                attention_mask = [1] * len(tokens)\n",
    "                \n",
    "                while len(tokens) < max_seq_length:\n",
    "                    tokens.append('[PAD]')\n",
    "                    segment_ids.append(0)\n",
    "                    attention_mask.append(0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                original_tokens = tokens\n",
    "                (tokens, masked_lm_positions, masked_lm_labels) = \\\n",
    "                    create_masked_lm_predictions(tokens,\n",
    "                        masked_lm_prob, max_predictions_per_seq,\n",
    "                        vocab_words, rng)\n",
    "                \n",
    "                instance = TrainingInstance(tokens=tokens,\n",
    "                        segment_ids=segment_ids,\n",
    "                        is_random_next=is_random_next,\n",
    "                        masked_lm_positions=masked_lm_positions,\n",
    "                        masked_lm_labels=masked_lm_labels,\n",
    "                        attention_mask = attention_mask,\n",
    "                        original_tokens = original_tokens)\n",
    "                \n",
    "                instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "#     print(instances)\n",
    "    pretrain_instance.extend(instances)\n",
    "\n",
    "print(len(pretrain_instance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '個', '管', '師', '：', '其', '實', '你', '這', '樣', '子', '的', '吃', '法', '是', '吃', '每', '天', '嗎', '。', '民', '眾', '：', '對', '我', '是', '吃', '每', '天', '。', '個', '管', '師', '：', '亲', '你', '有', '固', '定', '的', '時', '間', '嗎', '。', '[MASK]', '眾', '：', '每', '天', '大', '概', '都', '九', '、', '十', '點', '。', '個', '管', '師', '：', '九', '、', '十', '點', '晚', '上', '。', '民', '眾', '：', '晚', '上', '。', '個', '管', '師', '：', '晚', '上', '，', '就', '是', '飯', '前', '飯', '後', '，', '那', '你', '吃', '這', '樣', '[MASK]', 'r', 'E', '[MASK]', '有', '沒', '有', '[MASK]', '舒', '服', '。', '民', '眾', '：', '不', '會', '。', '[SEP]', '師', '：', '[MASK]', '，', '很', '可', '愛', '[MASK]', '，', '一', '歲', '。', '民', '眾', '：', '問', '題', '是', '她', '那', '個', '骨', '頭', '發', '育', '不', '全', '，', '所', '以', '…', '…', '醫', '師', '：', '先', '天', '的', '。', '民', '眾', '：', '對', '，', '就', '是', '基', '因', '突', '變', '。', '醫', '師', '：', '基', '因', '突', '變', '，', '玻', '璃', '娃', '娃', '嗎', '，', '還', '是', '。', '民', '眾', '：', '誒', '，', '類', '似', '那', '種', '侏', '儒', '症', '。', '醫', '師', '：', '侏', '儒', '症', '的', '，', '喔', '，', '阿', '你', '們', '有', '基', '因', '嗎', '，', '還', '是', '。', '民', '眾', '[MASK]', '沒', '[MASK]', '阿', '。', '醫', '師', '：', '沒', '有', '，', '阿', '那', '個', '普', '拿', '疼', '我', '要', '不', '要', '開', '給', '妳', '。', '民', '眾', '[MASK]', '普', '拿', '疼', '不', '用', '。', '醫', '師', '：', '不', '用', '喔', '。', '民', '眾', '：', '不', '用', '。', '醫', '師', '：', '妳', '家', '裡', '[MASK]', '有', '剩', '這', '樣', '子', '。', '民', '眾', '：', '還', '有', '。', '醫', '師', '：', '[MASK]', '，', '奇', '怪', '妳', '發', '炎', '指', '##pods', '怎', '麼', '這', '麼', '高', '，', '突', '然', '竄', '到', '那', '麼', '高', '，', '呵', '呵', '～', '民', '眾', '：', '[MASK]', '知', '。', '醫', '[MASK]', '：', '不', '知', '。', '民', '[MASK]', '：', '不', '知', '道', '為', '什', '麼', '。', '[MASK]', '師', '：', '好', '，', '阿', '妳', '[MASK]', '次', '要', '回', '診', '什', '麼', '時', '候', '。', '民', '眾', '：', '就', '…', '…', '醫', '師', '：', '禮', '拜', '一', '禮', '拜', '三', '或', '禮', '拜', '四', '。', '民', '眾', '：', '一', '或', '三', '喔', '。', '醫', '師', '：', '是', '，', '一', '[MASK]', '四', '都', '可', '以', '。', '民', '眾', '：', '我', '一', '三', '，', '一', '三', '比', '較', 'o', 'k', '。', '醫', '師', '：', '一', '三', '比', '較', 'o', '##鞘', '。', '民', '眾', '：', '都', '是', '下', '午', '嗎', '。', '醫', '師', '：', '是', '。', '民', '眾', '：', '都', '可', '以', '，', '一', '…', '…', '一', '跟', '三', '的', '下', '午', '都', 'o', 'k', '。', '醫', '師', '：', '剛', '好', '下', '禮', '拜', '三', '人', '很', '多', '。', '民', '眾', '：', '人', '很', '多', '。', '醫', '師', '：', '看', '看', '假', '如', '禮', '拜', '一', '不', '知', '道', '行', '不', '行', '。', '因', '為', '我', '下', '禮', '拜', '一', '是', '人', '多', '少', '。', '護', '理', '師', '：', '禮', '拜', '一', '喔', '，', '呵', '呵', '～', '醫', '師', '：', '[SEP]']\n",
      "['[CLS]', '個', '管', '師', '：', '其', '實', '你', '這', '樣', '子', '的', '吃', '法', '是', '吃', '每', '天', '嗎', '。', '民', '眾', '：', '對', '我', '是', '吃', '每', '天', '。', '個', '管', '師', '：', '啊', '你', '有', '固', '定', '的', '時', '間', '嗎', '。', '民', '眾', '：', '每', '天', '大', '概', '都', '九', '、', '十', '點', '。', '個', '管', '師', '：', '九', '、', '十', '點', '晚', '上', '。', '民', '眾', '：', '晚', '上', '。', '個', '管', '師', '：', '晚', '上', '，', '就', '是', '飯', '前', '飯', '後', '，', '那', '你', '吃', '這', '樣', 'P', 'r', 'E', 'P', '有', '沒', '有', '不', '舒', '服', '。', '民', '眾', '：', '不', '會', '。', '[SEP]', '師', '：', '喔', '，', '很', '可', '愛', '捏', '，', '一', '歲', '。', '民', '眾', '：', '問', '題', '是', '她', '那', '個', '骨', '頭', '發', '育', '不', '全', '，', '所', '以', '…', '…', '醫', '師', '：', '先', '天', '的', '。', '民', '眾', '：', '對', '，', '就', '是', '基', '因', '突', '變', '。', '醫', '師', '：', '基', '因', '突', '變', '，', '玻', '璃', '娃', '娃', '嗎', '，', '還', '是', '。', '民', '眾', '：', '誒', '，', '類', '似', '那', '種', '侏', '儒', '症', '。', '醫', '師', '：', '侏', '儒', '症', '的', '，', '喔', '，', '阿', '你', '們', '有', '基', '因', '嗎', '，', '還', '是', '。', '民', '眾', '：', '沒', '有', '阿', '。', '醫', '師', '：', '沒', '有', '，', '阿', '那', '個', '普', '拿', '疼', '我', '要', '不', '要', '開', '給', '妳', '。', '民', '眾', '：', '普', '拿', '疼', '不', '用', '。', '醫', '師', '：', '不', '用', '喔', '。', '民', '眾', '：', '不', '用', '。', '醫', '師', '：', '妳', '家', '裡', '還', '有', '剩', '這', '樣', '子', '。', '民', '眾', '：', '還', '有', '。', '醫', '師', '：', '好', '，', '奇', '怪', '妳', '發', '炎', '指', '數', '怎', '麼', '這', '麼', '高', '，', '突', '然', '竄', '到', '那', '麼', '高', '，', '呵', '呵', '～', '民', '眾', '：', '不', '知', '。', '醫', '師', '：', '不', '知', '。', '民', '眾', '：', '不', '知', '道', '為', '什', '麼', '。', '醫', '師', '：', '好', '，', '阿', '妳', '下', '次', '要', '回', '診', '什', '麼', '時', '候', '。', '民', '眾', '：', '就', '…', '…', '醫', '師', '：', '禮', '拜', '一', '禮', '拜', '三', '或', '禮', '拜', '四', '。', '民', '眾', '：', '一', '或', '三', '喔', '。', '醫', '師', '：', '是', '，', '一', '三', '四', '都', '可', '以', '。', '民', '眾', '：', '我', '一', '三', '，', '一', '三', '比', '較', 'o', 'k', '。', '醫', '師', '：', '一', '三', '比', '較', 'o', 'k', '。', '民', '眾', '：', '都', '是', '下', '午', '嗎', '。', '醫', '師', '：', '是', '。', '民', '眾', '：', '都', '可', '以', '，', '一', '…', '…', '一', '跟', '三', '的', '下', '午', '都', 'o', 'k', '。', '醫', '師', '：', '剛', '好', '下', '禮', '拜', '三', '人', '很', '多', '。', '民', '眾', '：', '人', '很', '多', '。', '醫', '師', '：', '看', '看', '假', '如', '禮', '拜', '一', '不', '知', '道', '行', '不', '行', '。', '因', '為', '我', '下', '禮', '拜', '一', '是', '人', '多', '少', '。', '護', '理', '師', '：', '禮', '拜', '一', '喔', '，', '呵', '呵', '～', '醫', '師', '：', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(pretrain_instance[0].tokens)\n",
    "print(pretrain_instance[0].original_tokens)\n",
    "# print(pretrain_instance[0].attention_mask)\n",
    "# print(pretrain_instance[0].segment_ids)\n",
    "# print(pretrain_instance[0].is_random_next)\n",
    "# print(pretrain_instance[0].masked_lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, pretrain_instance,tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.pretrain_instance = pretrain_instance\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer.encode(self.pretrain_instance[idx].tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        original_input_ids = self.tokenizer.encode(self.pretrain_instance[idx].original_tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        masked_lm_labels_ids = self.tokenizer.encode(self.pretrain_instance[idx].masked_lm_labels,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "\n",
    "        token_type_ids = torch.tensor(self.pretrain_instance[idx].segment_ids)\n",
    "        attention_mask = torch.tensor(self.pretrain_instance[idx].attention_mask)\n",
    "        is_random_next = torch.tensor(self.pretrain_instance[idx].is_random_next)\n",
    "        return input_ids, token_type_ids, attention_mask,  is_random_next , original_input_ids\n",
    "    def __len__(self):\n",
    "        return len(self.pretrain_instance)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "trainset = PreTrainDataset(pretrain_instance,tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in trainloader:\n",
    "#     input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "#     input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "#     masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "#     print(masked_lm_labels.size())\n",
    "#     print(masked_lm_labels)\n",
    "# #     masked_lm_labels_ids = torch.reshape(masked_lm_labels_ids,(masked_lm_labels_ids.size()[0],masked_lm_labels_ids.size()[2]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ../chinese_wwm_pytorch/ and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] 765/153 Loss: 0.6941 totaloss: 56.48549"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining\n",
    "model = BertForPreTraining.from_pretrained(pretrain_model_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for data in trainloader:\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "        input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "        masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         print(input_ids.size())\n",
    "#         print(token_type_ids.size())\n",
    "#         print(attention_mask.size())\n",
    "#         print(is_random_next.size())\n",
    "#         print(masked_lm_labels.size())\n",
    "        i += (input_ids.size()[0])\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels = masked_lm_labels,\n",
    "                        next_sentence_label = is_random_next.long())\n",
    "\n",
    "        loss = outputs[0]\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {loss.item():.4f} totaloss: {running_loss:.4f}', end='')\n",
    "    model.save_pretrained('./bertwwm_pretrain_aicup/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
