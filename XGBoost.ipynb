{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['type','title','text']\n",
    "dftrain = pd.read_csv('./data_after_sep/train.tsv',sep = '\\t',names = columns_name)\n",
    "dftest = pd.read_csv('./data_after_sep/test.tsv',sep = '\\t',names = columns_name)\n",
    "dfdev = pd.read_csv('./data_after_sep/dev.tsv',sep = '\\t',names = columns_name)\n",
    "\n",
    "\n",
    "model_path = './bert_pretrain_news/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return input_id, tokentype, attentionmask, y\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dftrain['text'].tolist()\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = 512,\n",
    "                                              truncation = True,                ##是否截斷\n",
    "                                              return_special_tokens_mask = True,\n",
    "                                              pad_to_max_length = True,\n",
    "                                              return_tensors = 'pt')\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "train_y = np.array(dftrain['type'].tolist())       ##np.array\n",
    "trainset = TrainDataset(train_input_dict, train_y) ##trainset參數如init\n",
    "trainloader = DataLoader(trainset, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "test_x = dftest['text'].tolist()\n",
    "test_input_dict = tokenizer.batch_encode_plus(test_x,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "test_y = np.array(dftest['type'].tolist())\n",
    "testset = TrainDataset(test_input_dict, test_y)\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((64,7))\n",
    "b = np.zeros((64,7))\n",
    "c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_pred(model_list,testloader):\n",
    "    count = 0\n",
    "    pred_list = []\n",
    "    ans_list = []\n",
    "    with torch.no_grad():\n",
    "        pred_concat = np.array([25546,7]) ##存三個分別的pred concat\n",
    "        for model in model_list:\n",
    "            pred = []\n",
    "            for data in testloader:\n",
    "                token_tensors,segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "                outputs = model(input_ids = token_tensors,\n",
    "                                token_type_ids = segment_tensors,\n",
    "                                attention_mask = masks_tensors,\n",
    "                                labels = labels)\n",
    "                for i in range(labels.size()[0]):\n",
    "                    if(count == 0):  ##labels拿一次就好\n",
    "                        ans_list.append(labels[i].to(\"cpu\").numpy())\n",
    "#                     result = torch.softmax(outputs[1][i],dim=-1)\n",
    "                    pred.append(outputs[1][i].to(\"cpu\").numpy())\n",
    "            pred = np.array(pred) ## 25546x7\n",
    "            if(count ==0):\n",
    "                pred_concat = pred\n",
    "            else:\n",
    "                pred_concat = np.concatenate((pred_concat,pred),axis = 1)\n",
    "            count += 1\n",
    "    return pred_concat,ans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 7\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = NUM_LABELS)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./BERT_for_xgboost_0.pkl'))\n",
    "model.eval()\n",
    "model2 = BertForSequenceClassification.from_pretrained(model_path,num_labels = NUM_LABELS)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model2.load_state_dict(torch.load('./BERT_for_xgboost_1.pkl'))\n",
    "model2.eval()\n",
    "model3 = BertForSequenceClassification.from_pretrained(model_path,num_labels = NUM_LABELS)\n",
    "model3 = model3.to(device)\n",
    "\n",
    "model3.load_state_dict(torch.load('./BERT_for_xgboost_2.pkl'))\n",
    "model3.eval()\n",
    "model_list = [model,model2,model3]\n",
    "# model_list = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model,model2,model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred,train_ans = get_model_pred(model_list,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25546,)\n",
      "(25546, 21)\n"
     ]
    }
   ],
   "source": [
    "train_ans = np.array(train_ans)\n",
    "train_pred = np.array(train_pred)\n",
    "print(train_ans.shape)\n",
    "print(train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "(5000, 21)\n"
     ]
    }
   ],
   "source": [
    "test_pred,test_ans = get_model_pred(model_list,testloader)\n",
    "test_ans = np.array(test_ans)\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_ans.shape)\n",
    "print(test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=160, n_jobs=0, num_class=7, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth = 5,learning_rate = 0.1,n_estimators=160,objective='multi:softmax',num_class=7,min_child_weight=5)\n",
    "xgb_model.fit(train_pred,train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = xgb_model.predict(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 1 ... 4 1 4]\n",
      "0.8464\n"
     ]
    }
   ],
   "source": [
    "print(pred_ans)\n",
    "count = 0 \n",
    "for i in range(5000):\n",
    "    if(pred_ans[i]==test_ans[i]):\n",
    "        count+=1\n",
    "print(count/5000)\n",
    "# print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:33] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.02059\ttest-merror:0.15480\n",
      "[1]\ttrain-merror:0.01930\ttest-merror:0.15600\n",
      "[2]\ttrain-merror:0.01903\ttest-merror:0.15580\n",
      "[3]\ttrain-merror:0.01934\ttest-merror:0.15640\n",
      "[4]\ttrain-merror:0.01934\ttest-merror:0.15640\n",
      "[5]\ttrain-merror:0.01891\ttest-merror:0.15580\n",
      "[6]\ttrain-merror:0.01906\ttest-merror:0.15480\n",
      "[7]\ttrain-merror:0.01898\ttest-merror:0.15420\n",
      "[8]\ttrain-merror:0.01863\ttest-merror:0.15440\n",
      "[9]\ttrain-merror:0.01848\ttest-merror:0.15480\n",
      "[10]\ttrain-merror:0.01828\ttest-merror:0.15500\n",
      "[11]\ttrain-merror:0.01805\ttest-merror:0.15480\n",
      "[12]\ttrain-merror:0.01793\ttest-merror:0.15460\n",
      "[13]\ttrain-merror:0.01769\ttest-merror:0.15380\n",
      "[14]\ttrain-merror:0.01742\ttest-merror:0.15400\n",
      "[15]\ttrain-merror:0.01722\ttest-merror:0.15380\n",
      "[16]\ttrain-merror:0.01695\ttest-merror:0.15440\n",
      "[17]\ttrain-merror:0.01679\ttest-merror:0.15500\n",
      "[18]\ttrain-merror:0.01672\ttest-merror:0.15520\n",
      "[19]\ttrain-merror:0.01644\ttest-merror:0.15440\n",
      "[20]\ttrain-merror:0.01644\ttest-merror:0.15460\n",
      "[21]\ttrain-merror:0.01624\ttest-merror:0.15440\n",
      "[22]\ttrain-merror:0.01609\ttest-merror:0.15420\n",
      "[23]\ttrain-merror:0.01605\ttest-merror:0.15440\n",
      "[24]\ttrain-merror:0.01601\ttest-merror:0.15440\n",
      "[25]\ttrain-merror:0.01597\ttest-merror:0.15380\n",
      "[26]\ttrain-merror:0.01605\ttest-merror:0.15360\n",
      "[27]\ttrain-merror:0.01593\ttest-merror:0.15300\n",
      "[28]\ttrain-merror:0.01589\ttest-merror:0.15260\n",
      "[29]\ttrain-merror:0.01570\ttest-merror:0.15340\n",
      "[30]\ttrain-merror:0.01558\ttest-merror:0.15320\n",
      "[31]\ttrain-merror:0.01542\ttest-merror:0.15320\n",
      "[32]\ttrain-merror:0.01534\ttest-merror:0.15300\n",
      "[33]\ttrain-merror:0.01523\ttest-merror:0.15240\n",
      "[34]\ttrain-merror:0.01523\ttest-merror:0.15240\n",
      "[35]\ttrain-merror:0.01527\ttest-merror:0.15240\n",
      "[36]\ttrain-merror:0.01523\ttest-merror:0.15260\n",
      "[37]\ttrain-merror:0.01534\ttest-merror:0.15240\n",
      "[38]\ttrain-merror:0.01523\ttest-merror:0.15240\n",
      "[39]\ttrain-merror:0.01519\ttest-merror:0.15220\n",
      "[40]\ttrain-merror:0.01511\ttest-merror:0.15240\n",
      "[41]\ttrain-merror:0.01507\ttest-merror:0.15280\n",
      "[42]\ttrain-merror:0.01491\ttest-merror:0.15240\n",
      "[43]\ttrain-merror:0.01487\ttest-merror:0.15300\n",
      "[44]\ttrain-merror:0.01468\ttest-merror:0.15320\n",
      "[45]\ttrain-merror:0.01464\ttest-merror:0.15320\n",
      "[46]\ttrain-merror:0.01468\ttest-merror:0.15320\n",
      "[47]\ttrain-merror:0.01452\ttest-merror:0.15320\n",
      "[48]\ttrain-merror:0.01452\ttest-merror:0.15320\n",
      "[49]\ttrain-merror:0.01444\ttest-merror:0.15320\n",
      "[50]\ttrain-merror:0.01429\ttest-merror:0.15300\n",
      "[51]\ttrain-merror:0.01417\ttest-merror:0.15320\n",
      "[52]\ttrain-merror:0.01397\ttest-merror:0.15300\n",
      "[53]\ttrain-merror:0.01390\ttest-merror:0.15300\n",
      "[54]\ttrain-merror:0.01390\ttest-merror:0.15300\n",
      "[55]\ttrain-merror:0.01394\ttest-merror:0.15240\n",
      "[56]\ttrain-merror:0.01374\ttest-merror:0.15260\n",
      "[57]\ttrain-merror:0.01370\ttest-merror:0.15280\n",
      "[58]\ttrain-merror:0.01366\ttest-merror:0.15320\n",
      "[59]\ttrain-merror:0.01350\ttest-merror:0.15320\n",
      "[60]\ttrain-merror:0.01347\ttest-merror:0.15320\n",
      "[61]\ttrain-merror:0.01335\ttest-merror:0.15320\n",
      "[62]\ttrain-merror:0.01335\ttest-merror:0.15320\n",
      "[63]\ttrain-merror:0.01331\ttest-merror:0.15320\n",
      "[64]\ttrain-merror:0.01323\ttest-merror:0.15320\n",
      "[65]\ttrain-merror:0.01296\ttest-merror:0.15320\n",
      "[66]\ttrain-merror:0.01276\ttest-merror:0.15300\n",
      "[67]\ttrain-merror:0.01272\ttest-merror:0.15300\n",
      "[68]\ttrain-merror:0.01276\ttest-merror:0.15300\n",
      "[69]\ttrain-merror:0.01257\ttest-merror:0.15260\n",
      "[70]\ttrain-merror:0.01264\ttest-merror:0.15280\n",
      "[71]\ttrain-merror:0.01257\ttest-merror:0.15280\n",
      "[72]\ttrain-merror:0.01233\ttest-merror:0.15260\n",
      "[73]\ttrain-merror:0.01229\ttest-merror:0.15260\n",
      "[74]\ttrain-merror:0.01221\ttest-merror:0.15280\n",
      "[75]\ttrain-merror:0.01206\ttest-merror:0.15260\n",
      "[76]\ttrain-merror:0.01202\ttest-merror:0.15280\n",
      "[77]\ttrain-merror:0.01190\ttest-merror:0.15300\n",
      "[78]\ttrain-merror:0.01178\ttest-merror:0.15240\n",
      "[79]\ttrain-merror:0.01147\ttest-merror:0.15300\n",
      "[80]\ttrain-merror:0.01120\ttest-merror:0.15260\n",
      "[81]\ttrain-merror:0.01116\ttest-merror:0.15280\n",
      "[82]\ttrain-merror:0.01108\ttest-merror:0.15260\n",
      "[83]\ttrain-merror:0.01080\ttest-merror:0.15240\n",
      "[84]\ttrain-merror:0.01057\ttest-merror:0.15220\n",
      "[85]\ttrain-merror:0.01053\ttest-merror:0.15220\n",
      "[86]\ttrain-merror:0.01037\ttest-merror:0.15240\n",
      "[87]\ttrain-merror:0.01041\ttest-merror:0.15240\n",
      "[88]\ttrain-merror:0.01033\ttest-merror:0.15200\n",
      "[89]\ttrain-merror:0.01006\ttest-merror:0.15180\n",
      "[90]\ttrain-merror:0.01006\ttest-merror:0.15200\n",
      "[91]\ttrain-merror:0.00975\ttest-merror:0.15220\n",
      "[92]\ttrain-merror:0.00971\ttest-merror:0.15220\n",
      "[93]\ttrain-merror:0.00959\ttest-merror:0.15220\n",
      "[94]\ttrain-merror:0.00940\ttest-merror:0.15180\n",
      "[95]\ttrain-merror:0.00928\ttest-merror:0.15180\n",
      "[96]\ttrain-merror:0.00920\ttest-merror:0.15160\n",
      "[97]\ttrain-merror:0.00900\ttest-merror:0.15180\n",
      "[98]\ttrain-merror:0.00896\ttest-merror:0.15160\n",
      "[99]\ttrain-merror:0.00885\ttest-merror:0.15160\n",
      "predicting, classification error=0.151600\n"
     ]
    }
   ],
   "source": [
    "xg_train = xgb.DMatrix(train_pred, label=train_ans)\n",
    "xg_test = xgb.DMatrix(test_pred, label=test_ans)\n",
    "param = {}\n",
    "\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 7\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 100\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist )\n",
    "\n",
    "pred = bst.predict( xg_test )\n",
    "print ('predicting, classification error=%f' % (sum( int(pred[i]) != test_ans[i] for i in range(len(test_ans))) / float(len(test_ans)) ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
