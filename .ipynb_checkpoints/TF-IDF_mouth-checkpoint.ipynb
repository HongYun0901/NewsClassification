{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from gensim.models import Word2Vec\n",
    "import keras\n",
    "from numpy import dot\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 3 6 2]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./all_after_mapping.tsv',sep='\\t',names=column_names)\n",
    "\n",
    "labels = df['type'].values\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "# labels = np_utils.to_categorical(labels)\n",
    "# print(labels.shape)\n",
    "\n",
    "column_names = ['type','title','text']\n",
    "df_train = pd.read_csv('./data_after_sep/train.tsv',sep='\\t',names=column_names)\n",
    "df_test = pd.read_csv('./data_after_sep/test.tsv',sep='\\t',names=column_names)\n",
    "df_dev = pd.read_csv('./data_after_sep/dev.tsv',sep='\\t',names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenlizeword = np.load('tokenlizeword0225_nopunct.npy',allow_pickle=True)\n",
    "dev_x = tokenlizeword[:5000]\n",
    "dev_y = labels[:5000]\n",
    "\n",
    "test_x = tokenlizeword[5000:10000]\n",
    "test_y = labels[5000:10000]\n",
    "\n",
    "train_x = tokenlizeword[10000:]\n",
    "train_y = labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6361\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in range(len(train_x)):\n",
    "    train_x[k] = np.array(train_x[k])\n",
    "    count+=train_x[k].shape[0]\n",
    "##存各類分開\n",
    "li_0 = []\n",
    "li_1 = []\n",
    "li_2 = []\n",
    "li_3 = []\n",
    "li_4 = []\n",
    "li_5 = []\n",
    "li_6 = []\n",
    "for k in range(len(train_x)):\n",
    "    if train_y[k]==0:\n",
    "        li_0.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==1:\n",
    "        li_1.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==2:\n",
    "        li_2.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==3:\n",
    "        li_3.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==4:\n",
    "        li_4.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==5:\n",
    "        li_5.append(' '.join(train_x[k]))\n",
    "    elif train_y[k]==6:\n",
    "        li_6.append(' '.join(train_x[k]))\n",
    "li_0 = np.array(li_0) ## input\n",
    "li_1 = np.array(li_1) ## input\n",
    "li_2 = np.array(li_2) ## input\n",
    "li_3 = np.array(li_3) ## input\n",
    "li_4 = np.array(li_4) ## input\n",
    "li_5 = np.array(li_5) ## input\n",
    "li_6 = np.array(li_6) ## input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全部搞成tfidf\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words=None, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", smooth_idf=True, norm='l2')\n",
    "tfidf_0 = vectorizer.fit_transform(li_0)\n",
    "df_tfidf_0 = pd.DataFrame(tfidf_0.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_1 = vectorizer.fit_transform(li_1)\n",
    "df_tfidf_1 = pd.DataFrame(tfidf_1.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_2 = vectorizer.fit_transform(li_2)\n",
    "df_tfidf_2 = pd.DataFrame(tfidf_2.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_3 = vectorizer.fit_transform(li_3)\n",
    "df_tfidf_3 = pd.DataFrame(tfidf_3.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_4 = vectorizer.fit_transform(li_4)\n",
    "df_tfidf_4 = pd.DataFrame(tfidf_4.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_5 = vectorizer.fit_transform(li_5)\n",
    "df_tfidf_5 = pd.DataFrame(tfidf_5.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_6 = vectorizer.fit_transform(li_6)\n",
    "df_tfidf_6 = pd.DataFrame(tfidf_6.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_0 = df_tfidf_0.describe()\n",
    "map_1 = df_tfidf_1.describe()\n",
    "map_2 = df_tfidf_2.describe()\n",
    "map_3 = df_tfidf_3.describe()\n",
    "map_4 = df_tfidf_4.describe()\n",
    "map_5 = df_tfidf_5.describe()\n",
    "map_6 = df_tfidf_6.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "map_list = []\n",
    "map_list.append(map_0)\n",
    "map_list.append(map_1)\n",
    "map_list.append(map_2)\n",
    "map_list.append(map_3)\n",
    "map_list.append(map_4)\n",
    "map_list.append(map_5)\n",
    "map_list.append(map_6)\n",
    "NUM_LABELS = 7\n",
    "##看看train的資料的\n",
    "for i in range(len(train_x)):\n",
    "    score_list = np.zeros((NUM_LABELS))\n",
    "    for j in range(len(train_x[i])):\n",
    "        for k in range(NUM_LABELS):\n",
    "            if train_x[i][j] not in map_list[k]:\n",
    "                score_list[k] += 0\n",
    "            else:\n",
    "                score_list[k] += map_list[k].iloc[1][train_x[i][j]]\n",
    "    pred_list.append(score_list)\n",
    "pred = np.argmax(pred_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred_list, axis=1)\n",
    "correct += (pred == labels).sum().item()\n",
    "\n",
    "print(correct/len(train_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
