{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import 套件\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW ##新ㄉ 好像比較好\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data & path\n",
    "columns_name = ['type','title','text']\n",
    "dftrain = pd.read_csv('./data_after_sep/train.tsv',sep = '\\t',names = columns_name)\n",
    "dftest = pd.read_csv('./data_after_sep/test.tsv',sep = '\\t',names = columns_name)\n",
    "dfdev = pd.read_csv('./data_after_sep/dev.tsv',sep = '\\t',names = columns_name)\n",
    "\n",
    "model_path = './bert_pretrain_news/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return input_id, tokentype, attentionmask, y\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PCADataset(Dataset):\n",
    "#     def __init__(self,x,y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#     def __getitem__(self,idx):\n",
    "#         X = self.x[idx]\n",
    "#         Y = self.y[idx]\n",
    "#         return X, Y\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dftrain['text'].tolist()\n",
    "train_input_dict = tokenizer.batch_encode_plus(train_x,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = 512,\n",
    "                                              truncation = True,                ##是否截斷\n",
    "                                              return_special_tokens_mask = True,\n",
    "                                              pad_to_max_length = True,\n",
    "                                              return_tensors = 'pt')\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "train_y = np.array(dftrain['type'].tolist())       ##np.array\n",
    "trainset = TrainDataset(train_input_dict, train_y) ##trainset參數如init\n",
    "trainloader = DataLoader(trainset, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "test_x = dftest['text'].tolist()\n",
    "test_input_dict = tokenizer.batch_encode_plus(test_x,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "test_y = np.array(dftest['type'].tolist())\n",
    "testset = TrainDataset(test_input_dict, test_y)\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x = dfdev['text'].tolist()\n",
    "dev_input_dict = tokenizer.batch_encode_plus(test_x,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "dev_y = np.array(dftest['type'].tolist())\n",
    "devset = TrainDataset(dev_input_dict, test_y)\n",
    "devloader = DataLoader(devset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## 類別對應 0：政治 1：生活 2：國際 3：體育 4：娛樂 5：社會 6：財經\n",
    "def get_test_acc(model, testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                           token_type_ids = segment_tensors,\n",
    "                           attention_mask = masks_tensors,\n",
    "                           labels = labels)\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA_DIM = 768\n",
    "\n",
    "# class PCA_linear(F.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PCA_linear,self).__init__()\n",
    "#         self.classifier = F.Sequential(\n",
    "#             F.Linear(PCA_DIM,256),\n",
    "#             F.Tanh(),\n",
    "#             F.Linear(256,64),\n",
    "#             F.ReLU(),\n",
    "#             F.Linear(64,7)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         X = self.classifier(x)\n",
    "#         return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4574eb12bbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./BERT_AdamW_label_smooth_weight_3.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "num_labels = 7\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./BERT_AdamW_label_smooth_weight_3.pkl'))\n",
    "model.eval()\n",
    "\n",
    "test_acc = get_test_acc(model,testloader)\n",
    "print('test acc:' , test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 類別對應 0：政治 1：生活 2：國際 3：體育 4：娛樂 5：社會 6：財經\n",
    "type_labels = ['0','1','2','3','4','5','6']\n",
    "type_accs = [0.9984,0.9965,0.9964,0.9929,0.9982,0.9898,0.9943]\n",
    "plt.title(\"train data two-ans\") \n",
    "plt.xlabel(\"News Class\") \n",
    "plt.ylabel(\"Acc\") \n",
    "plt.plot(type_labels,type_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sep_test_acc(model,testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    ##對應各類別的count\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                               token_type_ids = segment_tensors,\n",
    "                               attention_mask = masks_tensors,\n",
    "                               labels = labels)\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            for i in range(labels.size()[0]):\n",
    "                total_labels[labels[i]] += 1\n",
    "                if labels[i] == pred[i]:\n",
    "                    correct_labels[labels[i]] += 1\n",
    "            correct += (pred == labels).sum().item()\n",
    "        \n",
    "        for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "        print('Total：',correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sep_test_acc_two_ans(model,testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    ##對應各類別的count\n",
    "    label_list = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    total_labels =[0,0,0,0,0,0,0]\n",
    "    correct_labels = [0,0,0,0,0,0,0]\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                               token_type_ids = segment_tensors,\n",
    "                               attention_mask = masks_tensors,\n",
    "                               labels = labels)\n",
    "            print(outputs[1])\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            print(pred)\n",
    "            for i in range(labels.size()[0]):\n",
    "                outputs[1][i][pred[i]]= -9.9 #把最高調小一點\n",
    "            pred_sec = torch.argmax(outputs[1],dim=-1)\n",
    "            print(pred_sec)\n",
    "            total += labels.size()[0]\n",
    "            for i in range(labels.size()[0]):\n",
    "                total_labels[labels[i]] += 1\n",
    "                if labels[i] == pred[i] or labels[i] == pred_sec[i]:\n",
    "                    correct_labels[labels[i]] += 1\n",
    "            correct += (pred_sec == labels).sum().item()\n",
    "            correct += (pred == labels).sum().item()\n",
    "        \n",
    "        for i in range(len(total_labels)):\n",
    "            print(label_list[i],':',correct_labels[i],'/',total_labels[i], 'Acc :', (correct_labels[i]/total_labels[i]))\n",
    "        print('Total：',correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sep_test_acc_two_ans(model,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] 6386/6387 Loss: 2359.0848 Acc : 0.8777test acc: 0.8136\n",
      "Epoch [2/10] 6386/6387 Loss: 1360.6592 Acc : 0.9222test acc: 0.835\n",
      "Epoch [3/10] 6386/6387 Loss: 975.7502 Acc : 0.9439test acc: 0.8408\n",
      "Epoch [4/10] 6386/6387 Loss: 714.4038 Acc : 0.9591test acc: 0.8196\n",
      "Epoch [5/10] 6386/6387 Loss: 543.9036 Acc : 0.9679test acc: 0.8128\n",
      "Epoch [6/10] 2306/6387 Loss: 130.7072 Acc : 0.9805"
     ]
    }
   ],
   "source": [
    "## BertForSequencelassification\n",
    "num_labels = 7\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "e = 0.1 ##label smooth的參數\n",
    "# loss_fn = F.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 5\n",
    "type_weight = torch.FloatTensor([4.0160352e+00, 4.8995013e+00, 1.0133280e+01, 9.4230911e+00,\n",
    "7.3134841e+00, 1.2293551e+01, 8.0637626e+00]).to(device)\n",
    "type_loss_func = F.CrossEntropyLoss(weight=type_weight)\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5)\n",
    "\n",
    "for epoch in range(0,EPOCHS):\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for (i,data) in enumerate(trainloader):\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , labels  = [t.to(device) for t in data]\n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                             token_type_ids=segments_tensors, \n",
    "                             attention_mask=masks_tensors,\n",
    "                             labels = labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         logits = bert_outputs[1].clone()\n",
    "        logits = bert_outputs[1]\n",
    "        max_idx = torch.argmax(logits,dim = -1)\n",
    "        \n",
    "#         for count in range(labels.size()[0]):\n",
    "#             for idx in range(len(logits[count])):\n",
    "#                 if(idx == max_idx[count]):\n",
    "#                     logits[count][idx] *= (1-e)\n",
    "#                 else:\n",
    "#                     logits[count][idx] += e/(num_labels-1)\n",
    "        for count in range(labels.size()[0]):\n",
    "            for idx in range(len(logits[count])):\n",
    "                logits[count][idx] += e/(num_labels-1)\n",
    "#         print(logits)\n",
    "        ##權重\n",
    "        weight_loss = type_loss_func(logits,labels)\n",
    "        pred = torch.argmax(logits,dim = -1)\n",
    "        loss = 0.0\n",
    "        loss += weight_loss\n",
    "#         loss = bert_outputs[0]*0.5 +weight_loss*0.5\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += pred.size()[0]\n",
    "        correct += (pred == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {running_loss:.4f} Acc : {(correct/total):.4f}', end='')\n",
    "    test_acc = get_test_acc(model,testloader)\n",
    "#     if test_acc > highest_test:\n",
    "#         highest_test = test_acc\n",
    "    ##all save\n",
    "    torch.save(model.state_dict(),'./BERT_AdamW_label_smooth_weight_e01_add_' + str(epoch) + '.pkl')\n",
    "    print('test acc:' , test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 跑完ㄌ\n",
    "num_labels = 7\n",
    "config = BertConfig.from_pretrained(model_path + 'config.json',output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(model_path,config=config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "binary_model = PCA_linear()\n",
    "binary_model = binary_model.to(device)\n",
    "binary_model = binary_model.double()\n",
    "binary_model.train()\n",
    "\n",
    "loss_fn = F.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters())\n",
    "# optimizer = torch.optim.Adam(binary_model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0,EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for (i,data) in trainloader:\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , labels  = [t.to(device) for t in data]\n",
    "        \n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                                token_type_ids=segments_tensors, \n",
    "                                attention_mask=masks_tensors)\n",
    "        \n",
    "  \n",
    "\n",
    "        bert_all_768 = bert_outputs[1]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total += bert_all_768.size()[0]\n",
    "        \n",
    "        \n",
    "        logits = binary_model(bert_all_768.double())\n",
    "        loss = loss_fn(logits , labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        correct += (logits == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(binary_model.state_dict(),'./BERT_for_PCA' + str(epoch) + '.pkl')\n",
    "    print(epoch)\n",
    "    print('train acc:' , correct/total)\n",
    "    print(running_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 讀取\n",
    "num_labels = 7\n",
    "config = BertConfig.from_pretrained(model_path + 'config.json',output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(model_path,config=config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "binary_model = PCA_linear()\n",
    "binary_model.load_state_dict(torch.load('BERT_for_PCA9.pkl'))\n",
    "binary_model = binary_model.to(device)\n",
    "binary_model = binary_model.double()\n",
    "binary_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        token_tensors , segments_tensors , masks_tensors , labels = [t.to(device) for t in data]\n",
    "        bert_out = model(input_ids = token_tensors,\n",
    "                         token_type_ids = segments_tensors,\n",
    "                         attention_mask = masks_tensors)\n",
    "        test_768 = bert_out[1]\n",
    "        test_total += test_768.size()[0]\n",
    "        logits = binary_model(test_768.double())\n",
    "        pred = torch.argmax(logits,dim =-1)\n",
    "        test_correct += (pred==labels).sum().item()\n",
    "    print(\"acc:\",test_correct/test_total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels,output_hidden_states = True)\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.load_state_dict(torch.load(model_path + 'BERT_PCA.pkl'))\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_768 = []\n",
    "with torch.no_grad():\n",
    "    for (i,data) in enumerate(trainloader):\n",
    "        outputs = ()\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "        avg = np.zeros((768))\n",
    "        # forward pass\n",
    "        outputs = bert_model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors,\n",
    "                            output_hidden_states = True,\n",
    "                            labels = labels)\n",
    "        for k in range(512):\n",
    "            avg += outputs[2][12][0][k].to('cpu').numpy()\n",
    "        avg = np.true_divide(avg,512)\n",
    "        all_768.append(avg)\n",
    "        print(f'\\rEpoch: {i}/{len(trainloader)}', end='')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_768 = np.array(all_768)\n",
    "print(out_768.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_768 = np.array(all_768)\n",
    "print((all_768[0][0]))\n",
    "out_768 = np.zeros((35546,768))\n",
    "i_count = 0\n",
    "for i in all_768:\n",
    "    j_count = 0\n",
    "    for j in i:\n",
    "        j=j.cpu()\n",
    "        out_768[i_count][j_count] = j\n",
    "        j_count = j_count + 1\n",
    "    i_count = i_count + 1\n",
    "print(all_768.shape)\n",
    "print(out_768.shape)\n",
    "print(all_768[0])\n",
    "print(out_768[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = PCA_DIM)\n",
    "pca.fit(out_768)\n",
    "out = pca.transform(out_768)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PCA = out[:25546]\n",
    "test_PCA = out[25546:30546]\n",
    "dev_PCA = out[30546:]\n",
    "\n",
    "# train_PCA = train_PCA.tolist()\n",
    "# test_PCA = test_PCA.tolist()\n",
    "# dev_PCA = dev_PCA.tolist()\n",
    "print(train_PCA.shape)\n",
    "print(test_PCA.shape)\n",
    "print(dev_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_pca_dataset = PCADataset(out_768,train_y)\n",
    "train_pca_loader = DataLoader(train_pca_dataset,batch_size = BATCH_SIZE,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_pca_dataset = PCADataset(out_768[:25546],train_y)\n",
    "train_pca_loader = DataLoader(train_pca_dataset,batch_size = BATCH_SIZE,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pca_model = PCA_linear()\n",
    "# pca_model = pca_model.to(\"cuda:0\")\n",
    "pca_model = pca_model.double()\n",
    "pca_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight = torch.tensor([0.1,0.25,0.4,0.55,0.7,0.85,1]).double()\n",
    "\n",
    "loss_func = F.CrossEntropyLoss()\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.Adam(pca_model.parameters(), lr=3e-5)\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    correct = 0\n",
    "    for (i,data) in enumerate(train_pca_loader):\n",
    "        x,y =  [t for t in data]\n",
    "        outputs = pca_model(x.double())\n",
    "        optimizer.zero_grad()\n",
    "        pred = torch.argmax(outputs,dim=-1)\n",
    "        total +=y.size()[0]\n",
    "        correct += (pred == y).sum().item()\n",
    "        loss = loss_func(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss +=loss.item()\n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)/8} Loss: {running_loss:.4f} Acc : {(correct/total):.3f}', end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
