{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['type','title','text']\n",
    "unlabel_df = pd.read_csv('./udn_for_mct.tsv',sep='\\t',names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = unlabel_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['type','title','text']\n",
    "df = pd.read_csv('./all_after_mapping.tsv',sep='\\t',names=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenlizeword = np.load('tokenlizeword0225_nopunct.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel = Word2Vec(tokenlizeword, size=300, window=5, min_count=0)\n",
    "wmodel.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "labels = df['type'].values\n",
    "print(type(labels))\n",
    "# labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "max_size = 512\n",
    "x_lstm = []\n",
    "for k in range(tokenlizeword.shape[0]):\n",
    "  # every article have max_size * 300 embedding matrix\n",
    "    embedding_matrix = np.zeros((max_size,300))\n",
    "    for i in range(len(tokenlizeword[k])):\n",
    "        if(i>=max_size):\n",
    "            break\n",
    "        embedding_matrix[i] = wmodel[tokenlizeword[k][i]]\n",
    "    x_lstm.append(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lstm = np.array(x_lstm,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_x = x_lstm[10000:]\n",
    "lstm_train_y = labels[10000:]\n",
    "\n",
    "lstm_test_x = x_lstm[5000:10000]\n",
    "test_y = labels[5000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "n_hidden = 128 # number of hidden units in one cell\n",
    "num_classes = 7  \n",
    "BATCH_SIZE = 8\n",
    "epochs = 5\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(n_hidden * 2, num_classes)\n",
    "\n",
    "    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "#         return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        return context, soft_attn_weights # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "#         input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n",
    "        input = X\n",
    "        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n",
    "#         print(input)\n",
    "        \n",
    "        hidden_state = Variable(torch.zeros(1*2, BATCH_SIZE, n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        cell_state = Variable(torch.zeros(1*2, BATCH_SIZE, n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        hidden_state = hidden_state.double()\n",
    "        cell_state = cell_state.double()\n",
    "        hidden_state = hidden_state.to(device)\n",
    "        cell_state = cell_state.to(device)\n",
    "#         print(cell_state)\n",
    "#         print(hidden_state)\n",
    "#         print(hidden_state)\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "#         print('attn_output.shape',attn_output.shape)\n",
    "#         print('attention.shape',attention.shape)\n",
    "        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstmdataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x = torch.from_numpy(x)\n",
    "#         self.x = torch.DoubleTensor(x)\n",
    "#         self.x = self.x.double()\n",
    "        self.y = torch.from_numpy(y)\n",
    "#         self.y = torch.DoubleTensor(y)\n",
    "#         self.y = self.y.double()\n",
    "        self.len = x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "#         print(index)\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x,y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "lstm_trainset = Lstmdataset(lstm_train_x,lstm_train_y)\n",
    "lstm_trainloader = DataLoader(lstm_trainset,batch_size=BATCH_SIZE,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "264.60656613965006\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenlizeword[0]))\n",
    "count = 0\n",
    "for k in range(tokenlizeword.shape[0]):\n",
    "    tokenlizeword[k] = np.array(tokenlizeword[k])\n",
    "    count+=tokenlizeword[k].shape[0]\n",
    "print(count/tokenlizeword.shape[0])\n",
    "print(type(tokenlizeword[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35546,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "for k in range(tokenlizeword.shape[0]):\n",
    "    li.append(' '.join(tokenlizeword[k]))\n",
    "li = np.array(li)\n",
    "li.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "(35546, 512)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=512)\n",
    "X = vectorizer.fit_transform(li)\n",
    "word = vectorizer.get_feature_names()\n",
    "# print(word)\n",
    "# print(X.toarray())\n",
    "transformer = TfidfTransformer()\n",
    "print(transformer)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "x = tfidf.toarray()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_x = x[5000:10000]\n",
    "tfidf_train_x = x[10000:]\n",
    "tfidf_train_y = labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_train_x shape: (25546, 512)\n",
      "tfidf_train_y shape: (25546,)\n",
      "tfidf_test_x shape: (5000, 512)\n",
      "test_y shape: (5000,)\n",
      "lstm_train_x shape: (25546, 512, 300)\n",
      "lstm_train_y shape: (25546,)\n",
      "lstm_test_x shape: (5000, 512, 300)\n",
      "test_y shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print('tfidf_train_x shape:',tfidf_train_x.shape)\n",
    "print('tfidf_train_y shape:',tfidf_train_y.shape)\n",
    "print('tfidf_test_x shape:',tfidf_test_x.shape)\n",
    "print('test_y shape:',test_y.shape)\n",
    "print('lstm_train_x shape:',lstm_train_x.shape)\n",
    "print('lstm_train_y shape:',lstm_train_y.shape)\n",
    "print('lstm_test_x shape:',lstm_test_x.shape)\n",
    "print('test_y shape:',test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFdataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x,y\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "input_dim  = x.shape[1]\n",
    "class TFIDFmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TFIDFmodel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_token = np.load('tokenlizeword0226_udn_for_mct.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel_unlabel = Word2Vec(unlabel_token ,size=300, window=5, min_count=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10691, 512, 300)\n"
     ]
    }
   ],
   "source": [
    "max_size = 512\n",
    "unlabel_for_lstm = []\n",
    "for k in range(unlabel_token.shape[0]):\n",
    "  # every article have max_size * 300 embedding matrix\n",
    "    embedding_matrix = np.zeros((max_size,300))\n",
    "    for i in range(len(unlabel_token[k])):\n",
    "        if(i>=max_size):\n",
    "            break\n",
    "        embedding_matrix[i] = wmodel_unlabel[unlabel_token[k][i]]\n",
    "    unlabel_for_lstm.append(embedding_matrix)\n",
    "unlabel_for_lstm = np.array(unlabel_for_lstm,dtype='float32')\n",
    "print(unlabel_for_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "380.2110186137873\n",
      "<class 'numpy.ndarray'>\n",
      "(10691,)\n",
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "(10691, 512)\n"
     ]
    }
   ],
   "source": [
    "print(type(unlabel_token[0]))\n",
    "count = 0\n",
    "for k in range(unlabel_token.shape[0]):\n",
    "    unlabel_token[k] = np.array(unlabel_token[k])\n",
    "    count+=unlabel_token[k].shape[0]\n",
    "print(count/unlabel_token.shape[0])\n",
    "print(type(unlabel_token[0]))\n",
    "li = []\n",
    "for k in range(unlabel_token.shape[0]):\n",
    "    li.append(' '.join(unlabel_token[k]))\n",
    "li = np.array(li)\n",
    "print(li.shape)\n",
    "vectorizer = CountVectorizer(max_features=512)\n",
    "X = vectorizer.fit_transform(li)\n",
    "word = vectorizer.get_feature_names()\n",
    "# print(word)\n",
    "# print(X.toarray())\n",
    "transformer = TfidfTransformer()\n",
    "print(transformer)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "unlabel_for_tfidf = tfidf.toarray()\n",
    "print(unlabel_for_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(dataloader):\n",
    "    lstm_model = BiLSTM_Attention()\n",
    "    lstm_model = lstm_model.double()\n",
    "    lstm_model = lstm_model.to(device)\n",
    "    lstm_model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=1e-5)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for data in dataloader:\n",
    "            x,y = [t.to(device) for t in data]\n",
    "            x = x.double()\n",
    "            y = y.double()\n",
    "            lstm_optimizer.zero_grad()\n",
    "            output, attention = lstm_model(x)\n",
    "            y = y.long()\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            lstm_optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch:',epoch+1,'loss=',running_loss)\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf_model(dataloader):\n",
    "    tfidf_model = TFIDFmodel()\n",
    "    tfidf_model = tfidf_model.float()\n",
    "    tfidf_model = tfidf_model.to(device)\n",
    "    tfidf_model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tfidf_optimizer = optim.Adam(tfidf_model.parameters(), lr=1e-5)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for data in dataloader:\n",
    "            x,y = [t.to(device) for t in data]\n",
    "            x = x.float()\n",
    "            y = y.float()\n",
    "            tfidf_optimizer.zero_grad()\n",
    "            output = tfidf_model(x)\n",
    "            y = y.long()\n",
    "            loss = criterion(output, y)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            tfidf_optimizer.step()\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(running_loss))\n",
    "    return tfidf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_unlabel_dataset(Dataset):\n",
    "    def __init__(self,x):\n",
    "        self.x = x\n",
    "        self.len = x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        return x\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class Lstm_unlabel_dataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "#         print(type(x))\n",
    "        self.x = torch.from_numpy(x)\n",
    "#         print(type(self.x))\n",
    "        self.len = self.x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        return x\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def create_tfidf_unlabel_dataloader_dataset(x):\n",
    "#     BATCH_SIZE = 16\n",
    "    unlabel_tfidf_trainset = TFIDF_unlabel_dataset(x)\n",
    "    unlabel_tfidf_trainloader = DataLoader(unlabel_tfidf_trainset,batch_size=BATCH_SIZE,drop_last=True)\n",
    "    return unlabel_tfidf_trainset,unlabel_tfidf_trainloader\n",
    "\n",
    "def create_lstm_unlabel_dataloader_dataset(x):\n",
    "#     BATCH_SIZE = 16\n",
    "    unlabel_lstm_trainset = Lstm_unlabel_dataset(x)\n",
    "    unlabel_lstm_trainloader = DataLoader(unlabel_lstm_trainset,batch_size = BATCH_SIZE,drop_last=True)\n",
    "    return unlabel_lstm_trainset , unlabel_lstm_trainloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_lstm(model,dataloader):\n",
    "    predictions = None\n",
    "    predictions_withoutmax = None\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            \n",
    "#             print(data.shape)\n",
    "#             print(type(data))\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                x = data.to(device)\n",
    "            x = x.double()\n",
    "            outputs, state  = model(x)\n",
    "            after_softmax = F.softmax(outputs, dim=1)\n",
    "            _, pred = torch.max(after_softmax, 1)\n",
    "\n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                \n",
    "            if predictions_withoutmax is None:\n",
    "                predictions_withoutmax = after_softmax\n",
    "            else:\n",
    "                predictions_withoutmax = torch.cat((predictions_withoutmax,after_softmax))\n",
    "    return predictions_withoutmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_tfidf(model,dataloader):\n",
    "    predictions = None\n",
    "    predictions_withoutmax = None\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                x = data.to(device)\n",
    "            x = x.float()\n",
    "            outputs = model(x)\n",
    "            after_softmax = F.softmax(outputs, dim=1)\n",
    "            _, pred = torch.max(after_softmax, 1)\n",
    "\n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                \n",
    "            if predictions_withoutmax is None:\n",
    "                predictions_withoutmax = after_softmax\n",
    "            else:\n",
    "                predictions_withoutmax = torch.cat((predictions_withoutmax,after_softmax))\n",
    "    return predictions_withoutmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_high_confidence_data(lstm_result,tfidf_result):\n",
    "    baseline = 0.6\n",
    "    print(lstm_result.shape)\n",
    "    print(tfidf_result.shape)\n",
    "    count = 0\n",
    "    li = []\n",
    "    y = []\n",
    "    print(lstm_result.shape[0])\n",
    "    for i in range(lstm_result.shape[0]):\n",
    "        _lstm = lstm_result[i]\n",
    "        _tfidf = tfidf_result[i]\n",
    "#         if lstm max value's index equals to tfidf's\n",
    "        lstm_val , lstm_index = torch.max(_lstm, 0)\n",
    "        tfidf_val , tfidf_index = torch.max(_tfidf, 0)\n",
    "        if lstm_index.item() == tfidf_index.item():\n",
    "            count+=1\n",
    "            if lstm_val.item()>=baseline and tfidf_val.item()>=baseline:\n",
    "                li.append(i)\n",
    "                y.append(lstm_index.item())\n",
    "    print(count)\n",
    "    return np.array(li),np.array(y)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "before label_lstm.shape: (25546, 512, 300) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "before label_tfidf.shape: (25546, 512) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "before unlabel_shape: (10691, 512, 300) (10691, 512)\n",
      "Epoch: 1 loss= 4719.62287379822\n",
      "Epoch: 2 loss= 2624.5428890928342\n",
      "Epoch: 3 loss= 2060.1216377259675\n",
      "Epoch: 4 loss= 1804.5956267253887\n",
      "Epoch: 5 loss= 1633.944043532242\n",
      "Epoch: 0001 cost = 5471.775857\n",
      "Epoch: 0002 cost = 4005.419357\n",
      "Epoch: 0003 cost = 3013.727405\n",
      "Epoch: 0004 cost = 2554.254474\n",
      "Epoch: 0005 cost = 2238.595436\n",
      "torch.Size([10688, 7])\n",
      "torch.Size([10688, 7])\n",
      "10688\n",
      "3938\n",
      "(3938, 512, 300) <class 'numpy.ndarray'>\n",
      "(3938, 512) <class 'numpy.ndarray'>\n",
      "after combine label_lstm.shape: (29484, 512, 300) (29484,)\n",
      "after combine label_tfidf.shape: (29484, 512) (29484,)\n",
      "after unlabel_shape: (6753, 512, 300) (6753, 512)\n",
      "<class 'numpy.ndarray'>\n",
      "before label_lstm.shape: (25546, 512, 300) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "before label_tfidf.shape: (25546, 512) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "before unlabel_shape: (10691, 512, 300) (10691, 512)\n",
      "Epoch: 1 loss= 4614.1347084129375\n",
      "Epoch: 2 loss= 2512.707740481919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-b27064b3c268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#   using data to train lstm and tfidf model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_trainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtfidf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tfidf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_trainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3399db5b719f>\u001b[0m in \u001b[0;36mtrain_lstm_model\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlstm_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d4f2c61b44fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         print(hidden_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_cell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output : [batch_size, len_seq, n_hidden]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def co_training():\n",
    "# #     Define some parameter\n",
    "# #     BATCH_SIZE = 16\n",
    "    \n",
    "for i in range(10):\n",
    "#     init lstm label data\n",
    "    label_lstm_x =  lstm_train_x\n",
    "    label_lstm_y = lstm_train_y\n",
    "    print(type(label_lstm_x))\n",
    "    print('before label_lstm.shape:',label_lstm_x.shape,label_lstm_y.shape)\n",
    "    \n",
    "#     init tfidf label data\n",
    "    label_tfidf_x = tfidf_train_x\n",
    "    label_tfidf_y = tfidf_train_y\n",
    "    print(type(label_tfidf_x))\n",
    "    print('before label_tfidf.shape:',label_tfidf_x.shape,label_tfidf_y.shape)\n",
    "\n",
    "    \n",
    "#     init lstm unlabel data\n",
    "    unlabel_lstm_x = unlabel_for_lstm\n",
    "#     init tfidf unlabel data\n",
    "    unlabel_tfidf_x = unlabel_for_tfidf\n",
    "    print(type(unlabel_lstm_x))\n",
    "    print(type(unlabel_tfidf_x))\n",
    "    print('before unlabel_shape:',unlabel_lstm_x.shape,unlabel_tfidf_x.shape)\n",
    "#     create lstm label trainset and trainloader\n",
    "    lstm_trainset = Lstmdataset(label_lstm_x , label_lstm_y)\n",
    "    lstm_trainloader = DataLoader(lstm_trainset,batch_size=BATCH_SIZE,drop_last=True)\n",
    "    \n",
    "#     create tfidf label trainset and trainloader\n",
    "    tfidf_trainset = TFIDFdataset(label_tfidf_x,label_tfidf_y)\n",
    "    tfidf_trainloader = DataLoader(tfidf_trainset,batch_size=BATCH_SIZE,drop_last=True)\n",
    "    \n",
    "#     create unlabel trainset and trainloader\n",
    "    unlabel_lstm_trainset, unlabel_lstm_trainloader = create_lstm_unlabel_dataloader_dataset(unlabel_lstm_x)\n",
    "    unlabel_tfidf_trainset,unlabel_tfidf_trainloader = create_tfidf_unlabel_dataloader_dataset(unlabel_tfidf_x)\n",
    "    \n",
    "#     start co-training \n",
    "\n",
    "#   some judgement here\n",
    "\n",
    "\n",
    "#   using data to train lstm and tfidf model\n",
    "    lstm_model = train_lstm_model(lstm_trainloader)\n",
    "    tfidf_model = train_tfidf_model(tfidf_trainloader)\n",
    "    \n",
    "#     get predict result from model\n",
    "    lstm_predict_result = predict_model_lstm(lstm_model, unlabel_lstm_trainloader)\n",
    "    tfidf_predict_result = predict_model_tfidf(tfidf_model, unlabel_tfidf_trainloader)\n",
    "#     choose which unlabel data should be moved to label data\n",
    "\n",
    "    idx , y = pick_high_confidence_data(lstm_predict_result,tfidf_predict_result)\n",
    "    \n",
    "# #     update unlabel data\n",
    "\n",
    "    unlabel_be_chosen_tfidf = np.take(unlabel_tfidf_x, idx, 0) \n",
    "    unlabel_be_chosen_lstm = np.take(unlabel_lstm_x, idx, 0) \n",
    "    \n",
    "    unlabel_tfidf_x = np.delete(unlabel_tfidf_x, idx, axis=0)\n",
    "    unlabel_lstm_x = np.delete(unlabel_lstm_x, idx, axis=0)\n",
    "    \n",
    "    print(unlabel_be_chosen_lstm.shape,type(unlabel_be_chosen_lstm))\n",
    "    print(unlabel_be_chosen_tfidf.shape,type(unlabel_be_chosen_tfidf))\n",
    "\n",
    "    \n",
    "    \n",
    "    label_lstm_x =  np.concatenate((label_lstm_x,unlabel_be_chosen_lstm))\n",
    "    label_lstm_y = np.concatenate((label_lstm_y , y))\n",
    "    \n",
    "    label_tfidf_x =  np.concatenate((label_tfidf_x , unlabel_be_chosen_tfidf))\n",
    "    label_tfidf_y = np.concatenate((label_tfidf_y , y))\n",
    "    \n",
    "    print('after combine label_lstm.shape:',label_lstm_x.shape,label_lstm_y.shape)\n",
    "    print('after combine label_tfidf.shape:',label_tfidf_x.shape,label_tfidf_y.shape)\n",
    "    print('after unlabel_shape:',unlabel_lstm_x.shape,unlabel_tfidf_x.shape)\n",
    "    \n",
    "    torch.save(lstm_model, 'lstm_model_cotraining.pkl')\n",
    "    torch.save(tfidf_model, 'tfidf_model_cotraining.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tfidf_x = np.array(label_tfidf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25546,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lstm_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25546, 512, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lstm_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_tfidf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unlabel_be_chosen_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((label_tfidf_x,unlabel_be_chosen_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28934, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "label_lstm.shape: (25546, 512, 300) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "label_tfidf.shape: (25546, 512) (25546,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "unlabel_shape: (10691, 512, 300) (10691, 512)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e381adbfe913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-24e98e221f7a>\u001b[0m in \u001b[0;36mco_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#   using data to train lstm and tfidf model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_trainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtfidf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tfidf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_trainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3399db5b719f>\u001b[0m in \u001b[0;36mtrain_lstm_model\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mlstm_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "co_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
