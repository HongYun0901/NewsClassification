{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import collections\n",
    "import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some value\n",
    "max_seq_length = 512\n",
    "masked_lm_prob = 0.15\n",
    "max_predictions_per_seq = 20\n",
    "rng = random.Random()\n",
    "\n",
    "pretrain_model_path = './chinese_wwm_pytorch/'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['type','title','text']\n",
    "dftrain = pd.read_csv('./data_after_sep/train.tsv',sep='\\t',names=column_names)\n",
    "dftest = pd.read_csv('./data_after_sep/test.tsv',sep='\\t',names=column_names)\n",
    "dfdev = pd.read_csv('./data_after_sep/dev.tsv',sep='\\t',names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from zhon.hanzi import stops\n",
    "def cut_sent(para):\n",
    "    para = re.sub(\"([。！？\\?])([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\.{6})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"(\\…{2})([^”’])\", r\"\\1\\n\\2\", para)\n",
    "    para = re.sub(\"([。！？\\?][”’])([^，。！？\\?])\", r\"\\1\\n\\2\", para)\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "def clean_string(content):\n",
    "    content = content.replace('\\n','。').replace('\\t','，').replace('!', '！').replace('?', '？').replace('.','。')\n",
    "    content = re.sub(r\"[%s]+\" %stops, \"。\",content)\n",
    "    return content\n",
    "\n",
    "class TrainingInstance(object):\n",
    "    # \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,is_random_next,attention_mask,original_tokens):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.attention_mask = attention_mask\n",
    "        self.original_tokens = original_tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [(x) for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "# \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()   \n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(\n",
    "    tokens,\n",
    "    masked_lm_prob,\n",
    "    max_predictions_per_seq,\n",
    "    vocab_words,\n",
    "    rng,\n",
    "    ):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]' or token == '[PAD]':\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "\n",
    "    output_tokens = list(tokens)\n",
    "\n",
    "    num_to_predict = min(max_predictions_per_seq, max(1,\n",
    "                         int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if index in covered_indexes:\n",
    "            continue\n",
    "        covered_indexes.add(index)\n",
    "\n",
    "        masked_token = None\n",
    "\n",
    "    # 80% of the time, replace with [MASK]\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = '[MASK]'\n",
    "        else:\n",
    "\n",
    "      # 10% of the time, keep original\n",
    "\n",
    "            if rng.random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "            else:\n",
    "\n",
    "      # 10% of the time, replace with random word\n",
    "\n",
    "                masked_token = vocab_words[rng.randint(0,\n",
    "                        len(vocab_words) - 1)]\n",
    "\n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        masked_lms.append(MaskedLmInstance(index=index,\n",
    "                          label=tokens[index]))\n",
    "\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfall = pd.concat([dftrain,dftest,dfdev])\n",
    "all_texts = dfall['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35546\n"
     ]
    }
   ],
   "source": [
    "all_documents = [[]]\n",
    "for text in all_texts:\n",
    "    text = clean_string(text)\n",
    "    sentences = cut_sent(text)\n",
    "    all_documents.append(sentences)\n",
    "\n",
    "# Remove empty documents\n",
    "all_documents = [x for x in all_documents if x]\n",
    "rng.shuffle(all_documents)\n",
    "print(len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_instance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76423\n"
     ]
    }
   ],
   "source": [
    "for document_index in range(len(all_documents)):\n",
    "    document = all_documents[document_index]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "    target_seq_length = max_num_tokens\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                tokens_b = []\n",
    "\n",
    "        # Random next\n",
    "\n",
    "                is_random_next = False\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = rng.randint(0,\n",
    "                                len(all_documents) - 1)\n",
    "                        if random_document_index != document_index:\n",
    "                            break\n",
    "\n",
    "                    random_document = \\\n",
    "                        all_documents[random_document_index]\n",
    "                    random_start = rng.randint(0, len(random_document)\n",
    "                            - 1)\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                else:\n",
    "\n",
    "        # Actual next\n",
    "\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens,\n",
    "                                  rng)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                tokens.append('[CLS]')\n",
    "                segment_ids.append(0)\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append('[SEP]')\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                attention_mask = [1] * len(tokens)\n",
    "                \n",
    "                while len(tokens) < max_seq_length:\n",
    "                    tokens.append('[PAD]')\n",
    "                    segment_ids.append(0)\n",
    "                    attention_mask.append(0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                original_tokens = tokens\n",
    "                (tokens, masked_lm_positions, masked_lm_labels) = \\\n",
    "                    create_masked_lm_predictions(tokens,\n",
    "                        masked_lm_prob, max_predictions_per_seq,\n",
    "                        vocab_words, rng)\n",
    "                \n",
    "                instance = TrainingInstance(tokens=tokens,\n",
    "                        segment_ids=segment_ids,\n",
    "                        is_random_next=is_random_next,\n",
    "                        masked_lm_positions=masked_lm_positions,\n",
    "                        masked_lm_labels=masked_lm_labels,\n",
    "                        attention_mask = attention_mask,\n",
    "                        original_tokens = original_tokens)\n",
    "                \n",
    "                instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "#     print(instances)\n",
    "    pretrain_instance.extend(instances)\n",
    "\n",
    "print(len(pretrain_instance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '前', '行', '政', '院', '長', '賴', '清', '德', '以', '蔡', '英', '文', '總', '統', '副', '手', '之', '姿', '，', '即', '將', '成', '為', '新', '任', '副', '總', '統', '，', '他', '的', '政', '治', '[MASK]', '練', '完', '整', '，', '更', '是', '民', '進', '黨', '[MASK]', '星', '級', '人', '物', '，', '不', '過', '上', '[MASK]', '在', '宣', '布', '勝', '[MASK]', '那', '[MASK]', '砧', '，', '很', '多', '人', '隱', '約', '觀', '察', '到', '賴', '清', '德', '臉', '上', '沒', '有', '笑', '容', '。', '財', '經', '名', '嘴', '汪', '潔', '民', '昨', '晚', '在', '臉', '[MASK]', '直', '言', '，', '蔡', '英', '[MASK]', '、', '賴', '清', '德', '聯', '手', '拿', '下', '8', '1', '7', '萬', '[MASK]', '，', '對', '民', '進', '黨', '是', '[MASK]', '任', '壓', '[MASK]', '##35', '承', '擔', '，', '而', '這', '[MASK]', '數', '不', '是', '歡', '樂', '勝', '選', '[MASK]', '而', '是', '託', '付', '，', '「', '宣', '布', '勝', '選', '的', '片', '刻', '，', '我', '看', '到', '的', '是', '在', '賴', '清', '德', '的', '臉', '上', '沒', '有', '笑', '容', '，', '因', '為', '是', '承', '擔', '的', '開', '始', '。', '[MASK]', '汪', '潔', '民', '說', '，', '當', '晚', '蔡', '英', '文', '總', '[MASK]', '的', '笑', '容', '，', '是', '[MASK]', '自', '內', '心', '[MASK]', '悅', '，', '「', '喜', '悅', '，', '當', '然', '是', '正', '常', '的', '，', '但', '是', '不', '能', '成', '為', '驕', '傲', '，', '衷', '心', '期', '待', '沒', '有', '下', '一', '個', '[MASK]', '全', '出', '現', '，', '莫', '忘', '謙', '卑', '。', '[SEP]', '」', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', '前', '行', '政', '院', '長', '賴', '清', '德', '以', '蔡', '英', '文', '總', '統', '副', '手', '之', '姿', '，', '即', '將', '成', '為', '新', '任', '副', '總', '統', '，', '他', '的', '政', '治', '歷', '練', '完', '整', '，', '更', '是', '民', '進', '黨', '明', '星', '級', '人', '物', '，', '不', '過', '上', '周', '在', '宣', '布', '勝', '選', '那', '一', '刻', '，', '很', '多', '人', '隱', '約', '觀', '察', '到', '賴', '清', '德', '臉', '上', '沒', '有', '笑', '容', '。', '財', '經', '名', '嘴', '汪', '潔', '民', '昨', '晚', '在', '臉', '書', '直', '言', '，', '蔡', '英', '文', '、', '賴', '清', '德', '聯', '手', '拿', '下', '8', '1', '7', '萬', '票', '，', '對', '民', '進', '黨', '是', '責', '任', '壓', '力', '和', '承', '擔', '，', '而', '這', '票', '數', '不', '是', '歡', '樂', '勝', '選', '，', '而', '是', '託', '付', '，', '「', '宣', '布', '勝', '選', '的', '片', '刻', '，', '我', '看', '到', '的', '是', '在', '賴', '清', '德', '的', '臉', '上', '沒', '有', '笑', '容', '，', '因', '為', '是', '承', '擔', '的', '開', '始', '。', '」', '汪', '潔', '民', '說', '，', '當', '晚', '蔡', '英', '文', '總', '統', '的', '笑', '容', '，', '是', '發', '自', '內', '心', '喜', '悅', '，', '「', '喜', '悅', '，', '當', '然', '是', '正', '常', '的', '，', '但', '是', '不', '能', '成', '為', '驕', '傲', '，', '衷', '心', '期', '待', '沒', '有', '下', '一', '個', '林', '全', '出', '現', '，', '莫', '忘', '謙', '卑', '。', '[SEP]', '」', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(pretrain_instance[0].tokens)\n",
    "print(pretrain_instance[0].original_tokens)\n",
    "# print(pretrain_instance[0].attention_mask)\n",
    "# print(pretrain_instance[0].segment_ids)\n",
    "# print(pretrain_instance[0].is_random_next)\n",
    "# print(pretrain_instance[0].masked_lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, pretrain_instance,tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.pretrain_instance = pretrain_instance\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer.encode(self.pretrain_instance[idx].tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        original_input_ids = self.tokenizer.encode(self.pretrain_instance[idx].original_tokens,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "        masked_lm_labels_ids = self.tokenizer.encode(self.pretrain_instance[idx].masked_lm_labels,\n",
    "                                          add_special_tokens=False,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "\n",
    "        token_type_ids = torch.tensor(self.pretrain_instance[idx].segment_ids)\n",
    "        attention_mask = torch.tensor(self.pretrain_instance[idx].attention_mask)\n",
    "        is_random_next = torch.tensor(self.pretrain_instance[idx].is_random_next)\n",
    "        return input_ids, token_type_ids, attention_mask,  is_random_next , original_input_ids\n",
    "    def __len__(self):\n",
    "        return len(self.pretrain_instance)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "trainset = PreTrainDataset(pretrain_instance,tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "tensor([[ 101, 3173, 1094,  ..., 8038,  122,  102],\n",
      "        [ 101, 8024, 3413,  ..., 6525,  511,  102],\n",
      "        [ 101,  686, 3173,  ...,    0,    0,    0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# for data in trainloader:\n",
    "#     input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "#     input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "#     masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "#     print(masked_lm_labels.size())\n",
    "#     print(masked_lm_labels)\n",
    "# #     masked_lm_labels_ids = torch.reshape(masked_lm_labels_ids,(masked_lm_labels_ids.size()[0],masked_lm_labels_ids.size()[2]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ./chinese_wwm_pytorch/ and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4] 2808/25475 Loss: 0.2007 totaloss: 361.9043"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e3a4a4217aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining\n",
    "model = BertForPreTraining.from_pretrained(pretrain_model_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for data in trainloader:\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask,  is_random_next , masked_lm_labels = [t.to(device) for t in data]\n",
    "\n",
    "        input_ids = torch.reshape(input_ids,(input_ids.size()[0],input_ids.size()[2]))\n",
    "        masked_lm_labels = torch.reshape(masked_lm_labels,(masked_lm_labels.size()[0],masked_lm_labels.size()[2]))\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         print(input_ids.size())\n",
    "#         print(token_type_ids.size())\n",
    "#         print(attention_mask.size())\n",
    "#         print(is_random_next.size())\n",
    "#         print(masked_lm_labels.size())\n",
    "        i += (input_ids.size()[0])\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels = masked_lm_labels,\n",
    "                        next_sentence_label = is_random_next.long())\n",
    "\n",
    "        loss = outputs[0]\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {loss.item():.4f} totaloss: {running_loss:.4f}', end='')\n",
    "    model.save_pretrained('./bert_pretrain_news/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
