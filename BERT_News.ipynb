{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import 套件\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW ##新ㄉ 好像比較好\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data & path\n",
    "columns_name = ['type','title','text','time','url']\n",
    "dfchina = pd.read_csv('./data_after_sep/cn_all.tsv',sep = '\\t',names = columns_name) ##中時\n",
    "dffree = pd.read_csv('./data_after_sep/ltn_all.tsv',sep = '\\t',names = columns_name) ##自由\n",
    "dfunite = pd.read_csv('./data_after_sep/udn_all.tsv',sep = '\\t',names = columns_name) ##聯合\n",
    "\n",
    "model_path = './bert_pretrain_news/'\n",
    "# model_path = './chinese_wwm_pytorch/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##幫它們標label\n",
    "for i in range(dfchina.shape[0]):\n",
    "    dfchina.iloc[i][0] = 0\n",
    "for i in range(dffree.shape[0]):\n",
    "    dffree.iloc[i][0] = 1\n",
    "for i in range(dfunite.shape[0]):\n",
    "    dfunite.iloc[i][0] = 2\n",
    "train_china = dfchina['text'].tolist()\n",
    "train_free = dffree['text'].tolist()\n",
    "train_unite = dfunite['text'].tolist()\n",
    "\n",
    "china_y = dfchina['type'].tolist()\n",
    "free_y = dffree['type'].tolist()\n",
    "unite_y = dfunite['type'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24070\n",
      "32961\n",
      "38713\n"
     ]
    }
   ],
   "source": [
    "print(len(train_china))\n",
    "print(len(train_free))\n",
    "print(len(train_unite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24069\n",
      "32599\n",
      "38612\n"
     ]
    }
   ],
   "source": [
    "##過濾一堆 nan\n",
    "train_china = [data for data in train_china if type(data) != float]\n",
    "train_free = [data for data in train_free if type(data) != float]\n",
    "train_unite = [data for data in train_unite if type(data) != float] \n",
    "print(len(train_china))\n",
    "print(len(train_free))\n",
    "print(len(train_unite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24069\n",
      "32599\n",
      "38612\n"
     ]
    }
   ],
   "source": [
    "china_y = [china_y[count] for count in range(len(train_china))]\n",
    "free_y = [free_y[count] for count in range(len(train_free))]\n",
    "unite_y = [unite_y[count] for count in range(len(train_unite))]\n",
    "print(len(china_y))\n",
    "print(len(free_y))\n",
    "print(len(unite_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75480\n",
      "75480\n",
      "19800\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "## 切割train跟 test test取比例篇 先多一點\n",
    "train_x = train_china[5000:]+train_free[6800:]+train_unite[8000:]\n",
    "print(len(train_x))\n",
    "train_y = china_y[5000:]+free_y[6800:]+unite_y[8000:]\n",
    "print(len(train_y))\n",
    "test_x = train_china[:5000]+train_free[:6800]+train_unite[:8000]\n",
    "print(len(test_x))\n",
    "test_y = china_y[:5000]+free_y[:6800]+unite_y[:8000]\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return input_id, tokentype, attentionmask, y\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_dict = tokenizer.batch_encode_plus(train_x,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = 512,\n",
    "                                              truncation = True,                ##是否截斷\n",
    "                                              return_special_tokens_mask = True,\n",
    "                                              pad_to_max_length = True,\n",
    "                                              return_tensors = 'pt')\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "train_y = np.array(train_y)       ##np.array\n",
    "trainset = TrainDataset(train_input_dict, train_y) ##trainset參數如init\n",
    "trainloader = DataLoader(trainset, batch_size = TRAIN_BATCH_SIZE, shuffle = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "test_input_dict = tokenizer.batch_encode_plus(test_x,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = 512,\n",
    "                                              truncation = True,                ##是否截斷\n",
    "                                              return_special_tokens_mask = True,\n",
    "                                              pad_to_max_length = True,\n",
    "                                              return_tensors = 'pt')\n",
    "BATCH_SIZE = 64\n",
    "test_y = np.array(test_y)       ##np.array\n",
    "testset = TrainDataset(test_input_dict, test_y) ##trainset參數如init\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## 類別對應 0：政治 1：生活 2：國際 3：體育 4：娛樂 5：社會 6：財經\n",
    "def get_test_acc(model, testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                           token_type_ids = segment_tensors,\n",
    "                           attention_mask = masks_tensors,\n",
    "                           labels = labels)\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.6435858585858586\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./BERT_news_2e5_0.pkl'),strict = False)\n",
    "model.eval()\n",
    "\n",
    "test_acc = get_test_acc(model,testloader)\n",
    "print('test acc:' , test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] 6915/18870 Loss: 6301.8326 Acc : 0.5115"
     ]
    }
   ],
   "source": [
    "## BertForSequencelassification\n",
    "num_labels = 3\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5)\n",
    "\n",
    "for epoch in range(0,EPOCHS):\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for (i,data) in enumerate(trainloader):\n",
    "        \n",
    "        tokens_tensors ,  segments_tensors , masks_tensors , labels  = [t.to(device) for t in data]\n",
    "#         print(data)\n",
    "        bert_outputs = model(input_ids=tokens_tensors, \n",
    "                             token_type_ids=segments_tensors, \n",
    "                             attention_mask=masks_tensors,\n",
    "                             labels = labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = bert_outputs[1]\n",
    "        ##權重\n",
    "        pred = torch.argmax(logits,dim = -1)\n",
    "        loss = bert_outputs[0] \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += pred.size()[0]\n",
    "        correct += (pred == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        print(f'\\rEpoch [{epoch+1}/{EPOCHS}] {i}/{len(trainloader)} Loss: {running_loss:.4f} Acc : {(correct/total):.4f}', end='')\n",
    "#         del data\n",
    "    test_acc = get_test_acc(model,testloader)\n",
    "    torch.save(model.state_dict(),'./BERT_news_2e5_' + str(epoch) + '.pkl')\n",
    "    print('test acc:' , test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
