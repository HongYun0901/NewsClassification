{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import 套件\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW ##新ㄉ 好像比較好\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['type','title','text']\n",
    "dftest = pd.read_csv('./data_after_sep/test.tsv',sep = '\\t',names = columns_name)\n",
    "dfdev = pd.read_csv('./data_after_sep/dev.tsv',sep = '\\t',names = columns_name)\n",
    "\n",
    "model_path = './bert_pretrain_news/'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "    def __getitem__(self, idx):\n",
    "        input_id = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return input_id, tokentype, attentionmask, y\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "pred_list = np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "test_x = dftest['text'].tolist()\n",
    "test_input_dict = tokenizer.batch_encode_plus(test_x,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "test_y = np.array(dftest['type'].tolist())\n",
    "testset = TestDataset(test_input_dict, test_y)\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## 類別對應 0：政治 1：生活 2：國際 3：體育 4：娛樂 5：社會 6：財經\n",
    "def get_test_acc(model, testloader):\n",
    "    model.eval()  ##test mode\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                           token_type_ids = segment_tensors,\n",
    "                           attention_mask = masks_tensors,\n",
    "                           labels = labels)\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three ensemble\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## 類別對應 0：政治 1：生活 2：國際 3：體育 4：娛樂 5：社會 6：財經\n",
    "def get_test_ensemble_acc(model_list, testloader,batch_size):\n",
    "    total = 0 ##total_num\n",
    "    correct = 0 ##correct_num\n",
    "    pred_list = np.array([])\n",
    "    with torch.no_grad():   ##eval不計算gradient \n",
    "        for data in testloader:\n",
    "            pred = torch.empty(batch_size, 7, dtype=torch.float)\n",
    "            pred = pred.to(device)\n",
    "            for model in model_list:\n",
    "                model.eval()\n",
    "                tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "                outputs = model(input_ids = tokens_tensors,\n",
    "                                token_type_ids = segment_tensors,\n",
    "                                attention_mask = masks_tensors,\n",
    "                                labels = labels)\n",
    "                print(pred)\n",
    "                print(outputs[1])\n",
    "                pred += outputs[1]\n",
    "            pred = torch.argmax(outputs[1],dim=-1)\n",
    "            total += labels.size()[0]\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b6f2db736e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/nlp/NewsClassify/BERT_for_xgboost_0.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 7 \n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('/home/nlp/NewsClassify/BERT_for_xgboost_0.pkl'))\n",
    "\n",
    "model.eval()\n",
    "model1 = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model1 = model1.to(device)\n",
    "model1.load_state_dict(torch.load('/home/nlp/NewsClassify/BERT_for_xgboost_1.pkl'))\n",
    "model1.eval()\n",
    "\n",
    "model2 = BertForSequenceClassification.from_pretrained(model_path,num_labels = num_labels)\n",
    "model2 = model2.to(device)\n",
    "model2.load_state_dict(torch.load('/home/nlp/NewsClassify/BERT_for_xgboost_2.pkl'))\n",
    "model2.eval()\n",
    "model_list = [model,model1,model2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8574\n"
     ]
    }
   ],
   "source": [
    "total = 0 ##total_num\n",
    "correct = 0 ##correct_num\n",
    "pred_list = np.array([])\n",
    "with torch.no_grad():   ##eval不計算gradient \n",
    "    for data in testloader:\n",
    "        pred = torch.zeros(BATCH_SIZE, 7, dtype=torch.float)\n",
    "        pred = pred.to(device)\n",
    "        for model in model_list:\n",
    "            model.eval()\n",
    "            tokens_tensors , segment_tensors,masks_tensors,labels = [t.to(device) for t in data]\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                            token_type_ids = segment_tensors,\n",
    "                            attention_mask = masks_tensors,\n",
    "                            labels = labels)\n",
    "            pred += outputs[1]\n",
    "        pred = torch.argmax(pred,dim=-1)\n",
    "        total += labels.size()[0]\n",
    "        correct += (pred == labels).sum().item()\n",
    "    print(correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
